{\color{blue} exposition on square functions...}\todo{todo}

\section{Random sums and averages}

\begin{defn}
  A \emph{Rademacher variable} is a random variable $\map{\varepsilon}{\Omega}{\{-1, 1\}}$ defined on a probability space $(\Omega,\mc{A},\P)$ such that
  \begin{equation*}
    \P(\varepsilon = -1) = \P(\varepsilon = +1) = \frac{1}{2}.
  \end{equation*}
  A \emph{Rademacher sequence} is a sequence of mutually independent Rademacher variables $(\varepsilon_{n})_{n \in \N}$ on a probability space $(\Omega,\mc{A},\P)$.
\end{defn}

\begin{rmk}
  We will also consider Rademacher sequences $(\varepsilon_{\lambda})_{\lambda \in \Lambda}$ on different countable (or finite) indexing sets $\Lambda$.
  The definition is exactly the same.
\end{rmk}

When we discuss Rademacher sequences we generally discuss properties depending on the joint \emph{distributions} of the Rademacher variables, rather than on the Rademacher variables themselves.
Thus we can exploit properties or intuition coming from different realisations of Rademacher sequences.
Two standard choices are:

\begin{example}[The probabilist's Rademachers]
  As in Example \ref{eg:gambling-filtrations}, consider the probability space
  \begin{equation*}
    \Omega := \prod_{n \in \N} \{-1,1\} = \{-1,1\}^{\N}
  \end{equation*}
  with the product $\sigma$-algebra and measure, where the factors are equipped with the uniform probability measure, and let $\map{\pi_{n}}{\Omega}{\{-1,1\}}$ be the $n$-th coordinate function.
  Then $(\pi_{n})_{n \in \N}$ is a Rademacher sequence.
\end{example}

\begin{example}[The analyst's Rademachers]
  Let $\Omega = [0,1]$ with the Lebesgue measure and Borel $\sigma$-algebra, and consider the \emph{Rademacher functions}
  \begin{equation*}
    r_n(t) := \sgn(\sin(2^n\pi t)) \qquad \forall t \in [0,1], n = 1,2,\cdots.
  \end{equation*}
  These are square waves with period $2^{-n}$, and form a Rademacher sequence.
\end{example}

A \emph{(finite) Rademacher sum} in a Banach space $X$ is an $X$-valued random variable of the form
\begin{equation}\label{eq:rademacher-sum}
  \sum_{n = 0}^{N} \varepsilon_n \mb{x}_{n} \colon \Omega \to X,
\end{equation}
where $(\mb{x}_{n})_{n = 0}^{N}$ is a finite sequence of vectors in $X$ and $(\varepsilon_{n})_{n = 0}^{N}$ is a finite Rademacher sequence on $\Omega$.
Although the random variable in \eqref{eq:rademacher-sum} depends on the choice of Rademacher sequence, its distribution is independent of this choice (Exercise \ref{ex:rad-sum-dist}).\footnote{Recall that the distribution of a random variable $\map{f}{\Omega}{X}$ is the pushforward measure $f_{*}(\P)$ on $X$, given by $f_{*}(\P)(A) := \P(f^{-1}(A))$ for all Borel sets $A \subset X$.}
As a consequence, the \emph{(finite) Rademacher average}
\begin{equation*}
  \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} = \int_{\Omega}  \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega) \mb{x}_{n} \Big\|_{X} \, \dd\P(\omega)
\end{equation*}
is independent of the choice of Rademacher sequence.

Rademacher averages show up \emph{all the time} in Banach-valued analysis, so we better get used to them.
There are two fundamental properties that they satisfy: the \emph{contraction principle} (to be proven in a moment), and the \emph{Kahane--Khintchine inequalities} (the subject of the next section).

\begin{thm}[Contraction principle]
  There exists a constant $C \leq 2$ with the following property: for all Banach spaces $X$, all $N \in \N$, and all finite sequences $(\mb{x}_{n})_{n = 0}^{N}$ in $X$ and $(a_{n})_{n=0}^{N}$ in the scalar field $\K$,
  \begin{equation*}
    \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} a_{n} \mb{x}_{n} \Big\|_{X} \leq C \Big( \max_{0 \leq n \leq N} |a_{n}| \Big) \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}.
  \end{equation*}
\end{thm}

\begin{proof}
  First suppose that $a_n \in \{-1,+1\}$ for all $n = 0, \ldots, N$.
  Then $(a_n \varepsilon_{n})_{n=0}^{N}$ is a Rademacher sequence, and since Rademacher averages do not depend on the choice of Rademacher sequence we have
  \begin{equation*}
    \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} a_{n} \mb{x}_{n} \Big\|_{X} = \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}.
  \end{equation*}
  Now for a general \emph{real} sequence $a = (a_n)_{n=0}^{N}$ with $-1 \leq a_n \leq 1$ for each $n$, note that the point $a \in \R^{N+1}$ lies in the hypercube with vertices
  \begin{equation*}
    V = \{v = (v_{0}, v_{1}\ldots, v_{N}) : v_{n} \in \{-1,1\} \quad \forall n \}.
  \end{equation*}
  Since the hypercube is the convex hull of its vertices, there exist numbers $(\lambda_{v})_{v \in V}$ such that
  \begin{equation*}
    a = \sum_{v \in V} \lambda_{v} v \qquad \text{and} \qquad \sum_{v \in V} \lambda_{v} = 1.
  \end{equation*}
  Thus we have
  \begin{equation*}
    \begin{aligned}
    \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} a_{n} \mb{x}_{n} \Big\|_{X}
    &= \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \big( \sum_{v \in V} \lambda_{v} v_{n} \big) \mb{x}_{n} \Big\|_{X} \\
    &\leq \sum_{v \in V} \lambda_{v} \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} v_{n} \mb{x}_{n} \Big\|_{X} \\
    &= \sum_{v \in V} \lambda_{v} \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} 
    = \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}
  \end{aligned}
\end{equation*}
using the equality for $\pm 1$-valued sequences.
The result for general real-valued sequences (with $C=1$) follows by scaling.
For complex Banach spaces and $\C$-valued sequences, the result (with $C=2$) follows by considering real and imaginary parts of the sequence $(a_n)_{n = 0}^{N}$ separately. 
\end{proof}

\begin{rmk}
  The proof shows that $C = 1$ suffices for real Banach spaces.
  For complex spaces the optimal constant is $C = \pi/2$, but this needs a more clever proof.
  See \cite[Proposition 3.2.10]{HNVW16}.
\end{rmk}

\section{The Kahane--Khintchine inequality and its consequences}

We formulated our Rademacher averages simply as the expectation of a Rademacher sum.
One could instead consider the $p$-th moments (or `$L^p$-averages')
\begin{equation*}
  \Big( \E \Big\| \sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}^{p} \Big)^{1/p} = \Big\| \sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)}.
\end{equation*}
It turns out that these are independent of $p$ up to a constant which is independent of $N$.

\begin{rmk}\label{rmk:HA-notation}
  From now on we will start using notation which is common in harmonic analysis: for two expressions $A$ and $B$ (representing nonnegative real numbers) we write
  \begin{equation*}
    A \lesssim B
  \end{equation*}
  to mean that there exists a constant $C < \infty$, independent of all quantities appearing in $A$ and $B$, such that
  \begin{equation*}
    A \leq CB.
  \end{equation*}
  Given a list of parameters $p_{1}, p_{2}, \ldots$ (which may appear in $A$ and $B$), we write
  \begin{equation*}
    A \lesssim_{p_{1}, p_{2}, \ldots} B
  \end{equation*}
  to mean that there is a constant $C = C(p_{1}, p_{2}, \ldots) < \infty$ \emph{which may depend on the listed parameters} such that $A \leq CB$.
  We write
  \begin{equation*}
    A \simeq B
  \end{equation*}
  to mean that $A \lesssim B$ and $B \lesssim A$, and we may also include a list of parameters in this notation.
  The usage of this notation is usually clear from context, and if you aren't used to it yet, you will be soon.
  This notation saves a lot of time, particularly when we don't care about the precise value of the constant $C$.
\end{rmk}

\begin{thm}[Kahane--Khintchine inequality]\label{thm:kk}
  Let $X$ be a Banach space and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega,\mc{A},\P)$.
  Then for all $p,q \in [1,\infty)$ there exists a finite constant $\kappa_{p,q}$ such that for all finite sequences $(\mb{x}_n)_{n=1}^N$ in $X$,
  \begin{equation*}
    \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} \leq \kappa_{p,q} \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
  \end{equation*}
  That is, for all $p \in [1,\infty)$, the $L^p$-norms of a Rademacher sum are pairwise equivalent.
  In the notation of Remark \ref{rmk:HA-notation}, we have
  \begin{equation*}
    \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} \simeq_{p,q} \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
  \end{equation*}
\end{thm}

\begin{rmk}
  This result actually holds for all $p,q \in (0,\infty)$, but we haven't defined Bochner spaces $L^p(\Omega;X)$ for $p < 1$, and for the purposes of this course we don't need to.
\end{rmk}

Since $\Omega$ is a probability space, H\"older's inequality yields the case $p \leq q$ with constant $\kappa_{p,q} = 1$, so the real difficulty is when $p > q$.

The Kahane--Khintchine inequality is so useful that it is often used without comment in the literature.
One well-known proof is a reduction to \emph{hypercontractivity of the heat semigroup on the discrete cube}, providing an interesting link with the analysis of Boolean functions (and of course this link goes much deeper).
We will follow \cite{HNVW16} and prove it via the \emph{John--Nirenberg theorem for adapted sequences}, which is quite useful in itself.
But before that, let's look at some consequences of Kahane--Khintchine.

\begin{prop}
  Let $X$ be a Banach space and consider a sequence $(\mb{x}_{n})_{n \in \N}$ in $X$.
  For all $p,q \in [1,\infty)$, the Rademacher sum
  \begin{equation}\label{eq:rad-sum-example}
    \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n}
  \end{equation}
  converges in $L^p(\Omega;X)$ if and only if it converges in $L^q(\Omega;X)$.
\end{prop}

\begin{proof}
  Suppose that the Rademacher sum converges in $L^q(\Omega;X)$.
  Convergence of the Rademacher sum in $L^p(\Omega;X)$ is equivalent to the limit
  \begin{equation*}
    \limsup_{N,M \to \infty} \Big\| \sum_{n=N}^{M} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} = 0.
  \end{equation*}
  But of course by Kahane--Khintchine we have
  \begin{equation*}
    \limsup_{N,M \to \infty} \Big\| \sum_{n=N}^{M} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)}
    \lesssim_{p,q} \limsup_{N,M \to \infty} \Big\| \sum_{n=N}^{M} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)} = 0.
  \end{equation*}
\end{proof}

When the Banach space under consideration is a Hilbert space, Rademacher averages are equivalent to $\ell^2$ sums.

\begin{thm}[Khintchine's inequality]\label{thm:khintchine}
  Let $H$ be a Hilbert space and $(\mb{h}_{n})_{n \in \N}$ a sequence in $H$.
  Then for all $p \in [1,\infty)$,
  \begin{equation*}
    \Big(\E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{h}_{n} \Big\|_{H}^{p} \Big)^{1/p} \simeq_{p} \|\mb{h}\|_{\ell^2(H)}.
  \end{equation*}
  In particular, for all scalar sequences $(a_{n})_{n \in \N}$,
  \begin{equation*}
    \Big( \E \Big| \sum_{n \in \N} \varepsilon_{n} a_{n} \Big|^{p} \Big)^{1/p}
    \simeq_{p} \Big( \sum_{n \in \N} |a_{n}|^{2} \Big)^{1/2}.
  \end{equation*}
\end{thm}

\begin{proof}
  Using Kahane--Khintchine, independence of the Rademacher variables, and $\E(\varepsilon_{n}^{2}) = 1$,
  \begin{equation*}
    \begin{aligned}
      \Big( \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{h}_{n} \Big\|_{H}^{p} \Big)^{2/p}
      &\simeq_{p} \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{h}_{n} \Big\|_{L^2(\Omega;H)}^{2} \\
      &=  \int_{\Omega} \big\langle \sum_{n \in \N} \varepsilon_{n}(\omega) \mb{h}_{n}, \sum_{m \in \N} \varepsilon_{m}(\omega) \mb{h}_{m} \big\rangle \, \dd \P(\omega) \\
      & =  \sum_{n,m \in \N} \E(\varepsilon_n \varepsilon_m) \langle \mb{h}_{n}, \mb{h}_{m} \rangle 
      =  \sum_{n \in \N} \langle \mb{h}_{n}, \mb{h}_{n} \rangle  = \|\mb{h}\|_{\ell^2(H)}^{2}.
    \end{aligned}
  \end{equation*}
\end{proof}

When the Banach space $X$ is a Lebesgue space, the Kahane--Khintchine inequality also lets us identify Rademacher averages with \emph{square functions}.

\begin{thm}
  Let $(S,\mc{A},\mu)$ be a $\sigma$-finite measure space and $p \in [1,\infty)$.
  Then for all sequences $(f_{n})_{n \in \N}$ of (scalar-valued) functions in $L^p(S)$,
  \begin{equation}\label{eq:KK-functions}
    \E \Big\| \sum_{n \in \N} \varepsilon_{n} f_{n} \Big\|_{L^p(S)}
    \simeq_{p} \Big\| \Big( \sum_{n \in \N} |f_{n}|^{2} \Big)^{1/2} \Big\|_{L^p(S)},
  \end{equation}
  where
  \begin{equation*}
    \Big( \sum_{n \in \N} |f_{n}|^{2} \Big)^{1/2}(s) := \Big( \sum_{n \in \N} |f_{n}(s)|^{2} \Big)^{1/2} \qquad \forall s \in S.
  \end{equation*}
  \end{thm}
  
\begin{proof}
  By Kahane--Khintchine, along with Fubini and Theorem \ref{thm:khintchine}, we have
  \begin{equation*}
    \begin{aligned}
      \Big(\E \Big\| \sum_{n \in \N} \varepsilon_{n} f_{n} \Big\|_{L^p(S)}\Big)^{p}
      &\simeq_{p} \E \Big\| \sum_{n \in \N} \varepsilon_{n} f_{n} \Big\|_{L^p(S)}^{p} \\
      &=  \int_{\Omega} \int_{S} \Big| \sum_{n \in \N} \varepsilon_{n}(\omega) f_{n}(s) \Big|^{p} \, \dd\mu(s) \, \dd\P(\omega)  \\
      &=  \int_{S} \int_{\Omega} \Big| \sum_{n \in \N} \varepsilon_{n}(\omega) f_{n}(s) \Big|^{p} \, \dd\P(\omega) \, \dd\mu(s)  \\
      &=  \int_{S} \E \Big| \sum_{n \in \N} \varepsilon_{n} f_{n}(s) \Big|^{p}  \, \dd\mu(s)  \\
      &\simeq_{p} \int_{S} \Big( \sum_{n \in \N} |f_{n}(s)|^{2} \Big)^{p/2}  \, \dd\mu(s) 
      = \Big\| \Big( \sum_{n \in \N} |f_{n}|^{2} \Big)^{1/2} \Big\|_{L^p(S)}^{p}.
    \end{aligned}
\end{equation*}
\end{proof}

\section{The John--Nirenberg inequality for stochastic processes}

Fix a Banach space $X$ and a probability space $(\Omega,\mc{A},\P)$.
Consider a filtration $\mc{A}_{\bullet}$ and an $X$-valued stochastic process $\mb{f}_{\bullet}$ adapted to $\mc{A}_{\bullet}$.
For all $q \in [1,\infty)$ we consider the following measure of the oscillation of $\mb{f}_{\bullet}$:
\begin{equation*}
  \|\mb{f}_{\bullet}\|_{*,q} := \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{A \in \mc{A}_k^{+}} \Big( \fint_{A} \|(\mb{f}_n - \mb{f}_{k-1})(\omega)\|_{X}^{q} \, \dd\P(\omega)\Big)^{1/q}
\end{equation*}
where $\mc{A}_{k}^{+} = \{A \in \mc{A}_{k} : \mu(A) > 0\}$.
This depends on the choice of $\mc{A}_{\bullet}$, but we do not reference this in the notation.
The \emph{John--Nirenberg inequality} says that this measure of oscillation is, up to a constant, independent of the choice of $q$.

\begin{thm}[John--Nirenberg inequality for stochastic processes]\label{thm:jn-adapted-sequences}
  For all $p,q \in [1,\infty)$ and for all $\mb{f}_{\bullet}$ as above,
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,p} \simeq_{p,q} \|\mb{f}_{\bullet}\|_{*,q}.
  \end{equation*}
\end{thm}

We will prove this through a series of lemmas, in which we obtain increasingly fine control on the oscillation of $\mb{f}_{\bullet}$.

\begin{lem}\label{lem:JN-proof-1}
  For all $k \leq n$, $A \in \mc{A}_k$, and $t > 0$,
  \begin{equation}\label{eq:JN-proof-1-est}
    \P(A \cap \{ \|\mb{f}_n - \mb{f}_{k-1}\|_{X} > t \}) \leq \Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} \P(A).
  \end{equation}
\end{lem}

\begin{proof}
  The left hand side of \eqref{eq:JN-proof-1-est} is bounded by
  \begin{equation*}
    \int_{A} \Big( \frac{\|\mb{f}_n - \mb{f}_{k-1}\|_{X}}{t} \Big)^{q} \, \dd\P \leq \Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} \P(A) 
  \end{equation*}
  since $A \in \mc{A}_k$ and $k \leq n$, using the definition of $\|\mb{f}_{\bullet}\|_{*,q}$.
\end{proof}

Next we show that the oscillation control established in the previous lemma extends to more general stopping times (that is, more general than the constant stopping time $n$).

\begin{lem}\label{lem:st-extension}
  Suppose that there exist $t > 0$ and $C > 0$ such that
  \begin{equation*}
    \P(A \cap \{ \| \mb{f}_n - \mb{f}_{k-1} \| > t \} ) \leq C \P(A) \qquad \forall k \leq n, A \in \mc{A}_k.
  \end{equation*}
  Then for all $k \in \N$, $A \in \mc{A}_k$, and all stopping times $T$ such that $T \geq k$ on $A$,
  \begin{equation}\label{eq:jn-stoptime}
    \P(A \cap \{T < \infty\} \cap \{ \| \mb{f}_{T} - \mb{f}_{k-1} \| > 2t \} ) \leq 2C \P(A).
  \end{equation}
\end{lem}

Here we use the (standard) notation $\mb{f}_{T}(\omega) := \mb{f}_{T(\omega)}(\omega)$.
Since \eqref{eq:jn-stoptime} involves the condition $T < \infty$, we don't need to worry about defining $\mb{f}_{\infty}$.

\begin{proof}
  Sum over all possible values of the stopping time:
  \begin{equation*}
      \P(A \cap \{T < \infty\} \cap \{ \| \mb{f}_{T} - \mb{f}_{k-1} \| > 2t \} ) 
      = \lim_{N \to \infty} \sum_{n = k}^{N} \P(A_n \cap \{ \| \mb{f}_{n} - \mb{f}_{k-1} \| > 2t \} ),
  \end{equation*}
  where $A_{n} := A \cap \{T = n\}$.
  For fixed $N > n > k$, since $A_n \in \mc{A}_n \subset \mc{A}_{n+1}$, we have by assumption
  \begin{equation*}
    \begin{aligned}
      &\P(A_n \cap \{ \| \mb{f}_{n} - \mb{f}_{k-1} \| > 2t \} ) \\
      &\leq \P(A_n \cap \{ \| \mb{f}_{n} - \mb{f}_{N} \| > t \} )
      + \P(A_n \cap \{ \| \mb{f}_{k-1} - \mb{f}_{N} \| > t \} ) \\
      &\leq C \P(A_n) + \P(A_n \cap \{ \| \mb{f}_{k-1} - \mb{f}_{N} \| > t \} ).
    \end{aligned}
  \end{equation*}
  Thus we have
  \begin{equation*}
    \begin{aligned}
      &\lim_{N \to \infty} \sum_{n = k}^{N} \P(A_n \cap \{ \| \mb{f}_{n} - \mb{f}_{k-1} \| > 2t \} ) \\
      &\leq \lim_{N \to \infty} \Big( C \sum_{n=k}^N \P(A_n) + \sum_{n=k}^N \P(A_n \cap \{ \| \mb{f}_{k-1} - \mb{f}_{N} \| > t \}) \Big) \\
      &\leq C \P(A) + \lim_{N \to \infty} \P(A \cap \{ \| \mb{f}_{k-1} - \mb{f}_{N} \| > t \})  
      \leq 2C\P(A)
    \end{aligned}
  \end{equation*}
  using the assumption and that $A \in \mc{A}_k$ in the last estimate.
\end{proof}
  
In the following lemma we make use of the \emph{process started at $k-1$}
\begin{equation*}
  {}^{k-1}\mb{f}_{\bullet} = (\mb{f}_n - \mb{f}_{k-1})_{n = k-1}^{\infty}
\end{equation*}
and its Doob maximal function
\begin{equation*}
  \mc{M}({}^{k-1}\mb{f}_{\bullet})(\omega) = \sup_{n \geq k-1} \|{}^{k-1}\mb{f}_n(\omega)\|_X = \sup_{n \geq k} \|(\mb{f}_n - \mb{f}_{k-1})(\omega)\|_X.
\end{equation*}

\begin{lem}\label{lem:jn-mf}
  Suppose that $\mb{f}_{\bullet}$ satisfies \eqref{eq:jn-stoptime} for some $t > 0$ and $C > 0$, for all $k \in \N$, $A \in \mc{A}_k$, and all stopping times $T$ such that $T \geq k$ on $A$.
  Then for all $s > 0$, all $k \in \N$, and all $A \in \mc{A}_{k}$,
  \begin{equation}\label{eq:jn-mf-eq}
    \P(A \cap \{ \mc{M}({}^{k-1} \mb{f}_{\bullet}) > s + 2t \}) \leq 2C \P(A \cap \{  \mc{M}({}^{k-1} \mb{f}_{\bullet}) > s \}).
  \end{equation}
\end{lem}

\begin{proof}
  Fix $k \in \N$ and consider the stopping times
  \begin{equation*}
    \begin{aligned}
      S &:= \inf\{n \geq k : \|\mb{f}_n - \mb{f}_{k-1}\| > s\}, \\
      T &:= \inf\{n \geq k : \|\mb{f}_n - \mb{f}_{k-1}\| > s + 2t\}.
    \end{aligned}
  \end{equation*}
  Then $k \leq S \leq T$, and \eqref{eq:jn-mf-eq} can be rewritten as
  \begin{equation*}
    \P(A \cap \{T < \infty\}) \leq 2C \P(A \cap \{S < \infty\}).
  \end{equation*}
  Now fix $n \geq k$ and let $A_n := A \cap \{S = n\} \in \mc{A}_n$. 
  On $A_n \cap \{T < \infty\}$ we have
  \begin{equation*}
    \|\mb{f}_{T} - \mb{f}_{n-1}\| \geq \|\mb{f}_{T} - \mb{f}_{k-1}\| - \|\mb{f}_{n-1} - \mb{f}_{k-1}\| > (s + 2t) - s = 2t,
  \end{equation*}
  so
  \begin{equation*}
    \P(A_n \cap \{T < \infty\}) = \P(A_n \cap \{T < \infty\} \cap \{\|\mb{f}_{T} - \mb{f}_{n-1}\| > 2t\} )
    \leq 2C \P(A_n)
  \end{equation*}
  by assumption, since $T$ is a stopping time such that $T \geq k$ on $A_{n}$.
  Summing over $n \geq k$ completes the proof.
\end{proof}

The following technical lemma lets us deduce $L^p$-norm estimates from super-level measure estimates of the form \eqref{eq:jn-mf-eq}.
It's useful in more general contexts, so we state it in a general form.

\begin{lem}\label{lem:jn-ms-lem}
  Let $(S,\mc{A},\mu)$ be a measure space, and suppose that $\map{f}{S}{\R}$ is a non-negative measurable function supported in a set $A \in \mc{A}$ of finite measure, with
  \begin{equation*}
    \mu(\{f > s+t\}) \leq c \mu(\{f > s\}) \qquad \forall s > 0
  \end{equation*}
  for some $c \in (0,1)$ and $t > 0$.
  Then for all $p \in [1,\infty)$,
  \begin{equation}\label{eq:jn-lem-est}
    \|f\|_{L^p(S)} \leq \frac{1 + c^{1/p}}{1 - c^{1/p}} t \mu(A)^{1/p}.
  \end{equation}
\end{lem}

\begin{proof}
  First we prove the result assuming that $\|f\|_{L^p(S)} < \infty$.
  We can compute
  \begin{equation*}
    \begin{aligned}
      \|f\|_{p}^{p} &= \int_{0}^{\infty} ps^{p-1} \mu(\{f > s\}) \, \dd s \\
      &\leq \int_{0}^{t} ps^{p-1}\mu(A) \, \dd s + \int_{0}^{\infty} p(s+t)^{p-1}\mu(\{f > s+t\}) \, \dd s \\
      &\leq t^{p}\mu(A) + \int_{0}^{\infty} p(s + t)^{p-1}c\mu(\{f > s\}) \, \dd s \\
      &\leq t^{p}\mu(A) + \int_{0}^{\infty} p(s + t)^{p-1}c\mu(\{f + t\1_{A} > s + t\}) \, \dd s \\
      &= t^{p}\mu(A) + c\|f + t\1_{A}\|_{p}^{p}.
    \end{aligned}
  \end{equation*}
  This yields
  \begin{equation*}
    \|f\|_{p} \leq t \mu(A)^{1/p} + c^{1/p}\|f\|_{p} + c^{1/p} t \mu(A)^{1/p},
  \end{equation*}
  which implies \eqref{eq:jn-lem-est} by a bit of arithmetic.

  To establish the result without the \emph{a priori} assumption that $\|f\|_{p} < \infty$, fix $r > 0$ and consider the function $g = \min(f,r)$.
  Then $g$ is bounded and has support of finite measure, so $\|g\|_{p} < \infty$.
  Furthermore, $g$ satisfies the hypothesis of the lemma (this is true, but annoying to check and write), so we have
  \begin{equation*}
    \|g\|_{p} \leq \frac{1 + c^{1/p}}{1 - c^{1/p}} t\mu(A)^{1/p}.
  \end{equation*}
  Taking the limit $r \to \infty$ and invoking monotone convergence implies the corresponding estimate for $f$. 
\end{proof}

Now we can put everything together to prove the John--Nirenberg inequality.

\begin{proof}[Proof of Theorem \ref{thm:jn-adapted-sequences}]
  We suppose that $\|\mb{f}_{\bullet}\|_{*,q} < \infty$ for otherwise there is nothing to prove.
  Lemma \ref{lem:JN-proof-1} tells us that
  \begin{equation}\label{eq:JN-proof-1-est-end}
    \P(A \cap \{ \|\mb{f}_n - \mb{f}_{k-1}\|_{X} > t \}) \leq \Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} \P(A)
  \end{equation}
  For all $k \leq n$, $A \in \mc{A}_k$, and $t > 0$.
  Lemma \ref{lem:st-extension} then extends this, saying that
  \begin{equation}\label{eq:jn-stoptime-end}
    \P(A \cap \{T < \infty\} \cap \{ \| \mb{f}_{T} - \mb{f}_{k-1} \| > 2t \} ) \leq 2\Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} \P(A)
  \end{equation}
  for all $t > 0$, $k \in \N$, $A \in \mc{A}_k$, and all stopping times $T$ such that $T \geq k$ on $A$.
  Lemma \ref{lem:jn-mf} gives us the maximal function estimate
  \begin{equation}\label{eq:jn-mf-eq-end}
    \P(A \cap \{ \mc{M}({}^{k-1} \mb{f}_{\bullet}) > s + 2t \}) \leq c(t) \P(A \cap \{  \mc{M}({}^{k-1} \mb{f}_{\bullet}) > s \}).
  \end{equation}
  for all $t > 0$, $s > 0$, $k \in \N$, and $A \in \mc{A}_{k}$, where
  \begin{equation*}
    c(t) := 2\Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} = 2t^{-q} \|\mb{f}_{\bullet}\|_{*,q}^{q}.
  \end{equation*}
  For $t > 2^{1/q} \|\mb{f}_{\bullet}\|_{*,q}$  we have $c(t) < 1$, and thus given $k \in \N$ and $A \in \mc{A}_{k}$, Lemma \ref{lem:jn-ms-lem} yields
  \begin{equation*}
    \|\1_{A} \mc{M}({}^{k-1} \mb{f}_{\bullet})\|_{L^p(A)} \leq \frac{1 + c(t)^{1/p}}{1 - c(t)^{1/p}} t \P(A)^{1/p}.
  \end{equation*}
  So we can estimate
  \begin{equation*}
    \begin{aligned}
      \|\mb{f}_{\bullet}\|_{*,p}
      &= \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{A \in \mc{A}_{k}^{+}} \Big( \fint_{A} \|\mb{f}_{n} - \mb{f}_{k-1} \|_{X}^{p} \, \dd\P \Big)^{1/p} \\
      &\leq \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{A \in \mc{A}_{k}^{+}} \Big( \fint_{A} \mc{M}({}^{k-1}\mb{f}_{\bullet})^{p} \, \dd\P \Big)^{1/p} \\
      &= \sup_{k \in \N} \sup_{A \in \mc{A}_{k}^{+}} \P(A)^{-1/p} \|\1_{A} \mc{M}({}^{k-1}\mb{f}_{\bullet}) \|_{L^p(A)} \\
      &\leq \frac{1 + c(t)^{1/p}}{1 - c(t)^{1/p}} t .
    \end{aligned}
  \end{equation*}
  Now it's just a matter of making a good choice of $t > 2^{1/q} \|\mb{f}_{\bullet}\|_{*,q}$.
  This can be optimised for a better constant, but we won't aim that high: we'll just take
  \begin{equation*}
    t_{0} := 2^{1 + \frac{1}{q}} \|\mb{f}_{\bullet}\|_{*,q}.
  \end{equation*}
  Then we have
  \begin{equation*}
    c(t_0) = 2(2^{1 + \frac{1}{q}} \|\mb{f}_{\bullet}\|_{*,q})^{-q} \|\mb{f}_{\bullet}\|_{*,q}^{q} = 2^{-q},
  \end{equation*}
  and thus
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,p} \leq \frac{1 + 2^{-q/p}}{1 - 2^{-q/p}} 2^{1 + \frac{1}{q}} \|\mb{f}_{\bullet}\|_{*,q} \simeq_{p,q} \|\mb{f}_{\bullet}\|_{*,q}.
  \end{equation*}
  By exchanging the roles of $p$ and $q$ we get
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,p} \simeq_{p,q} \|\mb{f}_{\bullet}\|_{*,q},
  \end{equation*}
  which was our goal from the beginning.
  
\end{proof}

\subsection{Proof of the Kahane--Khintchine inequality}

Now we return to the equivalence of $L^p$- and $L^q$-Rademacher averages:
\begin{equation*}
  \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} \simeq_{p,q} \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
\end{equation*}

\begin{proof}[Proof of Theorem \ref{thm:kk}]
  Consider the filtration $\mc{A}_{\bullet}$ generated by the process $\varepsilon_{\bullet}$, and the $\mc{A}_{\bullet}$-adapted sequence $\mb{f}_{\bullet}$ given by
  \begin{equation*}
    \mb{f}_{N} := \sum_{n=1}^N \varepsilon_n \mb{x}_{n}.
  \end{equation*}
  We will show that for all $q \in [1,\infty)$ we have
  \begin{equation}\label{eq:kk-claim}
    \Big\| \sum_{n=1}^N \varepsilon_n \mb{x}_n \Big\|_{L^q(\Omega;X)} = \|\mb{f}_{\bullet}\|_{*,q};
  \end{equation}
  once we have this, the John--Nirenberg inequality completes the proof.

  First recall that
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{A \in \mc{A}_k^{+}} \Big( \fint_{A} \|\mb{f}_n - \mb{f}_{k-1}\|_{X}^{q} \, \dd\P\Big)^{1/q}.
  \end{equation*}
  Fix $k \leq n$ $A \in \mc{A}_k^{+}$.
  We will compute
  \begin{equation*}
    \fint_{A} \|\1_{A} (\mb{f}_n - \mb{f}_{k-1})\|_{X}^q \, \dd\P = \P(A)^{-1} \E \Big( \1_{A} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big).
  \end{equation*}
  For all $\omega \in \Omega$,
  \begin{equation*}
    \Big\| \sum_{j=k}^{n} \varepsilon_j(\omega) \mb{x}_j\Big\|_{X} = \Big\| \varepsilon_k(\omega) \Big( \mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime}(\omega) \mb{x}_j \Big) \Big\|_{X} = \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X},
  \end{equation*}
  where
  \begin{equation*}
    \varepsilon_{j}^{\prime} :=
    \begin{cases}
      \varepsilon_{j} & j \leq k \\
      \varepsilon_{k} \varepsilon_{j} & j \geq k+1.
    \end{cases}
  \end{equation*}
  Now note that the $\sigma$-algebra $\mc{A}^\prime_{k+1,n} := \sigma(\{\varepsilon_j^\prime : k+1 \leq j \leq n\})$ is independent of $\mc{A}_k$, since for all $1 \leq j \leq k$ and $k+1 \leq j' \leq n$ we have by independence of the original Rademacher sequence
  \begin{equation*}
    \E(\varepsilon_j \varepsilon_{j'}^\prime) = \E(\varepsilon_j \varepsilon_k \varepsilon_{j'})
    =
    \begin{cases}
      \E(\varepsilon_j) \E(\varepsilon_k) \E(\varepsilon_{j'}) & \text{if $j < k$} \\
       \E(\varepsilon_{j'}) & \text{if $j = k$}
    \end{cases}
    \quad = 0.
  \end{equation*}
  Thus, since $A \in \mc{A}_k$, we have by independence of $\mc{A}_k$ and $\mc{A}^{\prime}_{k+1, n}$
  \begin{equation*}
    \begin{aligned}
      \P(A)^{-1} \E \Big( \1_{A} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big)
      &=  \P(A)^{-1} \E \Big( \1_{A} \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} \Big) \\
      &=  \E \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} 
      =  \E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}. \\
    \end{aligned}
  \end{equation*}
  Now letting $\mc{A}_{k,n} := \sigma(\{\varepsilon_j : k \leq j \leq n\})$, we have by the $L^q$-nonexpansiveness of conditional expectations
  \begin{equation*}
    \E \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
    = \E \Big\|\E^{\mc{A}_{k,n}} \Big(\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big)\Big\|_{X}^{q}
    \leq \E \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
  \end{equation*}
  with equality when $k=1$ and $n=N$.
  Thus
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}}  \Big(\E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q} = \Big(\E  \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q}
  \end{equation*}
  which proves the claimed equality \eqref{eq:kk-claim} and completes the proof. 
\end{proof}



\section{Type and cotype}

\begin{itemize}
\item definitions and examples
\item K-convexity: implied by UMD; implies type/cotype duality
\item STATE that K-convexity iff nontrivial type (Maurey--Pisier theorem is too hard for this course)
\end{itemize}

\section*{Exercises}

\begin{exercise}\label{ex:rad-sum-dist}
  Fix $N \in \N$, and let $(\varepsilon_{n})_{n=0}^{N}$ and $(\varepsilon_{n}')_{n=0}^{N}$ be two finite Rademacher sequences on probability spaces $\Omega$ and $\Omega'$ respectively.
  Let $(\mb{x}_{n})_{n=0}^{N}$ be a finite sequence in a Banach space $X$.
  Show that the Rademacher sums
  \begin{equation*}
    \sum_{n = 0}^{N} \varepsilon_n \mb{x}_{n} \colon \Omega \to X \qquad \text{and} \qquad
    \sum_{n = 0}^{N} \varepsilon_n' \mb{x}_{n} \colon \Omega' \to X
  \end{equation*}
  have the same distribution.
  (Hint: use Theorem \ref{thm:characteristic-function-uniqueness}.) 
\end{exercise}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
