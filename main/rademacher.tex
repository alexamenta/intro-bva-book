\subsection{Random sums and Rademacher spaces}

\begin{defn}
  A \emph{Rademacher variable} is a random variable $\map{\varepsilon}{\Omega}{\{-1,+1\}}$ defined on a probability space $(\Omega,\mc{A},\P)$ such that
  \begin{equation*}
    \P(\varepsilon = -1) = \P(\varepsilon = +1) = \frac{1}{2}.
  \end{equation*}
  A \emph{Rademacher sequence} is a sequence of mutually independent Rademacher variables $(\varepsilon_{n})_{n \in \N}$ on a probability space $(\Omega,\mc{A},\P)$.
\end{defn}

\begin{rmk}
  We will also consider Rademacher sequences $(\varepsilon_{\lambda})_{\lambda \in \Lambda}$ on different countable (or finite) indexing sets $\Lambda$.
\end{rmk}

When we discuss Rademacher sequences we generally discuss properties with only depend on the (joint) distributions of the random variables, rather than on the random variables themselves.
Thus we can exploit properties or intuition coming from different realisations of Rademacher sequences.
Two standard choices are:

\begin{example}[The probabilist's Rademachers]
  As in Example \ref{eg:gambling-filtrations}, consider the probability space
  \begin{equation*}
    \Omega := \prod_{n \in \N} \{-1,1\} = \{-1,1\}^{\N}
  \end{equation*}
  with the product $\sigma$-algebra and measure, where the factors are equipped with the uniform probability measure, and let $\map{\pi_{n}}{\Omega}{\{-1,1\}}$ be the $n$-th coordinate function.
  Then $(\pi_{n})_{n \in \N}$ is a Rademacher sequence.
\end{example}

\begin{example}[The analyst's Rademachers]
  Let $\Omega = [0,1]$ with the Lebesgue measure and Borel $\sigma$-algebra, and consider the \emph{Rademacher functions}
  \begin{equation*}
    r_n(t) := \sgn(\sin(2^n\pi t)) \qquad \forall t \in [0,1], n = 1,2,\cdots.
  \end{equation*}
  These are square waves with period $2^{-n}$, and form a Rademacher sequence.
\end{example}

A \emph{finite Rademacher sum} in a Banach space $X$ is an $X$-valued random variable of the form
\begin{equation}\label{eq:rademacher-sum}
  \sum_{n = 0}^{N} \varepsilon_n \mb{x}_{n} \colon \Omega \to X,
\end{equation}
where $(\mb{x}_{n})_{n = 0}^{N}$ is a finite sequence of vectors in $X$ and $(\varepsilon_{n})_{n = 0}^{N}$ is a finite Rademacher sequence on $\Omega$.
Although the random variable in \eqref{eq:rademacher-sum} depends on the choice of Rademacher sequence, its distribution is independent of this choice (Exercise \ref{ex:rad-sum-dist}).\footnote{Recall that the distribution of a random variable $\map{f}{\Omega}{X}$ is the pushforward measure $f_{*}(\P)$ on $X$, given by $f_{*}(\P)(A) := \P(f^{-1}(A))$ for all Borel sets $A \subset X$.}
As a consequence, the \emph{(finite) Rademacher average}
\begin{equation*}
  \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} = \int_{\Omega}  \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega) \mb{x}_{n} \Big\|_{X} \, \dd\P(\omega)
\end{equation*}
is independent of the choice of Rademacher sequence.

Rademacher averages show up \emph{all the time} in Banach-valued analysis, so we better get used to them.
There are two fundamental properties that they satisfy: the \emph{contraction principle} (to be proven in a moment), and the \emph{Kahane--Khintchine inequalities} (the subject of the next section).

\begin{thm}[Contraction principle]
  There exists a constant $C \leq 2$ with the following property: for all Banach spaces $X$, all $N \in \N$, and all finite sequences $(\mb{x}_{n})_{n = 0}^{N}$ in $X$ and $(a_{n})_{n=0}^{N}$ in the scalar field $\K$,
  \begin{equation*}
    \E \Big\| \sum_{n \in \N} \varepsilon_{n} a_{n} \mb{x}_{n} \Big\|_{X} \leq C \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}
  \end{equation*}
\end{thm}

\begin{proof}
  First suppose that $a_n \in \{-1,+1\}$ for all $n = 0, \ldots, N$.
  Then $(a_n \varepsilon_{n})_{n=0}^{N}$ is a Rademacher sequence, and since Rademacher averages do not depend on the choice of Rademacher sequence we have
  \begin{equation*}
    \E \Big\| \sum_{n \in \N} \varepsilon_{n} a_{n} \mb{x}_{n} \Big\|_{X} = \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}.
  \end{equation*}
  Now for a general \emph{real} sequence $a = (a_n)_{n=0}^{N}$ with $-1 \leq a_n \leq 1$ for each $n$, note that the point $a \in \R^{N+1}$ lies in the (solid) hypercube with vertices
  \begin{equation*}
    V = \{(v_{0}, v_{1}\ldots, v_{N}) : v_{i} \in \{-1,+1\} \quad \forall i\}.
  \end{equation*}
  Since the solid hypercube is the convex hull of its vertices, there exist numbers $(\lambda_{v})_{v \in V}$ such that
  \begin{equation*}
    a = \sum_{v \in V} \lambda_{v} v \qquad \text{and} \qquad \sum_{v \in V} \lambda_{v} = 1.
  \end{equation*}
  Thus we have
  \begin{equation*}
    \begin{aligned}
    \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} a_{n} \mb{x}_{n} \Big\|_{X}
    &= \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \big( \sum_{v \in V} \lambda_{v} v_{n} \big) \mb{x}_{n} \Big\|_{X} \\
    &\leq \sum_{v \in V} \lambda_{v} \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} v_{n} \mb{x}_{n} \Big\|_{X} \\
    &= \sum_{v \in V} \lambda_{v} \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} 
    = \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}
  \end{aligned}
\end{equation*}
using the equality for $\pm 1$-valued sequences.
The result for general real-valued sequences (with $C=1$) follows by scaling.
For complex Banach spaces and $\C$-valued sequences, the result (with $C=2$) follows by considering real and imaginary parts of the sequence $(a_n)_{n = 0}^{N}$ separately. 
\end{proof}

\begin{rmk}
  The proof shows that $C = 1$ suffices for real Banach spaces.
  For complex spaces the optimal constant is $C = \pi/2$, but this needs a different proof.
  See \cite[Proposition 3.2.10]{HNVW16}.
\end{rmk}

\subsection{The Kahane--Khintchine and John--Nirenberg inequalities}

We formulated our Rademacher averages simply as the expectation of a Rademacher sum.
One could instead consider the $p$-th moments (or `$L^p$-averages')
\begin{equation*}
  \Big( \E \Big\| \sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}^{p} \Big)^{1/p} = \Big\| \sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)}.
\end{equation*}
It turns out that these are independent of $p$ up to a constant \emph{uniformly in $N$}.

\begin{thm}[Kahane--Khintchine inequalitiy]\label{thm:kk}
  Let $X$ be a Banach space and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega,\mc{F},\P)$.
  Then for all $p,q \in (0,\infty)$ there exists a finite constant $\kappa_{p,q}$ such that for all finite sequences $(\mb{x}_n)_{n=1}^N$ in $X$,
  \begin{equation*}
    \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} \leq \kappa_{p,q} \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
  \end{equation*}
  That is, for all $p \in (0,\infty)$, the $L^p$-norms of a Rademacher sum are pairwise equivalent.  
\end{thm}

Since $\Omega$ is a probability space, H\"older's inequality yields the case $p \leq q$ with constant $\kappa_{p,q} = 1$, so the real difficulty is when $p > q$.

The Kahane--Khintchine inequality is so useful that they are generally used without comment in the literature.
One well-known proof is a reduction to \emph{hypercontractivity of the heat semigroup on the discrete cube}, providing an interesting link with the analysis of Boolean functions (and of course this link goes much deeper).
We will follow \cite{HNVW16} and prove it via the \emph{John--Nirenberg theorem for adapted sequences}, which is quite useful in itself.

Fix a Banach space $X$ and a probability space $(\Omega,\mc{A},\P)$.
Consider a filtration $(\mc{A}_n)_{n \in \N}$ and an adapted $X$-valued stochastic process $f = (f_n)_{n \in \N}$ (i.e. $f_n$ is $\mc{A}_n$-measurable for all $n \in \N$).
For all $q \in (0,\infty)$ we consider the following measure of the oscillation of $f$:
\begin{equation*}
    \|f\|_{*,q} := \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{\substack{A \in \mc{A}_k \\ 0 < \mu(A) < \infty}} \Big( \fint_{A} \|(f_n - f_{k-1})(\omega)\|_{X}^{q} \, \dd\P(\omega)\Big)^{1/q}
\end{equation*}

\begin{thm}[John--Nirenberg inequality for adapted sequences]\label{thm:jn-adapted-sequences}
  For all $p,q \in (0,\infty)$ there exists a finite constant $c_{p,q}$ such that, for all $f$ as above,
  \begin{equation*}
    \|f\|_{*,p} \leq c_{p,q} \|f\|_{*,q}.
  \end{equation*}
\end{thm}

We will prove this as a consequence of a series of lemmas, but before that, let's derive the Kahane--Khintchine inequality from it.

\begin{proof}[Proof of Theorem \ref{thm:kk}, assuming the John--Nirenberg inequality]
  Consider the filtration and adapted sequence\todo{have to unify all the notation... UP TO HERE}
  \begin{equation*}
    \mc{F}_n := \sigma(\{\varepsilon_j : 1 \leq j \leq n\}), \qquad \phi_n := \sum_{j=1}^n \varepsilon_j \mb{x}_j.
  \end{equation*}
  We claim that for all $q \in [1,\infty)$ we have
  \begin{equation}\label{eq:kk-claim}
    \|\phi\|_{*,q} = \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^q(\Omega;X)}.
  \end{equation}
  Assuming this for the moment, the John--Nirenberg inequality yields a finite constant $c_{p,q}$ such that
  \begin{equation*}
    \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^p(\Omega;X)}
    = \|\phi\|_{*,p}
    \leq c_{p,q} \|\phi\|_{*,q}
    = \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^q(\Omega;X)}
  \end{equation*}
  whenever $1 \leq q < p$.
  If $q < 1 \leq p$, we fix $\theta \in (0,1)$ so that $1/p = \theta/q + (1-\theta)/2p$, and log-convexity of $L^p$-norms gives
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)}
    \leq \| \phi_N \|_{L^q(\Omega;X)}^{\theta} \| \phi_N \|_{L^{2p}(\Omega;X)}^{1 - \theta}
    \leq \| \phi_N \|_{L^q(\Omega;X)}^{\theta} (c_{2p,p} \| \phi_N \|_{L^{p}(\Omega;X)})^{1 - \theta},
  \end{equation*}
  which yields
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)} \leq c_{2p,p}^{(1 - \theta)/\theta} \| \phi_N \|_{L^q(\Omega;X)}.
  \end{equation*}
  Finally, if $q < p < 1$, we simply estimate
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)} \leq \| \phi_N \|_{L^1(\Omega;X)} \leq c_{1,q} \| \phi_N \|_{L^q(\Omega;X)}.
  \end{equation*}
  This covers all nontrivial cases, and it remains to prove the claimed equality \eqref{eq:kk-claim}.

  First recall that
  \begin{equation*}
    \|\phi\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{\substack{F \in \mc{F}_k \\ \mu(F) > 0}} \Big( \fint_{F} \|(\phi_n - \phi_{k-1})(s)\|_{X}^{q} \, \dd\P(s)\Big)^{1/q}.
  \end{equation*}
  Fix $k \leq n$ and a set $F \in \mc{F}_k$ of positive measure.
  We will compute
  \begin{equation*}
    \fint_{F} \|\1_{F} (\phi_n - \phi_{k-1})(s)\|_{X}^q \, \dd\P(s) = \P(F)^{-1} \E \Big( \1_{F} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big).
  \end{equation*}
  Observe that for all $\omega \in \Omega$
  \begin{equation*}
    \Big\| \sum_{j=k}^{n} \varepsilon_j(\omega) \mb{x}_j\Big\|_{X} = \Big\| \varepsilon_k(\omega) \Big( \mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime}(\omega) \mb{x}_j \Big) \Big\|_{X} = \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}
  \end{equation*}
  where
  \begin{equation*}
    \varepsilon_{j}^{\prime} :=
    \begin{cases}
      \varepsilon_{j} & j \leq k \\
      \varepsilon_{k} \varepsilon_{j} & k+1 \leq j,
    \end{cases}
  \end{equation*}
  since $\varepsilon_j$ is $\pm 1$-valued.
  Now note that the $\sigma$-algebra $\mc{F}^\prime_{k+1,n} := \sigma(\{\varepsilon_j^\prime : k+1 \leq j \leq n\})$ is independent of $\mc{F}_k$, since for all $1 \leq j \leq k$ and $k+1 \leq j' \leq n$ we have by independence of the original Rademacher sequence
  \begin{equation*}
    \E(\varepsilon_j \varepsilon_{j'}^\prime) = \E(\varepsilon_j \varepsilon_k \varepsilon_{j'})
    =
    \begin{cases}
      \E(\varepsilon_j) \E(\varepsilon_k) \E(\varepsilon_{j'}) & \text{if $j < k$} \\
       \E(\varepsilon_{j'}) & \text{if $j = k$}
    \end{cases}
    \quad = 0.
  \end{equation*}
  Thus, since $F \in \mc{F}_k$, we have by independence of $\mc{F}_k$ and $\mc{F}^{\prime}_{k+1, n}$
  \begin{equation*}
    \begin{aligned}
      \P(F)^{-1} \E \Big( \1_{F} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big)
      &=  \P(F)^{-1} \E \Big( \1_{F} \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} \Big) \\
      &=  \E \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} 
      =  \E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}. \\
    \end{aligned}
  \end{equation*}
  Now letting $\mc{F}_{k,n} := \sigma(\{\varepsilon_j : k \leq j \leq n\})$, we have by the $L^q$-contraction property of conditional expectations
  \begin{equation*}
    \E \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
    = \E \Big\|\E^{\mc{F}_{k,n}} \Big(\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big)\Big\|_{X}^{q}
    \leq \E \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
  \end{equation*}
  with equality when $k=1$ and $n=N$.
  Thus
  \begin{equation*}
    \|\phi\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}}  \Big(\E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q} = \Big(\E  \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q}
  \end{equation*}
  which proves the claimed equality \eqref{eq:kk-claim} and completes the proof. 
\end{proof}

In the setting of Hilbert-valued functions (and in particular, scalar-valued functions), the Kahane--Khintchine inequality leads to the classical Khintchine inequalities.

\begin{cor}[Khintchine's inequalities]
  Let $H$ be a Hilbert space, and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega, \mc{F}, \P)$.
  Then for all $p \in (0,\infty)$ there exist finite constants $A_p$ and $B_p$ such that for all finite sequences $(\mb{h}_n)_{n=1}^N$ in $H$,
  \begin{equation*}
    A_p \Big( \sum_{n=1}^N \|\mb{h}_n\|_H^2 \Big)^{1/2} \leq \Big\| \sum_{n=1}^N \varepsilon_n \mb{h}_n \Big\|_{L^p(\Omega;H)} \leq B_p \Big( \sum_{n=1}^N \|\mb{h}_n\|_H^2 \Big)^{1/2}.
  \end{equation*}
\end{cor}

\begin{proof}
  By independence of the Rademacher variables we have
  \begin{equation}
    \begin{aligned}
      \Big\| \sum_{n=1}^N \varepsilon_n \mb{h}_n \Big\|_{L^2(\Omega;H)}^2
      &= \Big\langle \sum_{n=1}^N \varepsilon_n \mb{h}_n , \sum_{m=1}^N \varepsilon_m \mb{h}_m \Big\rangle \\
      &= \sum_{n,m = 1}^N \E(\varepsilon_n \varepsilon_m) \langle \mb{h}_n, \mb{h}_m \rangle 
      = \sum_{n=1}^N \|\mb{h}_{n}\|_{H}^2,
    \end{aligned}
  \end{equation}
  so the result is true for $p=2$ with $A_2 = B_2 = 1$.
  Now use Kahane--Khintchine to extend the result to general $p \in (0,\infty)$.
\end{proof}

\subsubsection{Proof of the John--Nirenberg inequality}

We return to our analysis of a sequence $\phi = (\phi_n)_{n \in \N}$ of $X$-valued functions adapted to a filtration $(\mc{F}_n)_{n \in \N}$ on a $\sigma$-finite measure space $(S,\mc{A},\mu)$.
We prove the John--Nirenberg inequality via a series of lemmas in which we obtain increasingly fine control on the oscillation of $\phi$.

\begin{lem}\label{lem:JN-proof-1}
  For all $k \leq n$, $F \in \mc{F}_k$, and $\alpha > 0$,
  \begin{equation}\label{eq:JN-proof-1-est}
    \mu(F \cap \{ \|\phi_n - \phi_{k-1}\|_{X} > \alpha \}) \leq \Big( \frac{\|\phi\|_{*,q}}{\alpha} \Big)^{q} \mu(F).
  \end{equation}
\end{lem}

\begin{proof}
  We can assume that $0 < \mu(F) < \infty$, otherwise there is nothing to prove.
  The left hand side of \eqref{eq:JN-proof-1-est} is bounded by
  \begin{equation*}
    \int_{F} \Big( \frac{\|\phi_n - \phi_{k-1}\|_{X}}{\alpha} \Big)^{q} \, \dd\mu \leq \mu(F) \Big( \frac{\|\phi\|_{*,q}}{\alpha} \Big)^{q}
  \end{equation*}
  since $F \in \mc{F}_k$ and $k \leq n$, by the definition of $\|\phi\|_{*,q}$.
\end{proof}

Next we show that oscillation control of the form above extends to more general stopping times.

\begin{lem}
  Suppose that there exist $\alpha > 0$ and $\eta > 0$ such that
  \begin{equation*}
    \mu(F \cap \{ \| \phi_n - \phi_{k-1} \| > \alpha \} ) \leq \eta \mu(F) \qquad \forall k \leq n, F \in \mc{F}_k.
  \end{equation*}
  Then for all $k \in \N$, $F \in \mc{F}_k$, and all stopping times $\nu$ such that $\nu \geq k$ on $F$,
  \begin{equation}\label{eq:jn-stoptime}
    \mu(F \cap \{\nu < \infty\} \cap \{ \| \phi_{\nu} - \phi_{k-1} \| > 2\alpha \} ) \leq 2\eta \mu(F).
  \end{equation}
\end{lem}

\begin{proof}
  Sum over all possible values of the stopping time:
  \begin{equation*}
      \mu(F \cap \{\nu < \infty\} \cap \{ \| \phi_{\nu} - \phi_{k-1} \| > 2\alpha \} ) 
      = \lim_{N \to \infty} \sum_{n = k}^{N} \mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ),
  \end{equation*}
  where $F_{n} := F \cap \{\nu = n\}$.
  For fixed $N \geq n > k$, since $F_n \in \mc{F}_n \subset \mc{F}_{n+1}$, we have by assumption
  \begin{equation*}
    \begin{aligned}
      &\mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ) \\
      &\leq \mu(F_n \cap \{ \| \phi_{n} - \phi_{N} \| > \alpha \} )
      + \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \} ) \\
      &\leq \eta \mu(F_n) + \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \} ).
    \end{aligned}
  \end{equation*}
  Thus we have
  \begin{equation*}
    \begin{aligned}
      &\lim_{N \to \infty} \sum_{n = k}^{N} \mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ) \\
      &\leq \lim_{N \to \infty} \Big( \eta \sum_{n=k}^N \mu(F_n) + \sum_{n=k}^N \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \}) \Big) \\
      &\leq \eta \mu(F) + \lim_{N \to \infty} \mu(F \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \})  
      \leq 2\eta\mu(F)
    \end{aligned}
  \end{equation*}
  using the assumption and $F \in \mc{F}_k$ in the last estimate.
\end{proof}
  
In the following lemma we make use of the \emph{started sequence}
\begin{equation*}
  {}^{k-1}\phi = (\phi_n - \phi_{k-1})_{n \geq k-1}
\end{equation*}
and its maximal function
\begin{equation*}
  ({}^{k-1} \phi)^{*}(s) := \sup_{n \geq k-1} \|({}^{k-1}\phi)_n(s)\|_X = \sup_{n \geq k} \|(\phi_n - \phi_{k-1})(s)\|_X.
\end{equation*}

\begin{lem}\label{lem:jn-mf}
  Suppose that $\phi$ satisfies \eqref{eq:jn-stoptime} for all $k \in \N$, $F \in \mc{F}_k$, and all stopping times $\nu$ such that $\nu \geq k$ on $F$.
  Then for all $\lambda > 0$,
  \begin{equation}\label{eq:jn-mf-eq}
    \mu(F \cap \{ ({}^{k-1} \phi)^{*} > \lambda + 2\alpha \}) \leq 2\eta \mu(F \cap \{  ({}^{k-1} \phi)^{*} > \lambda \}) \qquad \forall k \in \N, F \in \mc{F}_{k}.
  \end{equation}
\end{lem}

\begin{proof}
  Fix $k \in \N$ and consider the stopping times
  \begin{equation*}
    \begin{aligned}
      \rho &:= \inf\{n \geq k : \|\phi_n - \phi_{k-1}\| > \lambda\}, \\
      \nu &:= \inf\{n \geq k : \|\phi_n - \phi_{k-1}\| > \lambda + 2\alpha\}.
    \end{aligned}
  \end{equation*}
  Then $k \leq \rho \leq \nu$, and \eqref{eq:jn-mf-eq} can be rewritten as
  \begin{equation*}
    \mu(F \cap \{\nu < \infty\}) \leq 2\eta \mu(F \cap \{\rho < \infty\}).
  \end{equation*}
  Now fix $n \geq k$ and let $F_n := F \cap \{\rho = n\} \in \mc{F}_n$.
  On $\{F_n \cap \{\nu < \infty\}\}$ we have
  \begin{equation*}
    \|\phi_{\nu} - \phi_{n-1}\| \geq \|\phi_{\nu} - \phi_{k-1}\| - \|\phi_{n-1} - \phi_{k-1}\| > (\lambda + 2\alpha) - \lambda = 2\alpha,
  \end{equation*}
  so
  \begin{equation*}
    \mu(F_n \cap \{\nu < \infty\}) = \mu(F_n \cap \{\nu < \infty\} \cap \{\|\phi_{\nu} - \phi_{n-1}\| > 2\alpha\} )
    \leq 2\eta \mu(F_n).
  \end{equation*}
  Summing over $n \geq k$ completes the proof.
\end{proof}

\begin{lem}
  Suppose that $f$ is a non-negative function supported in $F \in \mc{A}$, satisfying
  \begin{equation*}
    \mu(f > \lambda + \alpha) \leq \eta \mu(f > \lambda) \qquad \forall \lambda > 0
  \end{equation*}
  for some $\eta \in (0,1)$ and $\alpha > 0$.
  Then for all $p \in [1,\infty)$,
  \begin{equation*}
    \|f\|_p \leq \frac{1 + \eta^{1/p}}{1 - \eta^{1/p}} \alpha \mu(F)^{1/p}.
  \end{equation*}
\end{lem}

\begin{proof}
  {\color{red} WRITE PROOF}
\end{proof}

\todo{UP TO HERE. HAVE TO COMPLETE PROOF OF JOHN-NIRENBERG.}

{\color{blue}

\begin{equation*}
  \|\phi\|_{**,q} := \sup_{k \in \N} \sup_{\substack{F \in \mc{F}_k \\ 0 < \mu(F) < \infty}} \Big( \fint_{F} ({}^{k-1} \phi)^{*}(s)^q \, \dd\mu(s) \Big)^{1/q},
\end{equation*}

}




\subsection{Type and cotype}

\begin{itemize}
\item definitions and examples
\item K-convexity: implied by UMD; implies type/cotype duality
\item STATE that K-convexity iff nontrivial type (Maurey--Pisier theorem is too hard for this course)
\end{itemize}

\subsection{\texorpdfstring{$R$-boundedness of sets of operators}{R-boundedness of sets of operators}}

\subsection*{Exercises}

\begin{exercise}\label{ex:rad-sum-dist}
  Fix $N \in \N$, and let $(\varepsilon_{n})_{n=0}^{N}$ and $(\varepsilon_{n}')_{n=0}^{N}$ be two finite Rademacher sequences on probability spaces $\Omega$ and $\Omega'$ respectively.
  Let $(\mb{x}_{n})_{n=0}^{N}$ be a finite sequence in a Banach space $X$.
  Show that the Rademacher sums
  \begin{equation*}
    \sum_{n = 0}^{N} \varepsilon_n \mb{x}_{n} \colon \Omega \to X \qquad \text{and} \qquad
    \sum_{n = 0}^{N} \varepsilon_n' \mb{x}_{n} \colon \Omega' \to X
  \end{equation*}
  have the same distribution.
  (Hint: use Theorem \ref{thm:characteristic-function-uniqueness}.) 
\end{exercise}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
