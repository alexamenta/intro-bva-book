An important role in Banach-valued analysis is played by \emph{random sums} of various forms, the most common being \emph{Rademacher sums}.
Given a Banach space $X$, these can be used to define the \emph{Rademacher space} $\varepsilon(X)$, which is a space of $X$-valued functions that behaves like a `nicer' version of $\ell^{2}(\N;X)$.
If $X$ is not isomorphic to a Hilbert space, then (as we will eventually find out) neither is $\ell^{2}(\N;X)$.
For that matter, neither is the Rademacher space $\varepsilon(X)$, but this space is more amenable to generalisations of orthogonality arguments.
It is central in the formulation of \emph{square function estimates}, which arise often in harmonic analysis (e.g. Littlewood--Paley theory) and probability (e.g. Burkholder--Davis--Gundy estimates for martingales).
Before discussing these applications, we need to actually talk about Rademacher sums.

\section{Random sums and averages}

\begin{defn}
  A \emph{Rademacher variable}\index{Rademacher variable}\index{random variable!Rademacher} is a random variable $\map{\varepsilon}{\Omega}{\{-1, 1\}}$ defined on a probability space $(\Omega,\mc{A},\P)$ such that
  \begin{equation*}
    \P(\varepsilon = -1) = \P(\varepsilon = +1) = \frac{1}{2}.
  \end{equation*}
  A \emph{Rademacher sequence}\index{Rademacher sequence} is a sequence of mutually independent Rademacher variables $(\varepsilon_{n})_{n \in \N}$ on a probability space $(\Omega,\mc{A},\P)$.
\end{defn}

\begin{rmk}
  We will also consider Rademacher sequences $(\varepsilon_{\lambda})_{\lambda \in \Lambda}$ on different countable (or finite) indexing sets $\Lambda$.
  The definition is exactly the same.
\end{rmk}

When we discuss Rademacher sequences we generally discuss properties depending on the joint \emph{distributions} of the Rademacher variables, rather than on the Rademacher variables themselves.
Thus we can exploit properties or intuition coming from different realisations of Rademacher sequences.
Two standard choices are:

\begin{example}[The probabilist's Rademachers]
  As in Example \ref{eg:gambling-filtrations}, consider the probability space
  \begin{equation*}
    \Omega := \prod_{n \in \N} \{-1,1\} = \{-1,1\}^{\N}
  \end{equation*}
  with the product $\sigma$-algebra and measure, where the factors are equipped with the uniform probability measure, and let $\map{\pi_{n}}{\Omega}{\{-1,1\}}$ be the $n$-th coordinate function.
  Then $(\pi_{n})_{n \in \N}$ is a Rademacher sequence.
\end{example}

\begin{example}[The analyst's Rademachers]
  Let $\Omega = [0,1]$ with the Lebesgue measure and Borel $\sigma$-algebra, and consider the \emph{Rademacher functions}
  \begin{equation*}
    r_n(t) := \sgn(\sin(2^n\pi t)) \qquad \forall t \in [0,1], n = 1,2,\cdots.
  \end{equation*}
  These are square waves with period $2^{-n}$, and form a Rademacher sequence.
\end{example}

A \emph{(finite) Rademacher sum} in a Banach space $X$ is an $X$-valued random variable of the form
\begin{equation}\label{eq:rademacher-sum}
  \sum_{n = 0}^{N} \varepsilon_n \mb{x}_{n} \colon \Omega \to X,
\end{equation}
where $(\mb{x}_{n})_{n = 0}^{N}$ is a finite sequence of vectors in $X$ and $(\varepsilon_{n})_{n = 0}^{N}$ is a finite Rademacher sequence on $\Omega$.
Although the random variable in \eqref{eq:rademacher-sum} depends on the choice of Rademacher sequence, its distribution is independent of this choice (Exercise \ref{ex:rad-sum-dist}).\footnote{Recall that the distribution of a random variable $\map{f}{\Omega}{X}$ is the pushforward measure $f_{*}(\P)$ on $X$, given by $f_{*}(\P)(A) := \P(f^{-1}(A))$ for all Borel sets $A \subset X$.}
As a consequence, the \emph{(finite) Rademacher average}\index{Rademacher averages}
\begin{equation*}
  \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} = \int_{\Omega}  \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega) \mb{x}_{n} \Big\|_{X} \, \dd\P(\omega)
\end{equation*}
is independent of the choice of Rademacher sequence.

Rademacher averages show up \emph{all the time} in Banach-valued analysis, so we better get used to them.
There are two fundamental properties that they satisfy: the \emph{contraction principle} (to be proven in a moment), and the \emph{Kahane--Khintchine inequalities} (the subject of the next section).

\begin{thm}[Contraction principle]\label{thm:contraction}\index{Contraction principle}
  There exists a constant $C \leq 2$ with the following property: for all Banach spaces $X$, all $N \in \N$, and all finite sequences $(\mb{x}_{n})_{n = 0}^{N}$ in $X$ and $(a_{n})_{n=0}^{N}$ in the scalar field $\K$,
  \begin{equation*}
    \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} a_{n} \mb{x}_{n} \Big\|_{X} \leq C \Big( \max_{0 \leq n \leq N} |a_{n}| \Big) \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}.
  \end{equation*}
\end{thm}

\begin{proof}
  First suppose that $a_n \in \{-1,+1\}$ for all $n = 0, \ldots, N$.
  Then $(a_n \varepsilon_{n})_{n=0}^{N}$ is a Rademacher sequence, and since Rademacher averages do not depend on the choice of Rademacher sequence we have
  \begin{equation*}
    \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} a_{n} \mb{x}_{n} \Big\|_{X} = \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}.
  \end{equation*}
  Now for a general \emph{real} sequence $a = (a_n)_{n=0}^{N}$ with $-1 \leq a_n \leq 1$ for each $n$, note that the point $a \in \R^{N+1}$ lies in the hypercube with vertices
  \begin{equation*}
    V = \{v = (v_{0}, v_{1}\ldots, v_{N}) : v_{n} \in \{-1,1\} \quad \forall n \}.
  \end{equation*}
  Since the hypercube is the convex hull of its vertices, there exist numbers $(\lambda_{v})_{v \in V}$ such that
  \begin{equation*}
    a = \sum_{v \in V} \lambda_{v} v \qquad \text{and} \qquad \sum_{v \in V} \lambda_{v} = 1.
  \end{equation*}
  Thus we have
  \begin{equation*}
    \begin{aligned}
    \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} a_{n} \mb{x}_{n} \Big\|_{X}
    &= \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \big( \sum_{v \in V} \lambda_{v} v_{n} \big) \mb{x}_{n} \Big\|_{X} \\
    &\leq \sum_{v \in V} \lambda_{v} \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} v_{n} \mb{x}_{n} \Big\|_{X} \\
    &= \sum_{v \in V} \lambda_{v} \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} 
    = \E \Big\| \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}
  \end{aligned}
\end{equation*}
using the equality for $\pm 1$-valued sequences.
The result for general real-valued sequences (with $C=1$) follows by scaling.
For complex Banach spaces and $\C$-valued sequences, the result (with $C=2$) follows by considering real and imaginary parts of the sequence $(a_n)_{n = 0}^{N}$ separately. 
\end{proof}

\begin{rmk}
  The proof shows that $C = 1$ suffices for real Banach spaces.
  For complex spaces the optimal constant is $C = \pi/2$, but this needs a more clever proof.
  See \cite[Proposition 3.2.10]{HNVW16}.
\end{rmk}

\section{The Kahane--Khintchine inequality and its consequences}

We formulated our Rademacher averages simply as the expectation of a Rademacher sum.
One could instead consider the $p$-th moments (or `$L^p$-averages')
\begin{equation*}
  \Big( \E \Big\| \sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}^{p} \Big)^{1/p} = \Big\| \sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)}.
\end{equation*}
It turns out that these are independent of $p$ up to a constant which is independent of $N$.

\begin{rmk}\label{rmk:HA-notation}
  From now on we will start using notation which is common in harmonic analysis: for two expressions $A$ and $B$ (representing nonnegative real numbers) we write
  \begin{equation*}
    A \lesssim B
  \end{equation*}
  to mean that there exists a constant $C < \infty$, independent of all quantities appearing in $A$ and $B$, such that
  \begin{equation*}
    A \leq CB.
  \end{equation*}
  Given a list of parameters $p_{1}, p_{2}, \ldots$ (which may appear in $A$ and $B$), we write
  \begin{equation*}
    A \lesssim_{p_{1}, p_{2}, \ldots} B
  \end{equation*}
  to mean that there is a constant $C = C(p_{1}, p_{2}, \ldots) < \infty$ \emph{which may depend on the listed parameters} such that $A \leq CB$.
  We write
  \begin{equation*}
    A \simeq B
  \end{equation*}
  to mean that $A \lesssim B$ and $B \lesssim A$, and we may also include a list of parameters in this notation.
  The usage of this notation is usually clear from context, and if you aren't used to it yet, you will be soon.
  This notation saves a lot of time, particularly when we don't care about the precise value of the constant $C$.
\end{rmk}

\begin{thm}[Kahane--Khintchine inequality]\label{thm:kk}\index{inequality!Kahane--Khintchine}
  Let $X$ be a Banach space and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega,\mc{A},\P)$.
  Then for all $p,q \in [1,\infty)$ there exists a finite constant $\kappa_{p,q}$ such that for all finite sequences $(\mb{x}_n)_{n=1}^N$ in $X$,
  \begin{equation*}
    \Big\|\sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)}
    \leq \kappa_{p,q} \Big\|\sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
  \end{equation*}
  That is, for all $p \in [1,\infty)$, the $L^p$-norms of a Rademacher sum are pairwise equivalent.
  In the notation of Remark \ref{rmk:HA-notation}, we have
  \begin{equation*}
    \Big\|\sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} \simeq_{p,q} \Big\|\sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
  \end{equation*}
\end{thm}

\begin{rmk}
  This result actually holds for all $p,q \in (0,\infty)$, but we haven't defined Bochner spaces $L^p(\Omega;X)$ for $p < 1$, and for the purposes of this course we don't need to.
\end{rmk}

Since $\Omega$ is a probability space, H\"older's inequality yields the case $p \leq q$ with constant $\kappa_{p,q} = 1$, so the real difficulty is when $p > q$.

The Kahane--Khintchine inequality is so useful that it is often used without comment in the literature.
One well-known proof is a reduction to \emph{hypercontractivity of the heat semigroup on the discrete cube}, providing an interesting link with the analysis of Boolean functions (and of course this link goes much deeper).
We will follow \cite{HNVW16} and prove it via the \emph{John--Nirenberg theorem for adapted sequences}, which is quite useful in itself.
But before that, let's look at some consequences of Kahane--Khintchine.

\begin{prop}
  Let $X$ be a Banach space and consider a sequence $(\mb{x}_{n})_{n \in \N}$ in $X$.
  For all $p,q \in [1,\infty)$, the Rademacher sum
  \begin{equation}\label{eq:rad-sum-example}
    \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n}
  \end{equation}
  converges in $L^p(\Omega;X)$ if and only if it converges in $L^q(\Omega;X)$.
\end{prop}

\begin{proof}
  Suppose that the Rademacher sum converges in $L^q(\Omega;X)$.
  Convergence of the Rademacher sum in $L^p(\Omega;X)$ is equivalent to the limit
  \begin{equation*}
    \limsup_{N,M \to \infty} \Big\| \sum_{n=N}^{M} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} = 0.
  \end{equation*}
  But of course by Kahane--Khintchine we have
  \begin{equation*}
    \limsup_{N,M \to \infty} \Big\| \sum_{n=N}^{M} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)}
    \lesssim_{p,q} \limsup_{N,M \to \infty} \Big\| \sum_{n=N}^{M} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)} = 0.
  \end{equation*}
\end{proof}

When the Banach space under consideration is a Hilbert space, Rademacher averages are equivalent to $\ell^2$ sums.

\begin{thm}[Khintchine's inequality]\label{thm:khintchine}\index{inequality!Khintchine}
  Let $H$ be a Hilbert space and $(\mb{h}_{n})_{n \in \N}$ a sequence in $H$.
  Then for all $p \in [1,\infty)$,
  \begin{equation*}
    \Big(\E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{h}_{n} \Big\|_{H}^{p} \Big)^{1/p} \simeq_{p} \|\mb{h}_{\bullet}\|_{\ell^2(H)}.
  \end{equation*}
  In particular, for all scalar sequences $(a_{n})_{n \in \N}$,
  \begin{equation*}
    \Big( \E \Big| \sum_{n \in \N} \varepsilon_{n} a_{n} \Big|^{p} \Big)^{1/p}
    \simeq_{p} \Big( \sum_{n \in \N} |a_{n}|^{2} \Big)^{1/2}.
  \end{equation*}
\end{thm}

\begin{proof}
  Using Kahane--Khintchine, independence of the Rademacher variables, and $\E(\varepsilon_{n}^{2}) = 1$,
  \begin{equation*}
    \begin{aligned}
      \Big( \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{h}_{n} \Big\|_{H}^{p} \Big)^{2/p}
      &\simeq_{p} \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{h}_{n} \Big\|_{L^2(\Omega;H)}^{2} \\
      &=  \int_{\Omega} \big\langle \sum_{n \in \N} \varepsilon_{n}(\omega) \mb{h}_{n}, \sum_{m \in \N} \varepsilon_{m}(\omega) \mb{h}_{m} \big\rangle \, \dd \P(\omega) \\
      & =  \sum_{n,m \in \N} \E(\varepsilon_n \varepsilon_m) \langle \mb{h}_{n}, \mb{h}_{m} \rangle 
      =  \sum_{n \in \N} \langle \mb{h}_{n}, \mb{h}_{n} \rangle  = \|\mb{h}_{\bullet}\|_{\ell^2(H)}^{2}.
    \end{aligned}
  \end{equation*}
\end{proof}

When the Banach space $X$ is a Lebesgue space, the Kahane--Khintchine inequality also lets us identify Rademacher averages with \emph{square functions}.

\begin{thm}\index{Rademacher averages!in Lebesgue spaces} \label{thm:rademacher-lp}
  Let $(S,\mc{A},\mu)$ be a $\sigma$-finite measure space and $p \in [1,\infty)$.
  Then for all sequences $(f_{n})_{n \in \N}$ of (scalar-valued) functions in $L^p(S)$,
  \begin{equation}\label{eq:KK-functions}
    \E \Big\| \sum_{n \in \N} \varepsilon_{n} f_{n} \Big\|_{L^p(S)}
    \simeq_{p} \Big\| \Big( \sum_{n \in \N} |f_{n}|^{2} \Big)^{1/2} \Big\|_{L^p(S)},
  \end{equation}
  where
  \begin{equation*}
    \Big( \sum_{n \in \N} |f_{n}|^{2} \Big)^{1/2}(s) := \Big( \sum_{n \in \N} |f_{n}(s)|^{2} \Big)^{1/2} \qquad \forall s \in S.
  \end{equation*}
  \end{thm}
  
\begin{proof}
  By Kahane--Khintchine, along with Fubini and Theorem \ref{thm:khintchine}, we have
  \begin{equation*}
    \begin{aligned}
      \Big(\E \Big\| \sum_{n \in \N} \varepsilon_{n} f_{n} \Big\|_{L^p(S)}\Big)^{p}
      &\simeq_{p} \E \Big\| \sum_{n \in \N} \varepsilon_{n} f_{n} \Big\|_{L^p(S)}^{p} \\
      &=  \int_{\Omega} \int_{S} \Big| \sum_{n \in \N} \varepsilon_{n}(\omega) f_{n}(s) \Big|^{p} \, \dd\mu(s) \, \dd\P(\omega)  \\
      &=  \int_{S} \int_{\Omega} \Big| \sum_{n \in \N} \varepsilon_{n}(\omega) f_{n}(s) \Big|^{p} \, \dd\P(\omega) \, \dd\mu(s)  \\
      &=  \int_{S} \E \Big| \sum_{n \in \N} \varepsilon_{n} f_{n}(s) \Big|^{p}  \, \dd\mu(s)  \\
      &\simeq_{p} \int_{S} \Big( \sum_{n \in \N} |f_{n}(s)|^{2} \Big)^{p/2}  \, \dd\mu(s) 
      = \Big\| \Big( \sum_{n \in \N} |f_{n}|^{2} \Big)^{1/2} \Big\|_{L^p(S)}^{p}.
    \end{aligned}
\end{equation*}
\end{proof}

\section{The John--Nirenberg inequality for stochastic processes}

Fix a Banach space $X$ and a probability space $(\Omega,\mc{A},\P)$.
Consider a filtration $\mc{A}_{\bullet}$ and an $X$-valued stochastic process $\mb{f}_{\bullet}$ adapted to $\mc{A}_{\bullet}$.
For all $q \in [1,\infty)$ we consider the following measure of the oscillation of $\mb{f}_{\bullet}$:
\begin{equation*}
  \|\mb{f}_{\bullet}\|_{*,q} := \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{A \in \mc{A}_k^{+}} \Big( \fint_{A} \|(\mb{f}_n - \mb{f}_{k-1})(\omega)\|_{X}^{q} \, \dd\P(\omega)\Big)^{1/q}
\end{equation*}
where $\mc{A}_{k}^{+} = \{A \in \mc{A}_{k} : \P(A) > 0\}$.
This depends on the choice of $\mc{A}_{\bullet}$, but we do not reference this in the notation.
The \emph{John--Nirenberg inequality} says that this measure of oscillation is, up to a constant, independent of the choice of $q$.

\begin{thm}[John--Nirenberg inequality for stochastic processes]\label{thm:jn-adapted-sequences}\index{inequality!John--Nirenberg}
  For all $p,q \in [1,\infty)$ and for all $\mb{f}_{\bullet}$ as above,
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,p} \simeq_{p,q} \|\mb{f}_{\bullet}\|_{*,q}.
  \end{equation*}
\end{thm}

We will prove this through a series of lemmas, in which we obtain increasingly fine control on the oscillation of $\mb{f}_{\bullet}$.

\begin{lem}\label{lem:JN-proof-1}
  For all $k \leq n$, $A \in \mc{A}_k$, and $t > 0$,
  \begin{equation}\label{eq:JN-proof-1-est}
    \P(A \cap \{ \|\mb{f}_n - \mb{f}_{k-1}\|_{X} > t \}) \leq \Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} \P(A).
  \end{equation}
\end{lem}

\begin{proof}
  The left hand side of \eqref{eq:JN-proof-1-est} is bounded by
  \begin{equation*}
    \int_{A} \Big( \frac{\|\mb{f}_n - \mb{f}_{k-1}\|_{X}}{t} \Big)^{q} \, \dd\P \leq \Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} \P(A) 
  \end{equation*}
  since $A \in \mc{A}_k$ and $k \leq n$, using the definition of $\|\mb{f}_{\bullet}\|_{*,q}$.
\end{proof}

Next we show that the oscillation control established in the previous lemma extends to more general stopping times (that is, more general than the constant stopping time $n$).

\begin{lem}\label{lem:st-extension}
  Suppose that there exist $t > 0$ and $c > 0$ such that
  \begin{equation*}
    \P(A \cap \{ \| \mb{f}_n - \mb{f}_{k-1} \|_{X} > t \} ) \leq c \P(A) \qquad \forall k \leq n, A \in \mc{A}_k.
  \end{equation*}
  Then for all $k \in \N$, $A \in \mc{A}_k$, and all stopping times $T$ such that $T \geq k$ on $A$,
  \begin{equation}\label{eq:jn-stoptime}
    \P(A \cap \{T < \infty\} \cap \{ \| \mb{f}_{T} - \mb{f}_{k-1} \|_{X} > 2t \} ) \leq 2c \P(A).
  \end{equation}
\end{lem}

Here we use the (standard) notation $\mb{f}_{T}(\omega) := \mb{f}_{T(\omega)}(\omega)$.
Since \eqref{eq:jn-stoptime} involves the condition $T < \infty$, we don't need to worry about defining $\mb{f}_{\infty}$.

\begin{proof}
  Sum over all possible values of the stopping time:
  \begin{equation*}
      \P(A \cap \{T < \infty\} \cap \{ \| \mb{f}_{T} - \mb{f}_{k-1} \|_{X} > 2t \} ) 
      = \lim_{N \to \infty} \sum_{n = k}^{N} \P(A_n \cap \{ \| \mb{f}_{n} - \mb{f}_{k-1} \|_{X} > 2t \} ),
  \end{equation*}
  where $A_{n} := A \cap \{T = n\}$.
  For fixed $N > n > k$, since $A_n \in \mc{A}_n \subset \mc{A}_{n+1}$, we have by assumption
  \begin{equation*}
    \begin{aligned}
      &\P(A_n \cap \{ \| \mb{f}_{n} - \mb{f}_{k-1} \|_{X} > 2t \} ) \\
      &\leq \P(A_n \cap \{ \| \mb{f}_{n} - \mb{f}_{N} \|_{X} > t \} )
      + \P(A_n \cap \{ \| \mb{f}_{k-1} - \mb{f}_{N} \|_{X} > t \} ) \\
      &\leq c \P(A_n) + \P(A_n \cap \{ \| \mb{f}_{k-1} - \mb{f}_{N} \|_{X} > t \} ).
    \end{aligned}
  \end{equation*}
  Thus we have
  \begin{equation*}
    \begin{aligned}
      &\lim_{N \to \infty} \sum_{n = k}^{N} \P(A_n \cap \{ \| \mb{f}_{n} - \mb{f}_{k-1} \|_{X} > 2t \} ) \\
      &\leq \lim_{N \to \infty} \Big( c \sum_{n=k}^N \P(A_n) + \sum_{n=k}^N \P(A_n \cap \{ \| \mb{f}_{k-1} - \mb{f}_{N} \|_{X} > t \}) \Big) \\
      &\leq c \P(A) + \lim_{N \to \infty} \P(A \cap \{ \| \mb{f}_{k-1} - \mb{f}_{N} \|_{X} > t \})  
      \leq 2c\P(A)
    \end{aligned}
  \end{equation*}
  using the assumption and that $A \in \mc{A}_k$ in the last estimate.
\end{proof}
  
In the following lemma we make use of the \emph{process started at $k-1$}
\begin{equation*}
  {}^{k-1}\mb{f}_{\bullet} = (\mb{f}_n - \mb{f}_{k-1})_{n = k-1}^{\infty}
\end{equation*}
and its Doob maximal function
\begin{equation*}
  \mc{M}({}^{k-1}\mb{f}_{\bullet})(\omega) = \sup_{n \geq k-1} \|{}^{k-1}\mb{f}_n(\omega)\|_X = \sup_{n \geq k} \|(\mb{f}_n - \mb{f}_{k-1})(\omega)\|_X.
\end{equation*}

\begin{lem}\label{lem:jn-mf}
  Suppose that $\mb{f}_{\bullet}$ satisfies \eqref{eq:jn-stoptime} for some $t > 0$ and $c > 0$, for all $k \in \N$, $A \in \mc{A}_k$, and all stopping times $T$ such that $T \geq k$ on $A$.
  Then for all $s > 0$, all $k \in \N$, and all $A \in \mc{A}_{k}$,
  \begin{equation}\label{eq:jn-mf-eq}
    \P(A \cap \{ \mc{M}({}^{k-1} \mb{f}_{\bullet}) > s + 2t \}) \leq 2c \P(A \cap \{  \mc{M}({}^{k-1} \mb{f}_{\bullet}) > s \}).
  \end{equation}
\end{lem}

\begin{proof}
  Fix $k \in \N$ and consider the stopping times
  \begin{equation*}
    \begin{aligned}
      S &:= \inf\{n \geq k : \|\mb{f}_n - \mb{f}_{k-1}\|_{X} > s\}, \\
      T &:= \inf\{n \geq k : \|\mb{f}_n - \mb{f}_{k-1}\|_{X} > s + 2t\}.
    \end{aligned}
  \end{equation*}
  Then $k \leq S \leq T$, and \eqref{eq:jn-mf-eq} can be rewritten as
  \begin{equation*}
    \P(A \cap \{T < \infty\}) \leq 2c \P(A \cap \{S < \infty\}).
  \end{equation*}
  Now fix $n \geq k$ and let $A_n := A \cap \{S = n\} \in \mc{A}_n$. 
  On $A_n \cap \{T < \infty\}$ we have
  \begin{equation*}
    \|\mb{f}_{T} - \mb{f}_{n-1}\|_{X} \geq \|\mb{f}_{T} - \mb{f}_{k-1}\|_{X} - \|\mb{f}_{n-1} - \mb{f}_{k-1}\|_{X} > (s + 2t) - s = 2t,
  \end{equation*}
  so
  \begin{equation*}
    \P(A_n \cap \{T < \infty\}) = \P(A_n \cap \{T < \infty\} \cap \{\|\mb{f}_{T} - \mb{f}_{n-1}\|_{X} > 2t\} )
    \leq 2c \P(A_n)
  \end{equation*}
  by assumption, since $T$ is a stopping time such that $T \geq k$ on $A_{n}$.
  Summing over $n \geq k$ completes the proof.
\end{proof}

The following technical lemma lets us deduce $L^p$-norm estimates from super-level measure estimates of the form \eqref{eq:jn-mf-eq}.
It's useful in more general contexts, so we state it in a general form.

\begin{lem}\label{lem:jn-ms-lem}
  Let $(S,\mc{A},\mu)$ be a measure space, and suppose that $\map{f}{S}{\R}$ is a non-negative measurable function supported in a set $A \in \mc{A}$ of finite measure, with
  \begin{equation*}
    \mu(\{f > s+t\}) \leq c \mu(\{f > s\}) \qquad \forall s > 0
  \end{equation*}
  for some $c \in (0,1)$ and $t > 0$.
  Then for all $p \in [1,\infty)$,
  \begin{equation}\label{eq:jn-lem-est}
    \|f\|_{L^p(S)} \leq \frac{1 + c^{1/p}}{1 - c^{1/p}} t \mu(A)^{1/p}.
  \end{equation}
\end{lem}

\begin{proof}
  First we prove the result assuming that $\|f\|_{L^p(S)} < \infty$.
  We can compute
  \begin{equation*}
    \begin{aligned}
      \|f\|_{p}^{p} &= \int_{0}^{\infty} ps^{p-1} \mu(\{f > s\}) \, \dd s \\
      &\leq \int_{0}^{t} ps^{p-1}\mu(A) \, \dd s + \int_{0}^{\infty} p(s+t)^{p-1}\mu(\{f > s+t\}) \, \dd s \\
      &\leq t^{p}\mu(A) + \int_{0}^{\infty} p(s + t)^{p-1}c\mu(\{f > s\}) \, \dd s \\
      &\leq t^{p}\mu(A) + \int_{0}^{\infty} p(s + t)^{p-1}c\mu(\{f + t\1_{A} > s + t\}) \, \dd s \\
      &= t^{p}\mu(A) + c\|f + t\1_{A}\|_{p}^{p}.
    \end{aligned}
  \end{equation*}
  This yields
  \begin{equation*}
    \|f\|_{p} \leq t \mu(A)^{1/p} + c^{1/p}\|f\|_{p} + c^{1/p} t \mu(A)^{1/p},
  \end{equation*}
  which implies \eqref{eq:jn-lem-est} by a bit of arithmetic.

  To establish the result without the \emph{a priori} assumption that $\|f\|_{p} < \infty$, fix $r > 0$ and consider the function $g = \min(f,r)$.
  Then $g$ is bounded and has support of finite measure, so $\|g\|_{p} < \infty$.
  Furthermore, $g$ satisfies the hypothesis of the lemma (this is true, but annoying to check and write), so we have
  \begin{equation*}
    \|g\|_{p} \leq \frac{1 + c^{1/p}}{1 - c^{1/p}} t\mu(A)^{1/p}.
  \end{equation*}
  Taking the limit $r \to \infty$ and invoking monotone convergence implies the corresponding estimate for $f$. 
\end{proof}

Now we can put everything together to prove the John--Nirenberg inequality.

\begin{proof}[Proof of Theorem \ref{thm:jn-adapted-sequences}]
  We suppose that $\|\mb{f}_{\bullet}\|_{*,q} < \infty$ for otherwise there is nothing to prove.
  Lemma \ref{lem:JN-proof-1} tells us that
  \begin{equation}\label{eq:JN-proof-1-est-end}
    \P(A \cap \{ \|\mb{f}_n - \mb{f}_{k-1}\|_{X} > t \}) \leq \Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} \P(A)
  \end{equation}
  For all $k \leq n$, $A \in \mc{A}_k$, and $t > 0$.
  Lemma \ref{lem:st-extension} then extends this, saying that
  \begin{equation}\label{eq:jn-stoptime-end}
    \P(A \cap \{T < \infty\} \cap \{ \| \mb{f}_{T} - \mb{f}_{k-1} \|_{X} > 2t \} ) \leq 2\Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} \P(A)
  \end{equation}
  for all $t > 0$, $k \in \N$, $A \in \mc{A}_k$, and all stopping times $T$ such that $T \geq k$ on $A$.
  Lemma \ref{lem:jn-mf} gives us the maximal function estimate
  \begin{equation}\label{eq:jn-mf-eq-end}
    \P(A \cap \{ \mc{M}({}^{k-1} \mb{f}_{\bullet}) > s + 2t \}) \leq c(t) \P(A \cap \{  \mc{M}({}^{k-1} \mb{f}_{\bullet}) > s \}).
  \end{equation}
  for all $t > 0$, $s > 0$, $k \in \N$, and $A \in \mc{A}_{k}$, where
  \begin{equation*}
    c(t) := 2\Big( \frac{\|\mb{f}_{\bullet}\|_{*,q}}{t} \Big)^{q} = 2t^{-q} \|\mb{f}_{\bullet}\|_{*,q}^{q}.
  \end{equation*}
  For $t > 2^{1/q} \|\mb{f}_{\bullet}\|_{*,q}$  we have $c(t) < 1$, and thus given $k \in \N$ and $A \in \mc{A}_{k}$, Lemma \ref{lem:jn-ms-lem} yields
  \begin{equation*}
    \|\1_{A} \mc{M}({}^{k-1} \mb{f}_{\bullet})\|_{L^p(A)} \leq \frac{1 + c(t)^{1/p}}{1 - c(t)^{1/p}} t \P(A)^{1/p}.
  \end{equation*}
  So we can estimate
  \begin{equation*}
    \begin{aligned}
      \|\mb{f}_{\bullet}\|_{*,p}
      &= \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{A \in \mc{A}_{k}^{+}} \Big( \fint_{A} \|\mb{f}_{n} - \mb{f}_{k-1} \|_{X}^{p} \, \dd\P \Big)^{1/p} \\
      &\leq \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{A \in \mc{A}_{k}^{+}} \Big( \fint_{A} \mc{M}({}^{k-1}\mb{f}_{\bullet})^{p} \, \dd\P \Big)^{1/p} \\
      &= \sup_{k \in \N} \sup_{A \in \mc{A}_{k}^{+}} \P(A)^{-1/p} \|\1_{A} \mc{M}({}^{k-1}\mb{f}_{\bullet}) \|_{L^p(A)} 
      \leq \frac{1 + c(t)^{1/p}}{1 - c(t)^{1/p}} t .
    \end{aligned}
  \end{equation*}
  Now it's just a matter of making a good choice of $t > 2^{1/q} \|\mb{f}_{\bullet}\|_{*,q}$.
  This can be optimised for a better constant, but we won't aim that high: we'll just take
  \begin{equation*}
    t_{0} := 2^{1 + \frac{1}{q}} \|\mb{f}_{\bullet}\|_{*,q}.
  \end{equation*}
  Then we have
  \begin{equation*}
    c(t_0) = 2(2^{1 + \frac{1}{q}} \|\mb{f}_{\bullet}\|_{*,q})^{-q} \|\mb{f}_{\bullet}\|_{*,q}^{q} = 2^{-q},
  \end{equation*}
  and thus
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,p} \leq \frac{1 + 2^{-q/p}}{1 - 2^{-q/p}} 2^{1 + \frac{1}{q}} \|\mb{f}_{\bullet}\|_{*,q} \simeq_{p,q} \|\mb{f}_{\bullet}\|_{*,q}.
  \end{equation*}
  By exchanging the roles of $p$ and $q$ we get
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,p} \simeq_{p,q} \|\mb{f}_{\bullet}\|_{*,q},
  \end{equation*}
  which was our goal from the beginning.
  
\end{proof}

\subsection{Proof of the Kahane--Khintchine inequality}

Now we return to the equivalence of $L^p$- and $L^q$-Rademacher averages:
\begin{equation*}
  \Big\|\sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} \simeq_{p,q} \Big\|\sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
\end{equation*}

\begin{proof}[Proof of Theorem \ref{thm:kk}]\index{inequality!Kahane--Khintchine}
  Consider the filtration $\mc{A}_{\bullet}$ generated by the process $\varepsilon_{\bullet}$, and the $\mc{A}_{\bullet}$-adapted sequence $\mb{f}_{\bullet}$ given by
  \begin{equation*}
    \mb{f}_{m} := \sum_{n=0}^{\min(m,N)} \varepsilon_n \mb{x}_{n} \qquad \forall m \in \N.
  \end{equation*}
  We will show that for all $q \in [1,\infty)$ we have
  \begin{equation}\label{eq:kk-claim}
    \Big\| \sum_{n=0}^N \varepsilon_n \mb{x}_n \Big\|_{L^q(\Omega;X)} = \|\mb{f}_{\bullet}\|_{*,q};
  \end{equation}
  once we have this, the John--Nirenberg inequality completes the proof.

  First recall that
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n \leq N}} \sup_{A \in \mc{A}_k^{+}} \Big( \fint_{A} \|\mb{f}_n - \mb{f}_{k-1}\|_{X}^{q} \, \dd\P\Big)^{1/q};
  \end{equation*}
  we are free to add the restriction $n \leq N$ because $\mb{f}_{\bullet}$ is eventually constant.
  Fix $k \leq n \leq N$ and $A \in \mc{A}_k^{+}$.
  We will compute
  \begin{equation*}
    \fint_{A} \|\1_{A} (\mb{f}_n - \mb{f}_{k-1})\|_{X}^q \, \dd\P = \P(A)^{-1} \E \Big( \1_{A} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big).
  \end{equation*}
  For all $\omega \in \Omega$,
  \begin{equation*}
    \Big\| \sum_{j=k}^{n} \varepsilon_j(\omega) \mb{x}_j\Big\|_{X} = \Big\| \varepsilon_k(\omega) \Big( \mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime}(\omega) \mb{x}_j \Big) \Big\|_{X} = \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X},
  \end{equation*}
  where
  \begin{equation*}
    \varepsilon_{j}^{\prime} :=
    \begin{cases}
      \varepsilon_{j} & j \leq k \\
      \varepsilon_{k} \varepsilon_{j} & j \geq k+1.
    \end{cases}
  \end{equation*}
  Now note that the $\sigma$-algebra $\mc{A}^\prime_{k+1,n} := \sigma(\{\varepsilon_j^\prime : k+1 \leq j \leq n\})$ is independent of $\mc{A}_k$, since for all $1 \leq j \leq k$ and $k+1 \leq j' \leq n$ we have by independence of the original Rademacher sequence
  \begin{equation*}
    \E(\varepsilon_j \varepsilon_{j'}^\prime) = \E(\varepsilon_j \varepsilon_k \varepsilon_{j'})
    =
    \begin{cases}
      \E(\varepsilon_j) \E(\varepsilon_k) \E(\varepsilon_{j'}) & \text{if $j < k$} \\
       \E(\varepsilon_{j'}) & \text{if $j = k$}
    \end{cases}
    \quad = 0.
  \end{equation*}
  Thus, since $A \in \mc{A}_k$, we have by independence of $\mc{A}_k$ and $\mc{A}^{\prime}_{k+1, n}$
  \begin{equation*}
    \begin{aligned}
      \P(A)^{-1} \E \Big( \1_{A} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big)
      &=  \P(A)^{-1} \E \Big( \1_{A} \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} \Big) \\
      &=  \E \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} 
      =  \E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}. \\
    \end{aligned}
  \end{equation*}
  Now letting $\mc{A}_{k,n} := \sigma(\{\varepsilon_j : k \leq j \leq n\})$, we have by the $L^q$-nonexpansiveness of conditional expectations
  \begin{equation*}
    \E \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
    = \E \Big\|\E^{\mc{A}_{k,n}} \Big(\sum_{j=0}^{N} \varepsilon_{j} \mb{x}_j \Big)\Big\|_{X}^{q}
    \leq \E \Big\|\sum_{0=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
  \end{equation*}
  with equality when $k=0$ and $n=N$.
  Thus
  \begin{equation*}
    \|\mb{f}_{\bullet}\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n \leq N}}  \Big(\E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q} = \Big(\E  \Big\|\sum_{j=0}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q}
  \end{equation*}
  which proves the claimed equality \eqref{eq:kk-claim} and completes the proof. 
\end{proof}

\section{Rademacher spaces, type, and cotype}\label{sec:rademacher-spaces}

Recall Khintchine's inequality from Theorem \ref{thm:khintchine}: if $H$ is a Hilbert space and $(\mb{h}_{n})_{n \in \N}$ is a sequence of vectors in $H$, then for all $p \in [1,\infty)$ we have the identification of the $L^p$-Rademacher average
\begin{equation*}
  \Big( \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{h}_{n} \Big\|_{H}^{p} \Big)^{1/p} \simeq_{p} \|\mb{h}\|_{\ell^2(H)}.
\end{equation*}
This suggests that for a general Banach space we could use Rademacher averages to create a space of $X$-valued functions which retains some nice properties of the Hilbert space $\ell^2$.

\begin{defn}
  Let $X$ be a Banach space.
  For $N \in \N$ we define the (finite) \emph{Rademacher space}\index{Rademacher spaces} $\varepsilon_{N}(X)$ to be the space of finite sequences $\mb{x}_{\bullet} = (\mb{x}_{n})_{n=0}^{N}$ in $X$, equipped with the norm
  \begin{equation*}
    \|\mb{x}_{\bullet}\|_{\varepsilon_{N}(X)} := \E \Big\| \sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} = \Big\| \sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^{1}(\Omega;X)},
  \end{equation*}
  where $(\varepsilon_{n})_{n = 0}^{N}$ is an arbitrary finite Rademacher sequence on an arbitrary probability space $(\Omega,\mc{A},\P)$.
  We also define the (infinite) Rademacher space $\varepsilon(X)$ to be the space of infinite sequences $\mb{x}_{\bullet} = (\mb{x}_{n})_{n \in \N}$ such that the series
  \begin{equation}\label{eq:infinite-rademacher-sum}
    \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n} = \lim_{N \to \infty} \sum_{n = 0}^{N} \varepsilon_{n} \mb{x}_{n}
  \end{equation}
  converges in $L^{1}(\Omega;X)$, equipped with the norm
  \begin{equation*}
    \|\mb{x}_{\bullet}\|_{\varepsilon(X)} := \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} = \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^{1}(\Omega;X)}.
  \end{equation*}
  As before, $(\varepsilon_{n})_{n \in \N}$ is an arbitrary infinite Rademacher sequence on an arbitrary probability space $(\Omega,\mc{A},\P)$.
\end{defn}

\begin{rmk}
  The Kahane--Khintchine inequality says that the Rademacher averages in the definitions above can be replaced with $L^p$-Rademacher averages, and that the resulting norms are equivalent up to a constant independent of $N$.
  It also says that the convergence of the infinite Rademacher sum \eqref{eq:infinite-rademacher-sum} in $L^1(\Omega;X)$ is equivalent to its convergence in $L^p(\Omega;X)$ for any $p \in [1,\infty)$.
\end{rmk}

The Rademacher space $\varepsilon(X)$ is a Banach space (Exercise \ref{ex:on-rad-spaces}), and in most applications of Banach-valued analysis it plays the role of $\ell^2(\N;X)$.
We will see much more of it in later chapters.

We saw already that $\varepsilon(H) = \ell^2(\N;H)$ when $H$ is a Hilbert space; comparing the Rademacher space with more general sequence spaces leads to the important geometric notions of \emph{type} and \emph{cotype}.

\begin{defn}\index{type!Rademacher}\index{cotype!Rademacher}
  Let $p \in [1,2]$ and $q \in [2,\infty]$.
  A Banach space $X$ has \emph{(Rademacher) type $p$} if there exists a constant $C < \infty$ such that
  \begin{equation*}
    \| \mb{x}_{\bullet} \|_{\varepsilon(X)} := \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} \leq C \Big( \sum_{n \in \N} \|\mb{x}_{n}\|_{X}^{p} \Big)^{1/p} = C\|\mb{x}_{\bullet}\|_{\ell^p(X)}
  \end{equation*}
  for all sequences $\mb{x}_{\bullet} \in \ell^{p}(X)$, or equivalently if we have the continuous inclusion $\ell^{p}(X) \subset \varepsilon(X)$.
  On the other hand, $X$ has \emph{(Rademacher) cotype $q$} if there exists $C < \infty$ such that
  \begin{equation*}
    \|\mb{x}_{\bullet}\|_{\ell^q(X)} \leq C \|\mb{x}_{\bullet}\|_{\varepsilon(X)} ,
  \end{equation*}
  i.e. if we have the continuous inclusion $\varepsilon(X) \subset \ell^{q}(X)$.
\end{defn}

We restrict to $p \in [1,2]$ and $q \in [2,\infty]$ because the type and cotype inequalities can never hold for $p > 2$ and $q < 2$ (Exercise \ref{ex:no-extreme-types}).
Every Banach space $X$ has type $1$ and cotype $\infty$: indeed, by the triangle inequality
\begin{equation*}
  \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} \leq \E \sum_{n \in \N} \|\varepsilon_{n} \mb{x}_{n}\|_{X} = \sum_{n \in \N} \|\mb{x}_{n}\|_{X}
\end{equation*}
and by the contraction principle (Theorem \ref{thm:contraction})
\begin{equation*}
  \|\mb{x}_{n}\|_{X} \lesssim \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n}\Big\|_{X} \qquad \forall n \in \N.
\end{equation*}
Thus we always have continuous inclusions
\begin{equation*}
  \ell^1(X)\subset  \varepsilon(X) \subset \ell^{\infty}(X).
\end{equation*}
We say that $X$ has \emph{nontrivial type} if it has type $p$ for some $p > 1$, and \emph{finite cotype} if it has cotype $q$ for some $q < \infty$.
In this case the continuous inclusions above can be strengthened to
\begin{equation*}
  \ell^1(X)\subset \ell^p(X) \subset \varepsilon(X) \subset \ell^{q}(X) \subset \ell^{\infty}(X).
\end{equation*}
The continuous inclusions $\ell^{r}(X) \subset \ell^{s}(X)$ for $r < s$ imply that if $X$ has type $p$ and cotype $q$, then $X$ has type $\td{p}$ and cotype $\td{q}$ for all $1 \leq \td{p} \leq p$ and all $q \leq \tilde{q} \leq \infty$.

Hilbert spaces have type $2$ and cotype $2$, by Khintchine's inequality.
Since every finite-dimensional Banach space is isomorphic to a Hilbert space, and since type and cotype are stable under isomorphism (Exercise \ref{ex:isomorphic-cotype}), it follows that every finite-dimensional Banach space has type $2$ and cotype $2$.

\begin{example}\index{type!Rademacher!of Lebesgue spaces}\index{cotype!Rademacher!of Lebesgue spaces}
  Let $(S,\mc{A},\mu)$ be a measure space, and consider the Banach space $X = L^p(S)$ with $p \in [1,\infty)$.
  Then $X$ has type $\min(p,2)$ and cotype $\max(p,2)$.
  For simplicity we will show that $L^p(S)$ has type $p$ when $p \in [1,2]$, as the remaining statements have analogous proofs.
  For a sequence of functions $f_{n} \in L^p(S)$,
  \begin{equation*}
    \begin{aligned}
      \E \Big\| \sum_{n \in \N} \varepsilon_{n} f_{n} \Big\|_{L^p(S)}
      &\stackrel{(1)}{\simeq_{p}} \Big( \int_{\Omega} \Big\| \sum_{n \in \N} \varepsilon_{n}(\omega) f_{n} \Big\|_{L^p(S)}^{2} \, \dd\P(\omega) \Big)^{1/2} \\
      &= \Big( \int_{\Omega} \Big( \int_{S} \Big| \sum_{n \in \N} \varepsilon_{n}(\omega) f_{n}(s) \Big|^{p} \, \dd\mu(s) \Big)^{2/p} \, \dd\P(\omega) \Big)^{1/2} \\
      &\stackrel{(2)}{\leq} \Big( \int_{S} \Big( \int_{\Omega} \Big| \sum_{n \in \N} \varepsilon_{n}(\omega) f_{n}(s) \Big|^{2} \, \dd\P(\omega) \Big)^{p/2} \, \dd\mu(s) \Big)^{1/p} \\
      &= \Big( \int_{S} \Big( \E \Big| \sum_{n \in \N} \varepsilon_{n} f_{n}(s) \Big|^{2} \Big)^{p/2} \, \dd\mu(s) \Big)^{1/p} \\
      &\stackrel{(3)}{\lesssim} \Big( \int_{S} \sum_{n \in \N} |f_{n}(s)|^{p} \, \dd\mu(s) \Big)^{1/p}
      = \Big( \sum_{n \in \N} \|f_{n}\|_{L^p(S)}^{p} \Big)^{1/p}.
    \end{aligned}
  \end{equation*}
  Estimate (1) uses Kahane--Khintchine, estimate (2) uses Minkowski's inequality (using $2/p > 1$), and estimate (3) uses that the scalar field $\K$ has type $p$.
  
  As mentioned above, if the measure space $S$ is such that $L^p(S)$ is finite-dimensional, then $L^p(S)$ has type $2$ and cotype $2$ (the strongest possible type and cotype).
  If $L^p(S)$ is infinite-dimensional, then it does not have type greater than $\min(p,2)$ or cotype less than $\max(2,p)$ (Exercise \ref{ex:Lp-optimal-type}). 
\end{example}

\begin{example}\index{cotype!failure for $c_{0}$}
  Consider the Banach space $c_{0}$ of scalar-valued sequences $(a_{n})_{n \in \N}$ with $\lim_{n \to \infty} a_{n} = 0$, equipped with the $\sup$ norm.
  We will show that $c_{0}$ does not have finite cotype.
  Consider the standard basis vectors $\mb{e}_{n} \in c_{0}$.
  For all $N \in \N$ and $q < \infty$ we can compute
  \begin{equation*}
    \E \Big\| \sum_{n = 0}^{N-1} \varepsilon_{n} \mb{e}_{n} \Big\|_{c_0}
    = \E \| (\varepsilon_{0}, \varepsilon_{1}, \cdots, \varepsilon_{N-1}, 0, 0, \cdots) \|_{c_{0}} = 1
  \end{equation*}
  and
  \begin{equation*}
    \Big( \sum_{n=0}^{N-1} \|\mb{e}_{n}\|_{c_0}^{q} \Big)^{1/q} = N^{1/q}.
  \end{equation*}
  Thus if $c_{0}$ has cotype $q$, there exists a constant $C < \infty$ such that
  \begin{equation*}
    N^{1/q} \leq C
  \end{equation*}
  for all $N \in \N$, which is absurd.
  In Exercise \ref{ex:c0-notype} you will show that $c_{0}$ also does not have nontrivial type; thus $c_{0}$ has no nontrivial type or finite cotype. It's bad!
  
\end{example}

Given a Banach space $X$, one would expect that the Rademacher spaces $\varepsilon(X)$ and $\varepsilon(X^{*})$ are dual to each other.\index{K-convexity@$K$-convexity!and duality of Rademacher spaces}
This requires the additional assumption of \emph{$K$-convexity}, which we will discuss in Chapter \ref{sec:UMD}.\footnote{It turns out that this is equivalent to $X$ having non-trivial type.}
In general we do have the following H\"older-type (or Cauchy--Schwartz-type) inequality.
See Exercise \ref{ex:DPLMV} for some extensions of this result to three factors.

\begin{prop}\index{Rademacher spaces!Cauchy--Schwartz-type inequality}
  Let $X$ be a Banach space, and consider $\mb{x}_{\bullet} \in \varepsilon(X)$ and $\mb{x}_{\bullet}^{*} \in \varepsilon(X^{*})$.
  Then
  \begin{equation*}
    \Big| \sum_{n \in \N} \langle \mb{x}_{n}, \mb{x}_{n}^{*} \rangle \Big| \lesssim \|\mb{x}_{\bullet}\|_{\varepsilon(X)} \|\mb{x}_{\bullet}^{*}\|_{\varepsilon(X^{*})} 
  \end{equation*}
\end{prop}

\begin{proof}
  Let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega,\mc{A},\P)$.
  Since the Rademacher variables are mutually independent we have $\E(\varepsilon_{n} \varepsilon_m) = 0$ whenever $n \neq m$, and furthermore $\E(\varepsilon_{n}^{2}) = 1$, so we have
  \begin{equation*}
    \sum_{n \in \N} \langle \mb{x}_{n}, \mb{x}_{n}^{*} \rangle
    = \E \Big\langle \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n}, \sum_{m \in \N} \varepsilon_{m} \mb{x}_{m}^{*} \Big\rangle.
  \end{equation*}
  Thus we can use Cauchy--Schwarz and Kahane--Khintchine to estimate
    \begin{equation*}
      \begin{aligned}
        \Big|\sum_{n \in \N} \langle \mb{x}_{n}, \mb{x}_{n}^{*} \rangle \Big|
        &\leq \Big( \E \Big\| \sum_{n \in \N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}^{2} \Big)^{1/2} 
        \Big( \E \Big\| \sum_{m \in \N} \varepsilon_{m} \mb{x}_{m}^{*} \Big\|_{X^{*}}^{2} \Big)^{1/2} \\
        & \simeq \|\mb{x}_{\bullet}\|_{\varepsilon(X)} \|\mb{x}_{\bullet}^{*}\|_{\varepsilon(X^{*})} .
      \end{aligned}
    \end{equation*}
\end{proof}







\section*{Exercises}

\begin{exercise}\label{ex:rad-sum-dist}
  Fix $N \in \N$, and let $(\varepsilon_{n})_{n=0}^{N}$ and $(\varepsilon_{n}')_{n=0}^{N}$ be two finite Rademacher sequences on probability spaces $\Omega$ and $\Omega'$ respectively.
  Let $(\mb{x}_{n})_{n=0}^{N}$ be a finite sequence in a Banach space $X$.
  Show that the Rademacher sums
  \begin{equation*}
    \sum_{n = 0}^{N} \varepsilon_n \mb{x}_{n} \colon \Omega \to X \qquad \text{and} \qquad
    \sum_{n = 0}^{N} \varepsilon_n' \mb{x}_{n} \colon \Omega' \to X
  \end{equation*}
  have the same distribution.
  (Hint: use Theorem \ref{thm:characteristic-function-uniqueness}.) 
\end{exercise}

\begin{exercise}\label{ex:on-rad-spaces}
  Let $X$ be a Banach space.
  \begin{itemize}
  \item
    Show that the Rademacher space $\varepsilon(X)$ is a Banach space, and likewise for the finite Rademacher spaces $\varepsilon_{N}(X)$ for all $N \in \N$.
  \item
    Let $(\mb{x}_{n})_{n \in \N}$ be an infinite sequence in $X$.
    Show that
    \begin{equation*}
      \|\mb{x}_{\bullet}\|_{\varepsilon(X)} = \lim_{N \to \infty} \|(\mb{x}_{n})_{n=0}^{N}\|_{\varepsilon_{N}(X)}.
    \end{equation*}
  \item
    Show that the set of infinite $X$-valued sequences with finite support is dense in $\varepsilon(X)$.
  \end{itemize}
\end{exercise}

\begin{exercise}
  Let $N \in \N$ and suppose that the subsets $(E_{j})_{j=0}^{J}$, $E_{j} \subset \{0,1,\ldots,N\}$ form a partition of $\{0,1,\ldots,N\}$.
  Let $(\varepsilon_{n})_{n=0}^{N}$ be a Rademacher sequence on a probability space $(\Omega,\mc{A},\P)$, and let $(\varepsilon'_{j})_{j=0}^{J}$ be another Rademacher sequence on a probability space $(\Omega', \mc{A}', \P')$.
  Let $(\mb{x}_{n})_{n=0}^{N}$ be a finite sequence of vectors in a Banach space $X$.
  Show that
  \begin{equation*}
    \E \Big\| \sum_{n=0}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{X} = \E' \E \Big\| \sum_{j=0}^{J} \varepsilon'_{j} \sum_{n \in E_{j}} \varepsilon_{n} \mb{x}_{n} \Big\|_{X}
  \end{equation*}
  where $\E'$ denotes the expectation on $\P'$.
\end{exercise}

\begin{exercise}\index{Rademacher spaces!have RNP}\index{Radon--Nikodym property!of Rademacher spaces}
  Let $X$ be a Banach space.
  Show that the Rademacher space $\varepsilon(X)$ has the Radon--Nikodym property if and only if $X$ does. (Hint: use Exercise \ref{ex:Lp-RNP})
\end{exercise}

\begin{exercise}\index{Rademacher spaces!commutation with Lebesgue spaces}\label{ex:rad-leb-comm}
  Let $(S,\mc{A},\mu)$ be a measure space and $X$ a Banach space.
  For all $p \in [1,\infty)$, show that
  \begin{equation*}
    \varepsilon(L^{p}(\mu;X)) = L^{p}(\mu;\varepsilon(X))
  \end{equation*}
  with equivalent norms (after identifying sequences of functions $S \to X$ with functions $S \to \varepsilon(X)$). 
\end{exercise}

\begin{exercise}\label{ex:no-extreme-types}
  Show that it is impossible for a Banach space to have `type $p > 2$' or `cotype $q < 2$'. (Hint: consider the scalar field and use Khintchine's inequality)
\end{exercise}

\begin{exercise}\label{ex:isomorphic-cotype}
  Let $X$ and $Y$ be Banach spaces, such that $Y$ is isomorphic to a closed subspace of $X$.
  If $X$ has type $p$ and cotype $q$, show that $Y$ also has type $p$ and cotype $q$.
\end{exercise}

\begin{exercise}\label{ex:Lp-optimal-type}\index{type!Rademacher!of Lebesgue spaces}\index{cotype!Rademacher!of Lebesgue spaces}
  \begin{itemize}
  \item
    For $p \in [1,\infty)$, show that the sequence space $\ell^{p}(\N)$ does not have type greater than $\min(p,2)$ or cotype less than $\max(p,2)$.
  \item
    Let $(S,\mc{A},\mu)$ be a measure space.
    Then $L^p(S)$ is infinite-dimensional if and only if there exist infinitely many disjoint sets of positive measure in $S$.\footnote{You don't have to prove this, but you can if you like.}
    In this case, show that $L^p(S)$ does not have type greater than $\min(p,2)$ or cotype less than $\max(p,2)$.
  \end{itemize}
\end{exercise}

\begin{exercise}\label{ex:c0-notype}\index{type!failure for $c_{0}$}
  The goal of this exercise is to show that the Banach space $c_{0}$ does not have nontrivial type.
  \begin{enumerate}[(i)]
  \item Fix $N \in \N$ and define the set $\Theta_{N} := \{-1,1\}^{N}$.
    Let $\ell^{1}_{N} = \ell^{1}(\{1,2,\ldots,N\};\R)$.
    Define a map $\map{T}{\ell^{1}_{N}}{\ell^\infty(\Theta_{N})}$ by
    \begin{equation*}
      T(a_{\bullet}) := \Big( \sum_{n=1}^{N} \theta_{n} a_{n}\Big)_{\theta \in \Theta}.
    \end{equation*}
    Show that $T$ is a linear isometry.

  \item For $p > 1$, assuming that the (real) Banach space $c_{0}$ has type $p$ with constant $C$, show that $\ell^{1}_{N}$ also has type $p$ with constant $C$ for all $N \in \N$.

  \item Show that $c_{0}$ (over the scalar field $\R$ or $\C$) does not have type $p > 1$.
  \end{enumerate}
\end{exercise}

\begin{exercise}
  Give an example of a Banach space $X$ with finite cotype such that the dual $X^{*}$ does not have nontrivial type.
\end{exercise}


For the next exercise we need the following definition.

\begin{defn}\index{trilinear forms}
  Let $X$, $Y$, $Z$ be Banach spaces over the scalar field $\K$.
  A \emph{trilinear form} on $X \times Y \times Z$ is a map $\map{\Pi}{X \times Y \times Z}{\K}$ which is linear in each variable separately: that is, for fixed $\mb{x} \in X$, $\mb{y} \in Y$, $\mb{z} \in Z$ the maps
  \begin{equation*}
    \begin{aligned}
      \Pi(\cdot, \mb{y}, \mb{z}) &\colon X \to \K, &\mb{x}' \mapsto \Pi(\mb{x}',\mb{y},\mb{z}) \\
      \Pi(\mb{x}, \cdot, \mb{z}) &\colon Y \to \K, &\mb{y}' \mapsto \Pi(\mb{x},\mb{y}',\mb{z}) \\
      \Pi(\mb{x}, \mb{y}, \cdot) &\colon Z \to \K, &\mb{z}' \mapsto \Pi(\mb{x},\mb{y},\mb{z}')
  \end{aligned}
\end{equation*}
are all linear.
Such a trilinear form is \emph{bounded} if there exists a constant $C < \infty$ such that
\begin{equation*}
  |\Pi(\mb{x}, \mb{y}, \mb{z})| \leq C\|\mb{x}\|_{X} \|\mb{y}\|_{Y} \|\mb{z}\|_{Z}
\end{equation*}
for all $\mb{x} \in X$, $\mb{y} \in Y$, $\mb{z} \in Z$.
The infimum of all such $C$ is denoted by $\|\Pi\|_{\mathrm{Tri}(X \times Y \times Z)}$.
\end{defn}

\begin{exercise}\label{ex:DPLMV}
  Let $X$, $Y$, and $Z$ be Banach spaces, and consider sequences $\mb{x}_{\bullet} \in \varepsilon(X)$, $\mb{y}_{\bullet} \in \varepsilon(Y)$, and $\mb{z}_{\bullet} \in \varepsilon(Z)$.
  \begin{itemize}
  \item
    Let $\map{\Pi}{X \times Y \times Z}{\K}$ be a bounded trilinear form on the product of three Banach spaces.
    Show that
    \begin{equation*}
      \Big| \sum_{n \in \N} \Pi(\mb{x}_{n}, \mb{y}_{n}, \mb{z}_{n}) \Big|
      \lesssim \|\Pi\|_{\mathrm{Tri}(X \times Y \times Z)} \|\mb{x}_{\bullet}\|_{\varepsilon(X)} \|\mb{y}_{\bullet}\|_{\varepsilon(Y)} \|\mb{z}_{\bullet}\|_{\varepsilon(Z)}.
    \end{equation*}
    
  \item
    Consider a sequence $(\Pi_{n})_{n \in \N}$ of bounded trilinear forms $\map{\Pi_{n}}{X \times Y \times Z}{\K}$.
    Suppose that there exist $r_1, r_2, r_3 \in [2,\infty]$ such that $X$ has cotype $r_{1}$, $Y$ has cotype $r_{2}$, and $Z$ has cotype $r_{3}$, with $\sum_{i=1}^{3} r_{i}^{-1} \geq 1$.
    Show that
    \begin{equation*}
      \Big| \sum_{n \in \N} \Pi_{n}(\mb{x}_{n}, \mb{y}_{n}, \mb{z}_{n}) \Big|
      \lesssim \Big(\sup_{n \in \N} \|\Pi_{n}\|_{\mathrm{Tri}(X \times Y \times Z)} \Big) \|\mb{x}_{\bullet}\|_{\varepsilon(X)} \|\mb{y}_{\bullet}\|_{\varepsilon(Y)} \|\mb{z}_{\bullet}\|_{\varepsilon(Z)}.
    \end{equation*}
  \end{itemize}
\end{exercise}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
