
\todo{Write expository introduction}

\subsection{Gambling in Banach spaces}\label{sec:gambling}

To motivate the theory in this section, we're going to imagine a betting game.
At each turn, you bet on the outcome of a coin toss.
The quantities that you can bet are taken from a Banach space $X$.
The initial state of your wallet, $\mb{s}_{-1}$, is the zero vector
\begin{equation*}
  \mb{s}_{-1} = 0 \in X.
\end{equation*}
At each time $n \in \N = \{0,1,\ldots\}$, you choose a vector $\mb{x}_n \in X$ to wager.
I then flip a fair coin, which shows either Heads or Tails, and the state of your wallet becomes
\begin{equation*}
  \mb{s}_n =
  \begin{cases}
    \mb{s}_{n-1} + \mb{x}_n & \text{if the coin shows Heads} \\
    \mb{s}_{n-1} - \mb{x}_n & \text{if the coin shows Tails.}
  \end{cases}
\end{equation*}
The Banach space $X$ is not ordered, so there is no canonical notion of $\mb{s}_n$ being `more' or `less' than $\mb{s}_{n-1}$. Thus the game is not about winning or losing (the true winner of the game is Functional Analysis).
In this chapter we will discuss various probabilistic concepts that can be well-understood in the context of this game.


\subsection{Filtrations and stochastic processes}

\begin{defn}
  A \emph{filtration} on a probability space $(\Omega,\mc{A},\P)$ is a monotone increasing (i.e. nondecreasing) sequence of $\sigma$-subalgebras
  \begin{equation*}
    \mc{A}_0 \subseteq \mc{A}_1 \subseteq \mc{A}_2 \subseteq \cdots \subseteq \mc{A}.
  \end{equation*}
\end{defn}

Filtrations are closely linked with stochastic processes.
While we don't plan on saying anything really serious about these in this course, it will be useful to keep the core concept in mind, as it guides a lot of probabilistic intuition.

\begin{defn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  A \emph{(discrete-time) $X$-valued stochastic process} on $(\Omega,\mc{A},\P)$ is a sequence of $\mc{A}$-measurable random variables $\map{f_n}{\Omega}{X}$, $n \in \N$.
  Given a filtration $(\mc{A}_n)_{n \in \N}$, a stochastic process $(f_n)_{n \in \N}$ is called \emph{predictable} (with respect to the filtration) if each $f_n$ is $\mc{A}_{n-1}$-measurable (with the convention that $\mc{A}_{-1} = \{\varnothing, \Omega\}$).
\end{defn}

\begin{rmk}
  With obvious modifications one can talk about filtrations and stochastic processes starting at an arbitrary index, finite filtrations/processes, or filtrations/processes with respect to arbitrary (total or partial) orders, for example with a continuous time index.
  In this course we will only consider discrete indexing sets contained in $\N$.
\end{rmk}

One should think of a filtration $(\mc{A}_n)_{n=0}^\infty$ as representing the progression of available information over time, usually in relation to a stochastic process.
Each $\sigma$-subalgebra $\mc{A}_n \subset \mc{A}$ represents the information available at time $n$.
There are two equivalent ways of thinking about the availability of information: one is that at time $n$ one has access to all $\mc{A}_n$-measurable subsets; the other is that at time $n$ one has access to all $\mc{A}_n$-measurable functions.
The monotonicity assumption says that no information is lost as time progresses.
Predictability of a stochastic process $(f_n)_{n \in \N}$ with respect to the filtration $(\mc{A}_n)_{n \in \N}$ thus says the following: if the available information is represented by $(\mc{A}_n)_{n \in \N}$, then at each time $n$, one already has access to the $\mc{A}_n$-measurable function $f_{n+1}$.

\begin{example}\label{eg:filtration-generated-by-process}
  Let $(\Omega,\mc{A},\P)$ be a probability space, $X$ a Banach space, and let $(f_n)_{n \in \N}$ be an $X$-valued stochastic process on $(\Omega,\mc{A},\P)$.
  The \emph{filtration generated by the process $(f_n)_{n \in \N}$} is given by
  \begin{equation*}
    \mc{F}_n := \sigma(f_0,f_1,\ldots,f_n) \qquad \forall n \in \N.
  \end{equation*}
  The information-theoretic intuition says that at time $n \in \N$, one `knows' the functions $f_0, f_1, \ldots f_n$, as these are in $\mc{F}_n$, and one also knows all functions of the form
  \begin{equation*}
    g \circ (f_0, f_1, \ldots, f_n) \colon \omega \mapsto g(f_0(\omega),f_1(\omega),\ldots,f_n(\omega))
  \end{equation*}
  where $\map{g}{X^{n+1}}{\C}$ is measurable (as such compositions are automatically measurable).
  In fact, all $\mc{F}_n$-measurable functions $\Omega \to \C$ are of this form.\todo{cite a reference} %mk
\end{example}

\begin{example}\label{eg:gambling-filtrations}
  Consider the game we introduced in Section \ref{sec:gambling}.
  At each time $n \in \N$ I flip a fair coin, which comes up Heads ($H$) or Tails $(T)$ with equal probability.
  The natural probability space on which to base this game is the infinite product $\Omega = \{-1,+1\}^{\N}$, where each factor has the uniform probability measure, and the product has the natural product $\sigma$-algebra and probability measure.
  The value $-1$ represents Tails, while $+1$ represents Heads.
  For each $n \in \N$ let $\map{\pi_n}{\Omega}{\{-1,+1\}}$ be the $n$-th coordinate function, which represents the outcome of the $n$-th coin toss.
  The sequence of functions $(\pi_n)_{n \in \N}$ is then a scalar-valued stochastic process.
  Let $\mc{F}_n = \sigma(\pi_0,\pi_1,\ldots,\pi_n)$ be the filtration generated by this process.

  
  Your bet at time $n$, the vector $\mb{x}_n \in X$, is allowed to depend on the outcomes $\pi_0, \pi_1, \ldots, \pi_{n-1}$: you do not need to register all your bets in advance.
  In probabilistic language, $\map{\mb{x}_n}{\Omega}{X}$ is $\mc{F}_{n-1}$-measurable, i.e. the sequence $(\mb{x}_n)_{n \in \N}$ is a stochastic process which is predictable with respect to the filtration $(\mc{F}_n)_{n \in \N}$.

  Now consider the stochastic process $(\mb{s}_n)_{n \in \N}$, representing the evolution of the state of your wallet.
  By definition we have
  \begin{equation*}
    \mb{s}_{n+1} = \mb{s}_n + \pi_{n+1} \mb{x}_{n+1} \qquad \forall n \in \N;
  \end{equation*}
  keep in mind that this is an equality of $X$-valued random variables, i.e. functions $\Omega \to X$.
  Since $\mb{s}_n$, $\pi_{n+1}$, and $\mb{x}_{n+1}$ are all $\mc{F}_{n+1}$-measurable, we find that $\mb{s}_{n+1}$ is $\mc{F}_{n+1}$-measurable (i.e. we know the state of our wallet $\mb{s}_{n+1}$ at time $n+1$).
  Heuristically, $\mb{s}_{n+1}$ should not be $\mc{F}_n$-measurable unless $\mb{x}_{n+1} \equiv 0$, as this would amount to predicting the future (which can only be done by wagering nothing).
  You should prove this rigourously (Exercise \ref{ex:winnings-unpredictability}).
\end{example}


\begin{defn}
  Given a filtration $(\mc{A}_n)_{n=0}^\infty$ on a probability space $(\Omega, \mc{A}, \P)$, a random variable $\map{T}{\Omega}{\N \cup \{\infty\}}$ is called a \emph{stopping time} (with respect to $(\mc{A}_n)$) if 
  \begin{equation*}
    \{\omega \in \Omega: T(\omega) \leq n\} \in \mc{A}_n \qquad \forall n \geq 0.
  \end{equation*}
  The stopping time $T$ is \emph{finite} if $T$ is almost surely finite.
\end{defn}

Generally stopping times $T$ are defined in terms of some kind of stochastic \emph{stopping condition}.
Interpreting the filtration $(\mc{A}_n)_{n \in \N}$ as modelling the available information at time $n$, $T$ being a stopping time says precisely that at time $n$, one `knows' the set of points $\omega \in \Omega$ for which $T(\omega) \leq n$.
Said less precisely, if $T$ is a stopping time, then at time $n$, one can determine whether or not $T \leq n$.

\begin{example}\label{eg:gambling-stoppingtimes}
  We return to the betting game of Section \ref{sec:gambling}, elaborated upon in Example \ref{eg:gambling-filtrations}.
  Let's suppose that our goal is to get the state of our wallet $\mb{s} \in X$ into a fixed Borel measurable set $K \subset X$, and that we intend to stop betting once this condition holds (i.e. from that point on we only wager the zero vector).
  
  Let
  \begin{equation*}
    T_K(\omega) := \inf\{n \in \N : \mb{s}_n(\omega) \in K \}
  \end{equation*}
  with the usual convention that $T_K(\omega) = \infty$ if $\mb{s}_n(\omega) \notin K$ for all $n \in \N$.
  That is, $T_K$ is the first time $n$ at which $\mb{s}_n \in K$.
  At time $n$ we heuristically know whether or not our wallet satisfied $\mb{s}_m \in K$ for some $m \leq n$, which indicates that $T_K$ should be a stopping time with respect to the filtration $(\mc{F}_n)_{n \in \N}$ associated with the stochastic process $(\pi_n)_{n \in \N}$.
  Rigourously, one shows this by writing for all $n \in \N$
  \begin{equation*}
    \begin{aligned}
      \{\omega \in \Omega : T_K(\omega) \leq n\}
      &= \big\{\omega : \inf\{m : \mb{s}_m(\omega) \in K\} \leq n\big\} \\
      &= \{\omega  : \text{$\mb{s}_m(\omega) \in K$ for some $m \leq n$}\} \\
      &= \bigcup_{m = 0}^n \mb{s}_m^{-1}(K),
    \end{aligned}
  \end{equation*}
  and noting that since each $\mb{s}_m$ is $\mc{F}_m$-measurable, the set above is $\mc{F}_n$-measurable.
  Thus $T_K$ is a stopping time.
  Of course, whether $T_K$ is a finite stopping time depends on the set $K \subset X$, the wager vectors $(\mb{x}_n)_{n \in \N}$, and potentially even the geometry of $X$ (see Exercise \ref{ex:gambling-in-linfty}).
\end{example}

The proof of the following proposition was already done in the previous exercise for the a particular stochastic process, but the proof is identical for a general stochastic process.

\begin{prop}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space.
  Let $(f_n)_{n \in \N}$ be an $X$-valued stochastic process and $K \subset X$ a Borel measurable set.
  Then the function $\map{T_K}{\Omega}{\N \cup \{\infty\}}$ defined by
  \begin{equation*}
    T_K(\omega) := \inf\{n \in \N : f_n(\omega) \in K \}
  \end{equation*}
  is a stopping time.
\end{prop}

The stopping time $T_K$ defined above is called the \emph{first hitting time of $K$}.

\subsection{Conditional expectations}

\todo{a bit of exposition; include defn of $\E$ and talk about `best approximation with given information'}

\begin{defn}\label{defn:conditional-expectation} %Dudley, p336
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space.
  Let $f \in L^1(\Omega, \mc{A}; X)$ be a integrable $X$-valued random variable.
  Given a $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a \emph{conditional expectation of $f$ given $\mc{B}$} is a $\mc{B}$-measurable random variable $\E^{\mc{B}}f \in L^1(\Omega, \mc{B}; X) \subset L^1(\Omega,\mc{A};X)$ such that
  \begin{equation}\label{eq:conditional-expectation-property}
    \qquad \int_B \E^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P \qquad \text{for all $B \in \mc{B}$.}
  \end{equation}
\end{defn}

\begin{example}
  Let $(\Omega,\mc{A},\P)$ be a probability space and let $\mc{B} \subset \mc{A}$ be a sub-$\sigma$-algebra which is \emph{atomic}, in the sense that there is a collection of pairwise disjoint subsets $(B_\lambda)_{\lambda \in \Lambda}$ of $\mc{B}$ which generate $\mc{B}$, such that $\P(B_{\lambda}) > 0$ for all $\lambda$, and such that if $B_\lambda$ can be written as a disjoint union $B_\lambda = C \cup D$ for some sets $C,D \in \mc{B}$, then $\P(C) = 0$ or $\P(D) = 0$ (i.e. the sets $B_\lambda$ are \emph{atoms}).
  Let's compute \emph{the} conditional expectation $\E^{\mc{B}}f$ of an integrable random variable $f \in L^1(\mc{A};X)$ (it turns out there is only one).
  Since the atoms $(B_\lambda)_{\lambda}$ generate $\mc{B}$ and are pairwise disjoint, and since $\E^{\mc{B}}f$ is $\mc{B}$-measurable, $\E^{\mc{B}}f$ must be constant on each $B_\lambda$, so that
  \begin{equation*}
    \E^{\mc{B}} f = \sum_{\lambda \in \Lambda} \1_{B_\lambda} \otimes \mb{x}_\lambda
  \end{equation*}
  for some vectors $\mb{x}_{\lambda} \in X$.
  Averaging over one of the atoms $B_{\lambda}$ and using \eqref{eq:conditional-expectation-property} tells us that
  \begin{equation*}
    \begin{aligned}
      \mb{x}_\lambda = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} \E^{\mc{B}} f \, \dd\P = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} f \, \dd\P.
    \end{aligned}
  \end{equation*}
  In probabilistic terms, the quantity on the right hand side is the conditional expectation of $f$ given $B_\lambda$, which exists since $\P(B_\lambda) > 0$.
\end{example}

The previous example shows that conditional expectations with respect to atomic $\sigma$-algebras exist and are unique.
The same is true for general $\sigma$-algebras, but proving this will take a few steps.
First we will establish a foundational result in the scalar-valued case.

\begin{thm}\label{thm:conditional-expectation-EU}
  Let $(\Omega,\mc{A},\P)$ be a probability space.
  For any $f \in L^1(\mc{A})$ and any $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a unique (up to almost sure equality) conditional expectation $\E^{\mc{B}}f$ exists.
  Furthermore, for all $p \in [1,\infty]$, $\E^{\mc{B}}$ is a positive contraction on $L^p(\mc{A})$: that is, if $f \in L^p(\mc{A})$, then
  \begin{equation*}
    \|\E^{\mc{B}}f\|_{p} \leq \|f\|_{p},
  \end{equation*}
  and if $f$ is a.s. nonnegative then so is $\E^{\mc{B}}f$.
\end{thm}

\begin{proof}
  First we handle uniqueness.
  For this it suffices to consider the case $\K = \R$, as the case $\K = \C$ follows by considering real and imaginary parts.
  Suppose that $\E^{\mc{B}}f$ and $\td{E}^{\mc{B}}f$ are two conditional expectations of $f$ given $\mc{B}$.
  For all subsets $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P - \int_B f \, \dd\P = 0.
  \end{equation*}
  Since $\E^{\mc{B}}f - \td{\E}^{\mc{B}}f$ is $\mc{B}$-measurable, the subsets
  \begin{equation*}
    B_+ := \{ \E^{\mc{B}}f - \td{\E}^{\mc{B}}f > 0\} \quad \text{and} \quad B_- := \{\E^{\mc{B}}f - \td{\E}^{\mc{B}}f < 0\}
  \end{equation*}
  are both in $\mc{B}$, so we get
  \begin{equation*}
    \int_\Omega |\E^{\mc{B}}f - \td{\E}^{\mc{B}}f | \, \dd\P
    = \int_{B_+}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P
    -  \int_{B_-}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P
    = 0,
  \end{equation*}
  establishing that $\E^{\mc{B}}f = \td{\E}^{\mc{B}}f$ almost surely.

  We can prove positivity from the defining property \eqref{eq:conditional-expectation-property}, before we establish existence.
  Suppose $f \in L^1(\mc{A})$ is a.s. nonnegative.
  Then for all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P = \int_B f \, \dd\P \geq 0,
  \end{equation*}
  which implies that the $\mc{B}$-measurable function $\E^{\mc{B}}f$ is a.s. nonnegative.\footnote{This uses an exercise from measure theory: if $g$ is $\mc{B}$-measurable and $\int_B g \geq 0$ for all $\mc{B}$-measurable sets, then $g \geq 0$ a.e.. Proof: the set $N := \{g(\omega) < 0\}$ is $\mc{B}$-measurable, and assuming it has positive measure leads to the contradiction $0 \leq \int_B g < 0$.}


  Now we prove existence.
  Fix $p \in [1,\infty]$ and let $f \in L^p(\mc{A})$; we will construct a contractive conditional expectation $\E^{\mc{B}}f \in L^p(\mc{B};\K) \subset L^p(\mc{A};\K)$ directly.

  \textbf{Mild case: $p > 1$}, so most importantly $p' < \infty$.
  The inclusion map $\map{\iota}{L^{p'}(\mc{B})}{L^{p'}(\mc{A})}$ is contractive, so (using that $L^p$ is the dual of $(L^{p'})^*$, which requires $p' < \infty$) its adjoint $\map{\iota^*}{L^p(\mc{A})}{L^p(\mc{B})}$ is also contractive.
  For all $f \in L^p(\mc{A})$ and $B \in \mc{B}$ we then have
  \begin{equation*}
    \int_B \iota^* f \, \dd\P = \langle \iota^* f, \1_{B} \rangle = \langle f, \iota \1_{B} \rangle = \langle f, \1_{B} \rangle = \int_B f \, \dd\P,
  \end{equation*}
  so $\iota^* f \in L^p(\mc{B}) \subset L^1(\mc{B})$ is a conditional expectation of $f$ given $\mc{B}$.

  \textbf{(German) spicy case: $p = 1$.} The difficulty here is that $L^1$ is \emph{strictly} contained in the dual of $L^\infty$, so we can't just take an adjoint of the inclusion $L^\infty(\mc{B}) \to L^\infty(\mc{A})$.
  Instead we argue by density.
  We have that $L^2(\mc{A})$ is dense in $L^1(\mc{A})$, so we aim to extend the conditional expectation defined above (in the case $p=2$) by continuity.
  For $f \in L^2(\mc{A})$ and $g \in L^\infty(\mc{B})$ we have
  \begin{equation*}
    |\langle \iota^* f, g \rangle| = |\langle f, \iota g \rangle| = |\langle f, g \rangle| \leq \|f\|_1 \|g\|_\infty
  \end{equation*}
  using that $L^\infty(\mc{B}) \subset L^2(\mc{B})$.
  Taking the supremum over all nonzero $g \in L^\infty(\mc{B})$ proves that
  \begin{equation*}
    \|\iota^* f\|_1 \leq \|f\|_1,
  \end{equation*}
  so $\iota^*$ extends to a contraction $L^1(\mc{A}) \to L^1(\mc{B})$.
  For $f \in L^1(\mc{A})$ and $B \in \mc{B}$, using that integration on $B$ is a continuous linear functional on $L^1$, we have
  \begin{equation*}
    \int_B \iota^* f \, \dd\P
    = \lim_{n \to \infty} \int_B \iota^* f_n \, \dd\P
    = \lim_{n \to \infty} \int_B  f_n \, \dd\P
    = \int_B f \, \dd\P
  \end{equation*}
  where $f_n$ is a sequence in $L^2(\mc{A})$ converging to $f$ in $L^1(\mc{A})$.
  Thus the continuous extension of $\iota^*$ is a conditional expectation of $f$, and we are done.
\end{proof}

Note that the proof of the previous result also establishes the following adjoint relation (the case $p=\infty$ involves a few lines of work which are left as an exercise).\todo{include exercise}

\begin{prop}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$.
  For all $p \in (1,\infty]$, the conditional expectation $\E^{\mc{B}}$ on $L^p(\mc{A})$ is the adjoint of the corresponding conditional expectation on $L^{p'}(\mc{A})$. 
\end{prop}

Now we can use the extension theorem for positive operators to show the existence of conditional expectations of Banach-valued random variables.

\begin{prop}
  Let $(\Omega,\mc{A},\P)$ be a probability space, let $\mc{B}$ be a $\sigma$-subalgebra of $\mc{A}$, and let $X$ be a Banach space.
  Then for any $f \in L^1(\mc{A};X)$, a unique (up to a.s. equality) conditional expectation $\E_X^{\mc{B}}f$ exists.
  Furthermore, for all $p \in [1,\infty]$, $\E^{\mc{B}}$ is a contraction on $L^p(\mc{A};X)$.
\end{prop}

\begin{proof}
  \todo{do this after sorting out real vs. complex Banach spaces}
\end{proof}


{\color{blue}
\begin{defn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  Given a $\sigma$-subalgebra $\mc{B} \subset \mc{A}$ and an exponent $p \in [1,\infty)$, we define the \emph{conditional expectation operator}
  \begin{equation*}
    \map{\E_X^{\mc{B}}}{L^p(\Omega,\mc{A},\P;X)}{L^p(\Omega,\mc{B};\P;X)} 
  \end{equation*}
  as the unique bounded linear extension of
  \begin{equation*}
    \map{\E^{\mc{B}} \otimes I}{L^p(\Omega,\mc{A},\P) \otimes X}{L^p(\Omega,\mc{B},\P) \otimes X}.
  \end{equation*}
  We will often overload notation and write $\E^{\mc{B}}$ to denote $\E_X^{\mc{B}}$.
\end{defn}
\todo{up to here}
Since the conditional expectation $\E^{\mc{B}}$ on scalar-valued functions is positive and contractive on $L^p$ for all $p \in [1,\infty)$, Theorem \ref{thm:positive-extensions} tells us that $\E_X^{\mc{B}}$ is well-defined and is in fact a contraction on $L^p(\Omega,\mc{A},\P;X)$ for all $p \in [1,\infty)$.
For $p = \infty$, since $L^\infty(\Omega;X)$ is lacking in convenient dense subspaces, we can't argue so abstractly.
Here is a somewhat ad-hoc argument for contractivity on $L^\infty$.

\begin{prop}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  Then the conditional expectation $\E_X^{\mc{B}}$, which \emph{a priori} maps
  \begin{equation*}
    \map{\E_X^{\mc{B}}}{L^\infty(\Omega,\mc{A},\P;X) \subset L^1(\Omega,\mc{A},\P;X)}{L^1(\Omega,\mc{B},\P;X)},
  \end{equation*}
  is a contraction on $L^\infty(\Omega,\mc{A},\P;X)$.
\end{prop}

\begin{proof}
  Let $f \in L^\infty(\Omega,\mc{A},P;X)$.
  Since $L^\infty(\Omega,\mc{A},\P;X) \subset L^2(\Omega,\mc{A},\P;X)$, $E^{\mc{B}} f = \td{P^{\mc{B}}} f$ can be seen as the tensor extension of the orthogonal projection of $L^2(\Omega,\mc{A},\P)$ onto $L^2(\Omega, \mc{B},\P)$ (as in the proof of Theorem \ref{thm:conditional-expectation-EU}).
  By Proposition \ref{prop:bochner-preduality}, we can test $\|\E_X^{\mc{B}} f\|_\infty$ by duality, using that $L^2(\Omega,\mc{B}; X^*)$ is dense in $L^1(\Omega, \mc{B}; X^*)$.
  For all nonzero $g \in L^2(\Omega, \mc{B}; X^*)$ we have
\begin{equation*}
  \begin{aligned}
    | \langle \E_X^{\mc{B}} f, g \rangle |
    =  | \langle \td{P^{\mc{B}}} f, g \rangle | 
    \stackrel{(*)}{=} | \langle f, \td{P^{\mc{B}}} g \rangle | 
    &=  | \langle f, \E_X^{\mc{B}} g \rangle | \\
    &\leq  \|f\|_{L^\infty(\Omega,\mc{A},\P;X)} \|\E_X^{\mc{B}} g\|_{L^1(\Omega,\mc{A},\P;X^*)} \\
    &\leq  \|f\|_{L^\infty(\Omega,\mc{A},\P;X)} \| g\|_{L^1(\Omega,\mc{A},\P;X^*)}.
  \end{aligned}
\end{equation*}
For the starred equality see Exercise \ref{ex:tensor-adjoint} in the previous chapter.
Taking the supremum over all such $g$ completes the proof.
\end{proof}

}


\todo{further properties of vector-valued conditional expectations}

\begin{rmk}
  We have only considered conditional expectations $\E^{\mc{B}}$ on probability spaces, but the concept can be extended to general measure spaces $(S,\mc{A},\mu)$ provided that the measure $\mu$ is $\sigma$-finite on the $\sigma$-subalgebra $\mc{B} \subset \mc{A}$ (although the arguments require a fair bit of modification).
  This approach is taken in \cite{HNVW16}.
\end{rmk}


\subsection{Definitions and key examples of martingales}

\begin{defn}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and let $X$ be a Banach space.
  A sequence of integrable $X$-valued random variables $(M_n)_{n \in \N}$ in $L^1(\Omega, \mc{A}, \P; X)$ is called a \emph{martingale} with respect to a filtration $(\mc{A}_n)_{n \in \N}$ if
  \begin{itemize}
  \item $M_n$ is $\mc{A}_n$-measurable for all $n \in \N$,
  \item for all $n \in \N$,
    \begin{equation}\label{eq:mgale-defining-property}
      M_n = \E^{\mc{A}_n} M_{n+1}.
    \end{equation}
  \end{itemize}
\end{defn}

\begin{rmk}
  Now is a good time to complete Exercise \ref{ex:martingale-elementary-properties}, establishing a few elementary properties of martingales.
\end{rmk}

-stopped martingales
-Rademacher variables and sums

\begin{defn}
  limiting $\sigma$-algebra 
\end{defn}

\begin{thm}[$L^p$-convergence of martingales]
  %Pisier Theorem 1.14
  
\end{thm}

\subsection{Maximal inequalities and pointwise convergence}
% proofs from Pisier with little modifications to allow for infinite martingales


\begin{defn}
  definition of $\R$-valued submartingale
\end{defn}

As an example, consider a martingale $(f_n)_{n \in \N}$ taking values in a Banach space $X$.
Then by {\color{red} CITE PART OF POSITIVE OP EXTENSION HERE} we have for all $m \geq n$
\begin{equation*}
  \|f_n\|_X = \|\E^{\mc{F}_n} f_m\|_X \leq \E^{\mc{F}_n} \|f_m\|_X \qquad \text{almost surely},
\end{equation*}
so that the sequence $(\|f_n\|_X)_{n \in \N}$ is an $\R$-valued submartingale.

\begin{thm}[Doob's maximal inequalties]
  
\end{thm}

% i think we can do without this
% but include it if necessary
% \begin{thm}[Burkholder--Davis--Gundy inequality]
% \end{thm}

\begin{thm}[Martingale convergence theorem]
  
\end{thm}



\subsection{John--Nirenberg for adapted sequences and the Kahane--Khintchine inequalities}
% following HNVW1 section 3.2.c

Fix a Banach space $X$ and a $\sigma$-finite measure space $(S,\mc{A},\mu)$.\todo{put this all on a probability space}
Consider a filtration $(\mc{F}_n)_{n \in \N}$ and a sequence $\phi = (\phi_n)_{n \in \N}$ of $X$-valued functions adapted to the filtration (i.e. $\phi_n$ is $\mc{F}_n$-measurable for all $n \in \N$).\todo{Strongly?}
For all $q \in (0,\infty)$ we consider the following measure of the oscillation of $\phi$:
\begin{equation*}
    \|\phi\|_{*,q} := \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{\substack{F \in \mc{F}_k \\ 0 < \mu(F) < \infty}} \Big( \fint_{F} \|(\phi_n - \phi_{k-1})(s)\|_{X}^{q} \, \dd\mu(s)\Big)^{1/q} .
\end{equation*}


\begin{thm}[John--Nirenberg inequality for adapted sequences]\label{thm:jn-adapted-sequences}
  With notation as above, for all $p,q \in (0,\infty)$ there exists a finite constant $c_{p,q}$ independent of $\phi$ such that
  \begin{equation*}
    \|\phi\|_{*,p} \leq c_{p,q} \|\phi\|_{*,q}.
  \end{equation*}
\end{thm}

We will prove this as a consequence of a series of lemmas, but before proving this, we demonstrate an important application to Rademacher sums.

\begin{thm}[Kahane--Khintchine inequality]\label{thm:kk}
  Let $X$ be a Banach space and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega,\mc{F},\P)$.
  Then for all $p,q \in (0,\infty)$ there exists a finite constant $\kappa_{p,q}$ such that for all finite sequences $(\mb{x}_n)_{n=1}^N$ in $X$,
  \begin{equation*}
    \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} \leq \kappa_{p,q} \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
  \end{equation*}
  That is, for all $p \in (0,\infty)$, the $L^p$-norms of a Rademacher sum are pairwise equivalent.  
\end{thm}

Since $\Omega$ is a probability space, H\"older's inequality yields the case $p \leq q$ with constant $\kappa_{p,q} = 1$, so we only need to consider the case $q < p$.

\begin{proof}[Proof, assuming the John--Nirenberg inequality]
  Consider the filtration and adapted sequence \todo{make sure to discuss filtrations generated by functions, particularly Rademacher variables} 
  \begin{equation*}
    \mc{F}_n := \sigma(\{\varepsilon_j : 1 \leq j \leq n\}), \qquad \phi_n := \sum_{j=1}^n \varepsilon_j \mb{x}_j.
  \end{equation*}
  We claim that for all $q \in [1,\infty)$ we have
  \begin{equation}\label{eq:kk-claim}
    \|\phi\|_{*,q} = \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^q(\Omega;X)}.
  \end{equation}
  Assuming this for the moment, the John--Nirenberg inequality yields a finite constant $c_{p,q}$ such that
  \begin{equation*}
    \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^p(\Omega;X)}
    = \|\phi\|_{*,p}
    \leq c_{p,q} \|\phi\|_{*,q}
    = \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^q(\Omega;X)}
  \end{equation*}
  whenever $1 \leq q < p$.
  If $q < 1 \leq p$, we fix $\theta \in (0,1)$ so that $1/p = \theta/q + (1-\theta)/2p$, and log-convexity of $L^p$-norms gives
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)}
    \leq \| \phi_N \|_{L^q(\Omega;X)}^{\theta} \| \phi_N \|_{L^{2p}(\Omega;X)}^{1 - \theta}
    \leq \| \phi_N \|_{L^q(\Omega;X)}^{\theta} (c_{2p,p} \| \phi_N \|_{L^{p}(\Omega;X)})^{1 - \theta},
  \end{equation*}
  which yields
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)} \leq c_{2p,p}^{(1 - \theta)/\theta} \| \phi_N \|_{L^q(\Omega;X)}.
  \end{equation*}
  Finally, if $q < p < 1$, we simply estimate
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)} \leq \| \phi_N \|_{L^1(\Omega;X)} \leq c_{1,q} \| \phi_N \|_{L^q(\Omega;X)}.
  \end{equation*}
  This covers all nontrivial cases, and it remains to prove the claimed equality \eqref{eq:kk-claim}.

  First recall that
  \begin{equation*}
    \|\phi\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{\substack{F \in \mc{F}_k \\ \mu(F) > 0}} \Big( \fint_{F} \|(\phi_n - \phi_{k-1})(s)\|_{X}^{q} \, \dd\P(s)\Big)^{1/q}.
  \end{equation*}
  Fix $k \leq n$ and a set $F \in \mc{F}_k$ of positive measure.
  We will compute
  \begin{equation*}
    \fint_{F} \|\1_{F} (\phi_n - \phi_{k-1})(s)\|_{X}^q \, \dd\P(s) = \P(F)^{-1} \E \Big( \1_{F} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big).
  \end{equation*}
  Observe that for all $\omega \in \Omega$
  \begin{equation*}
    \Big\| \sum_{j=k}^{n} \varepsilon_j(\omega) \mb{x}_j\Big\|_{X} = \Big\| \varepsilon_k(\omega) \Big( \mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime}(\omega) \mb{x}_j \Big) \Big\|_{X} = \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}
  \end{equation*}
  where
  \begin{equation*}
    \varepsilon_{j}^{\prime} :=
    \begin{cases}
      \varepsilon_{j} & j \leq k \\
      \varepsilon_{k} \varepsilon_{j} & k+1 \leq j,
    \end{cases}
  \end{equation*}
  since $\varepsilon_j$ is $\pm 1$-valued.
  Now note that the $\sigma$-algebra $\mc{F}^\prime_{k+1,n} := \sigma(\{\varepsilon_j^\prime : k+1 \leq j \leq n\})$ is independent of $\mc{F}_k$, since for all $1 \leq j \leq k$ and $k+1 \leq j' \leq n$ we have by independence of the original Rademacher sequence
  \begin{equation*}
    \E(\varepsilon_j \varepsilon_{j'}^\prime) = \E(\varepsilon_j \varepsilon_k \varepsilon_{j'})
    =
    \begin{cases}
      \E(\varepsilon_j) \E(\varepsilon_k) \E(\varepsilon_{j'}) & \text{if $j < k$} \\
       \E(\varepsilon_{j'}) & \text{if $j = k$}
    \end{cases}
    \quad = 0.
  \end{equation*}
  Thus, since $F \in \mc{F}_k$, we have by independence of $\mc{F}_k$ and $\mc{F}^{\prime}_{k+1, n}$
  \begin{equation*}
    \begin{aligned}
      \P(F)^{-1} \E \Big( \1_{F} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big)
      &=  \P(F)^{-1} \E \Big( \1_{F} \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} \Big) \\
      &=  \E \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} 
      =  \E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}. \\
    \end{aligned}
  \end{equation*}
  Now letting $\mc{F}_{k,n} := \sigma(\{\varepsilon_j : k \leq j \leq n\})$, we have by the $L^q$-contraction property of conditional expectations
  \begin{equation*}
    \E \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
    = \E \Big\|\E^{\mc{F}_{k,n}} \Big(\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big)\Big\|_{X}^{q}
    \leq \E \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
  \end{equation*}
  with equality when $k=1$ and $n=N$.
  Thus
  \begin{equation*}
    \|\phi\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}}  \Big(\E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q} = \Big(\E  \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q}
  \end{equation*}
  which proves the claimed equality \eqref{eq:kk-claim} and completes the proof. 
\end{proof}

In the setting of Hilbert-valued functions (and in particular, scalar-valued functions), the Kahane--Khintchine inequality leads to the classical Khintchine inequalities.

\begin{cor}[Khintchine's inequalities]
  Let $H$ be a Hilbert space, and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega, \mc{F}, \P)$.
  Then for all $p \in (0,\infty)$ there exist finite constants $A_p$ and $B_p$ such that for all finite sequences $(\mb{h}_n)_{n=1}^N$ in $H$,
  \begin{equation*}
    A_p \Big( \sum_{n=1}^N \|\mb{h}_n\|_H^2 \Big)^{1/2} \leq \Big\| \sum_{n=1}^N \varepsilon_n \mb{h}_n \Big\|_{L^p(\Omega;H)} \leq B_p \Big( \sum_{n=1}^N \|\mb{h}_n\|_H^2 \Big)^{1/2}.
  \end{equation*}
\end{cor}

\begin{proof}
  By independence of the Rademacher variables we have
  \begin{equation}
    \begin{aligned}
      \Big\| \sum_{n=1}^N \varepsilon_n \mb{h}_n \Big\|_{L^2(\Omega;H)}^2
      &= \Big\langle \sum_{n=1}^N \varepsilon_n \mb{h}_n , \sum_{m=1}^N \varepsilon_m \mb{h}_m \Big\rangle \\
      &= \sum_{n,m = 1}^N \E(\varepsilon_n \varepsilon_m) \langle \mb{h}_n, \mb{h}_m \rangle 
      = \sum_{n=1}^N \|\mb{h}_{n}\|_{H}^2,
    \end{aligned}
  \end{equation}
  so the result is true for $p=2$ with $A_2 = B_2 = 1$.
  Now use Kahane--Khintchine to extend the result to general $p \in (0,\infty)$.
\end{proof}

\subsubsection{Proof of the John--Nirenberg inequality}

We return to our analysis of a sequence $\phi = (\phi_n)_{n \in \N}$ of $X$-valued functions adapted to a filtration $(\mc{F}_n)_{n \in \N}$ on a $\sigma$-finite measure space $(S,\mc{A},\mu)$.
We prove the John--Nirenberg inequality via a series of lemmas in which we obtain increasingly fine control on the oscillation of $\phi$.

\begin{lem}\label{lem:JN-proof-1}
  For all $k \leq n$, $F \in \mc{F}_k$, and $\alpha > 0$,
  \begin{equation}\label{eq:JN-proof-1-est}
    \mu(F \cap \{ \|\phi_n - \phi_{k-1}\|_{X} > \alpha \}) \leq \Big( \frac{\|\phi\|_{*,q}}{\alpha} \Big)^{q} \mu(F).
  \end{equation}
\end{lem}

\begin{proof}
  We can assume that $0 < \mu(F) < \infty$, otherwise there is nothing to prove.
  The left hand side of \eqref{eq:JN-proof-1-est} is bounded by
  \begin{equation*}
    \int_{F} \Big( \frac{\|\phi_n - \phi_{k-1}\|_{X}}{\alpha} \Big)^{q} \, \dd\mu \leq \mu(F) \Big( \frac{\|\phi\|_{*,q}}{\alpha} \Big)^{q}
  \end{equation*}
  since $F \in \mc{F}_k$ and $k \leq n$, by the definition of $\|\phi\|_{*,q}$.
\end{proof}

Next we show that oscillation control of the form above extends to more general stopping times.

\begin{lem}
  Suppose that there exist $\alpha > 0$ and $\eta > 0$ such that
  \begin{equation*}
    \mu(F \cap \{ \| \phi_n - \phi_{k-1} \| > \alpha \} ) \leq \eta \mu(F) \qquad \forall k \leq n, F \in \mc{F}_k.
  \end{equation*}
  Then for all $k \in \N$, $F \in \mc{F}_k$, and all stopping times $\nu$ such that $\nu \geq k$ on $F$,
  \begin{equation}\label{eq:jn-stoptime}
    \mu(F \cap \{\nu < \infty\} \cap \{ \| \phi_{\nu} - \phi_{k-1} \| > 2\alpha \} ) \leq 2\eta \mu(F).
  \end{equation}
\end{lem}

\begin{proof}
  Sum over all possible values of the stopping time:
  \begin{equation*}
      \mu(F \cap \{\nu < \infty\} \cap \{ \| \phi_{\nu} - \phi_{k-1} \| > 2\alpha \} ) 
      = \lim_{N \to \infty} \sum_{n = k}^{N} \mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ),
  \end{equation*}
  where $F_{n} := F \cap \{\nu = n\}$.
  For fixed $N \geq n > k$, since $F_n \in \mc{F}_n \subset \mc{F}_{n+1}$, we have by assumption
  \begin{equation*}
    \begin{aligned}
      &\mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ) \\
      &\leq \mu(F_n \cap \{ \| \phi_{n} - \phi_{N} \| > \alpha \} )
      + \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \} ) \\
      &\leq \eta \mu(F_n) + \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \} ).
    \end{aligned}
  \end{equation*}
  Thus we have
  \begin{equation*}
    \begin{aligned}
      &\lim_{N \to \infty} \sum_{n = k}^{N} \mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ) \\
      &\leq \lim_{N \to \infty} \Big( \eta \sum_{n=k}^N \mu(F_n) + \sum_{n=k}^N \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \}) \Big) \\
      &\leq \eta \mu(F) + \lim_{N \to \infty} \mu(F \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \})  
      \leq 2\eta\mu(F)
    \end{aligned}
  \end{equation*}
  using the assumption and $F \in \mc{F}_k$ in the last estimate.
\end{proof}
  
In the following lemma we make use of the \emph{started sequence}
\begin{equation*}
  {}^{k-1}\phi = (\phi_n - \phi_{k-1})_{n \geq k-1}
\end{equation*}
and its maximal function
\begin{equation*}
  ({}^{k-1} \phi)^{*}(s) := \sup_{n \geq k-1} \|({}^{k-1}\phi)_n(s)\|_X = \sup_{n \geq k} \|(\phi_n - \phi_{k-1})(s)\|_X.
\end{equation*}

\begin{lem}\label{lem:jn-mf}
  Suppose that $\phi$ satisfies \eqref{eq:jn-stoptime} for all $k \in \N$, $F \in \mc{F}_k$, and all stopping times $\nu$ such that $\nu \geq k$ on $F$.
  Then for all $\lambda > 0$,
  \begin{equation}\label{eq:jn-mf-eq}
    \mu(F \cap \{ ({}^{k-1} \phi)^{*} > \lambda + 2\alpha \}) \leq 2\eta \mu(F \cap \{  ({}^{k-1} \phi)^{*} > \lambda \}) \qquad \forall k \in \N, F \in \mc{F}_{k}.
  \end{equation}
\end{lem}

\begin{proof}
  Fix $k \in \N$ and consider the stopping times
  \begin{equation*}
    \begin{aligned}
      \rho &:= \inf\{n \geq k : \|\phi_n - \phi_{k-1}\| > \lambda\}, \\
      \nu &:= \inf\{n \geq k : \|\phi_n - \phi_{k-1}\| > \lambda + 2\alpha\}.
    \end{aligned}
  \end{equation*}
  Then $k \leq \rho \leq \nu$, and \eqref{eq:jn-mf-eq} can be rewritten as
  \begin{equation*}
    \mu(F \cap \{\nu < \infty\}) \leq 2\eta \mu(F \cap \{\rho < \infty\}).
  \end{equation*}
  Now fix $n \geq k$ and let $F_n := F \cap \{\rho = n\} \in \mc{F}_n$.
  On $\{F_n \cap \{\nu < \infty\}\}$ we have
  \begin{equation*}
    \|\phi_{\nu} - \phi_{n-1}\| \geq \|\phi_{\nu} - \phi_{k-1}\| - \|\phi_{n-1} - \phi_{k-1}\| > (\lambda + 2\alpha) - \lambda = 2\alpha,
  \end{equation*}
  so
  \begin{equation*}
    \mu(F_n \cap \{\nu < \infty\}) = \mu(F_n \cap \{\nu < \infty\} \cap \{\|\phi_{\nu} - \phi_{n-1}\| > 2\alpha\} )
    \leq 2\eta \mu(F_n).
  \end{equation*}
  Summing over $n \geq k$ completes the proof.
\end{proof}

\begin{lem}
  Suppose that $f$ is a non-negative function supported in $F \in \mc{A}$, satisfying
  \begin{equation*}
    \mu(f > \lambda + \alpha) \leq \eta \mu(f > \lambda) \qquad \forall \lambda > 0
  \end{equation*}
  for some $\eta \in (0,1)$ and $\alpha > 0$.
  Then for all $p \in [1,\infty)$,
  \begin{equation*}
    \|f\|_p \leq \frac{1 + \eta^{1/p}}{1 - \eta^{1/p}} \alpha \mu(F)^{1/p}.
  \end{equation*}
\end{lem}

\begin{proof}
  {\color{red} WRITE PROOF}
\end{proof}

\todo{UP TO HERE. HAVE TO COMPLETE PROOF OF JOHN-NIRENBERG.}

{\color{blue}

\begin{equation*}
  \|\phi\|_{**,q} := \sup_{k \in \N} \sup_{\substack{F \in \mc{F}_k \\ 0 < \mu(F) < \infty}} \Big( \fint_{F} ({}^{k-1} \phi)^{*}(s)^q \, \dd\mu(s) \Big)^{1/q},
\end{equation*}

}



\subsection{Gundy's decomposition}

\subsection{The Radon--Nikodym property, martingale convergence, and Bochner space duality}\label{sec:RNP}

\subsection*{Exercises}

\begin{exercise}
  Let $(f_n)_{n \in \N}$ be a stochastic process on a probability space $(\Omega,\mc{A},\P)$.
  Suppose that $(f_n)$ is predictable with respect to the filtration generated by $(f_n)$ (see Example \ref{eg:filtration-generated-by-process}).
  Show that the process is deterministic, in the sense that each $f_n$ is constant.
\end{exercise}

\begin{exercise}\label{ex:winnings-unpredictability}
  In the setting of Example \ref{eg:gambling-filtrations}, show that the random variable $\mb{s}_{n+1}$ is $\mc{F}_n$-measurable if and only if $\mb{x}_{n+1} \equiv 0$.
\end{exercise}

\begin{exercise}\label{ex:gambling-in-linfty}
  This exercise takes place in the setting of Example \ref{eg:gambling-stoppingtimes}.
  \begin{itemize}
  \item
    Let $X = \ell^\infty(\N)$.
    Suppose that the wager vectors $\map{\mb{x}_n}{\Omega}{\ell^\infty(\N)}$ are such that for all $\omega \in \Omega$, the vectors $(\mb{x}_n(\omega))_{n \in \N}$ are pairwise distinct standard basis vectors (i.e. $\{0,1\}$-valued sequences, zero for all but one index).
    Fix $\lambda > 0$ and let $K = \{\mb{a} \in \ell^\infty(\N) : \|\mb{a}\|_\infty \geq \lambda\} = \ell^\infty(\N) \sm B_\lambda(0)$.
    Show that the stopping time
    \begin{equation*}
      T_K(\omega) := \inf\{n \in \N : \mb{s}_n(\omega) \in K\} 
    \end{equation*}
    is finite if and only if $\lambda \leq 1$.
  \item
    As above, but now let $X = \ell^2(\N)$, and show that the stopping time $T_K$ is finite for all $\lambda > 0$.
  \end{itemize}
\end{exercise}

\begin{exercise}\label{ex:martingale-elementary-properties}
  Let $(\Omega,\mc{A},\P)$ be a probability space, $X$ a Banach space, and let $(M_n)_{n \in \N}$ be a martingale with respect to some filtration $(\mc{A}_n)_{n \in \N}$.
  \begin{itemize}
  \item
    Show that $M_n  \E^{\mc{A}_n} M_m$ for all $n,m \in \N$ with $m > n$,
  \item
    Let $(\mc{F}_n)_{n \in \N}$ be the filtration generated by $(M_n)_{n \in \N}$, i.e.
    \begin{equation*}
      \mc{F}_n := \sigma(M_0, M_1, \ldots, M_n).
    \end{equation*}
    Show that $(M_n)_{n \in \N}$ is a martingale with respect to $(\mc{F}_n)_{n \in \N}$.
  \item
    For all $p \in [1,\infty]$, show that the sequence $\|M_n\|_{L^p(\Omega,\P;X)}$ is monotonically increasing in $n$.
  \end{itemize}

\end{exercise}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
