\subsection{Conditional expectations}


\begin{defn}\label{defn:conditional-expectation} %Dudley, p336
  Let $(\Omega, \mc{A}, \P)$ be a probability space, and let $f \in L^1(\Omega, \mc{A}; \P)$ be an integrable random variable.
  Given a $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a \emph{conditional expectation of $f$ given $\mc{B}$} is a $\mc{B}$-measurable random variable $\E^{\mc{B}}f$ such that
  \begin{equation}\label{eq:conditional-expectation-property}
    \qquad \int_B \E^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P \qquad \text{for all $B \in \mc{B}$.}
  \end{equation}
\end{defn}

\begin{example}
  Let $(\Omega,\mc{A},\P)$ be a probability space and let $\mc{B} \subset \mc{A}$ be a sub-$\sigma$-algebra which is \emph{atomic}, in the sense that there is a collection of pairwise disjoint subsets $(B_\lambda)_{\lambda \in \Lambda}$ of $\mc{B}$ which generate $\mc{B}$, such that $\P(B_{\lambda}) > 0$ for all $\lambda$, and such that if $B_\lambda$ can be written as a disjoint union $B_\lambda = C \cup D$ for some sets $C,D \in \mc{B}$, then $\P(C) = 0$ or $\P(D) = 0$ (i.e. the sets $B_\lambda$ are \emph{atoms}).
  Let's compute \emph{the} conditional expectation $\E^{\mc{B}}f$ of an integrable random variable $f \in L^1(\Omega,\mc{A},\P)$ (it turns out there is only one).
  Since the atoms $(B_\lambda)_{\lambda}$ generate $\mc{B}$ and are pairwise disjoint, and since $\E^{\mc{B}}f$ is $\mc{B}$-measurable, $\E^{\mc{B}}f$ must be constant on each $B_\lambda$, so that
  \begin{equation*}
    \E^{\mc{B}} f = \sum_{\lambda \in \Lambda} \1_{B_\lambda} c_\lambda
  \end{equation*}
  for some scalars $(c_\lambda)_{\lambda \in \Lambda}$.
  Averaging over one of the atoms $B_{\lambda}$ and using \eqref{eq:conditional-expectation-property} tells us that
  \begin{equation*}
    \begin{aligned}
      c_\lambda = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} \E^{\mc{B}} f \, \dd\P = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} f \, \dd\P.
    \end{aligned}
  \end{equation*}
  In probabilistic terms, the quantity on the right hand side is the conditional expectation of $f$ given $B_\lambda$, which exists since $\P(B_\lambda) > 0$.
\end{example}

The previous example shows that conditional expectations with respect to atomic $\sigma$-algebras exist and are unique.
This is true in full generality.

\begin{thm}\label{thm:conditional-expectation-EU}
  For any $f \in L^1(\Omega, \mc{A}; \P)$ and any $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a unique conditional expectation $\E^{\mc{B}}f \in L^1(\Omega, \mc{B}, \P)$ exists.
  Furthermore, for all $p \in [1,\infty]$, the operator $\E^{\mc{B}}$ is a positive contraction from $L^p(\Omega, \mc{A}; \P)$ to $L^p(\Omega, \mc{B}; \P)$.
\end{thm}

\begin{proof} %Dudley p337 for uniqueness and 'proof by measure theory', 
  First let's handle the uniqueness.
  Suppose $\E^{\mc{B}}f$ and $\td{\E}^{\mc{B}}f$ are two conditional expectations of $f$ given $\mc{B}$.
  Then for all subsets $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P - \int_B f \, \dd\P = 0.
  \end{equation*}
  Since $\E^{\mc{B}}f - \td{\E}^{\mc{B}}f$ is $\mc{B}$-measurable, the subsets
  \begin{equation*}
    B_+ := \{\E^{\mc{B}}f - \td{\E}^{\mc{B}}f > 0\} \quad \text{and} \quad B_- := \{\E^{\mc{B}}f - \td{\E}^{\mc{B}}f < 0\}
  \end{equation*}
  are both in $\mc{B}$, so we get
  \begin{equation*}
    \int_\Omega | \E^{\mc{B}}f - \td{\E}^{\mc{B}}f | \, \dd\P
    = \int_{B_+}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P -  \int_{B_-}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P 
    = 0,
  \end{equation*}
  establishing that $\E^{\mc{B}}f = \td{\E}^{\mc{B}}f$ almost surely.

  We can prove positivity from the defining property \eqref{eq:conditional-expectation-property}, before we establish existence.
  Suppose $f \in L^1(\Omega, \mc{A}, \P)$ is almost-everywhere nonnegative.
  Then for all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P = \int_B f \, \dd\P \geq 0,
  \end{equation*}
  which implies that the $\mc{B}$-measurable function $\E^{\mc{B}}f$ is a.e. nonnegative.\footnote{This uses an exercise from measure theory: if $g$ is $\mc{B}$-measurable and $\int_B g \geq 0$ for all $\mc{B}$-measurable sets, then $g \geq 0$ a.e.. Proof: the set $N := \{g(\omega) < 0\}$ is $\mc{B}$-measurable, and assuming it has positive measure leads to the contradiction $0 \leq \int_B g < 0$.}

  As for existence: let $P^{\mc{B}}$ be the orthogonal projection of $L^2(\Omega, \mc{A}, \P)$ onto the closed subspace $L^2(\Omega, \mc{B}, \P)$.
  Then for all $f \in L^1(\Omega, \mc{A}, \P) \cap L^2(\Omega, \mc{A}, \P)$ and all $B \in \mc{B}$,
  \begin{equation}\label{eq:CE-integration}
    \int_B P^{\mc{B}} f \, \dd\P = \langle P^{\mc{B}} f, \1_{B} \rangle = \langle f, P^{\mc{B}} \1_{B} \rangle = \langle f, \1_{B} \rangle = \int_B f \, \dd\P
  \end{equation}
  where $\langle \cdot, \cdot \rangle$ denotes the inner product on $L^2(\Omega, \mc{A}, \P)$, using that $\1_{B} \in L^2(\Omega, \mc{B}, \P)$.
  Furthermore, 
  \begin{equation*}
    \|P^{\mc{B}} f \|_{L^1(\Omega, \mc{B}, \P)} = \sup_{g} |\langle P^{\mc{B}} f, g \rangle|
    = \sup_{g} |\langle f, P^{\mc{B}} g \rangle|
    = \sup_{g} |\langle f, g \rangle| 
    \leq \|f\|_{L^1(\Omega, \mc{A}, \P)}
  \end{equation*}
  where the supremum is over all $g \in L^\infty(\Omega, \mc{B}, \P)$ with $\|g\|_\infty = 1$ (note that $g \in L^2(\Omega, \mc{B}, \P)$ follows since $\Omega$ is a probability space).
  Thus $P^{\mc{B}}$ extends to a contraction $\map{\E^{\mc{B}}}{L^1(\Omega, \mc{A}, \P)}{L^1(\Omega, \mc{B}, \P)}$, and for all $f \in L^1(\Omega, \mc{A}, \P)$ and $B \in \mc{B}$, since integration on $B$ is a continuous linear functional on both $L^1(\Omega, \mc{B}, \P)$ and $L^1(\Omega, \mc{A}, \P)$, this extension satisfies
  \begin{equation*}
    \begin{aligned}
      \int_B \E^{\mc{B}}f \, \dd\P = \lim_{n \to \infty} \int_B P^B f_n \, \dd\P \stackrel{\eqref{eq:CE-integration}}{=} \lim_{n \to \infty} \int_B f_n \, \dd\P = \int_B f \, \dd\P
    \end{aligned}
  \end{equation*}
  where $(f_n)_{n \in \N}$ is a sequence in $L^2(\Omega, \mc{A}, \P)$ converging to $f$ in $L^1$.
  Thus the extension $\E^{\mc{B}}$ is a conditional expectation as claimed.
  
  We now show that $\E^{\mc{B}}$ is a contraction from $L^p(\Omega,\mc{A};\P)$ to $L^p(\Omega,\mc{B};\P)$ for all $p \in (1,\infty]$, the case $p=1$ already being established.
  Here is a clever proof which represents $\E^{\mc{B}}$ as the adjoint of an inclusion (this can be reworked to provide another proof of existence).
  For all $f \in L^p(\Omega,\mc{A};\P) \cap L^2(\Omega,\mc{A};\P)$ and $g \in L^{p'}(\Omega,\mc{B};\P) \cap L^2(\Omega,\mc{B};\P)$ we have
  \begin{equation*}
    \begin{aligned}
      \langle \E^{\mc{B}} f, g \rangle = \langle P^B f, g \rangle = \langle f, P^B g \rangle = \langle f, g \rangle
    \end{aligned}
  \end{equation*}
  since the orthogonal projection $P^B$ is self-adjoint on $L^2(\Omega,\mc{A};\P)$.
  Since $g$ was assumed to lie in a dense subspace of $L^{p'}(\Omega,\mc{B};\P)$, this shows that the conditional expectation $\map{\E^{\mc{B}}}{L^p(\Omega,\mc{A};\P)}{L^1(\Omega,\mc{B};\P)}$ is the adjoint of the canonical inclusion $L^{p'}(\Omega,\mc{B},\P) \to L^{p'}(\Omega,\mc{A},\P)$, which is contractive.
  Thus by duality $\E^{\mc{B}}$ is a contraction from $L^p(\Omega,\mc{A};\P)$ to $L^p(\Omega,\mc{B};\P)$.
\end{proof}

Having established that the conditional expectation $\E^{\mc{B}}$ is a \textbf{positive} bounded operator from $L^p(\Omega, \mc{A};\P)$ to $L^p(\Omega,\mc{B};\P)$, we can appeal to the extension theorem for positive operators to define conditonal expectations on Banach-valued random variables.

\begin{defn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  Given a $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, we define the \emph{conditional expectation operator}
  \begin{equation*}
    \map{\E_X^{\mc{B}}}{L^1(\Omega,\mc{A},\P;X)}{L^1(\Omega,\mc{B};\P;X)} 
  \end{equation*}
  as the unique bounded linear extension of the tensor extension
  \begin{equation*}
    \map{\E^{\mc{B}} \otimes I}{L^1(\Omega,\mc{A},\P) \otimes X}{L^1(\Omega,\mc{B},\P) \otimes X}.
  \end{equation*}
  We will often overload notation and write $\E^{\mc{B}}$ to denote $\E_X^{\mc{B}}$.
\end{defn}

Since the conditional expectation $\E^{\mc{B}}$ on scalar-valued functions is positive and contractive on $L^p$ for all $p \in [1,\infty)$, Theorem \ref{thm:positive-extensions} tells us that $\E_X^{\mc{B}}$ is well-defined and is in fact a contraction on $L^p(\Omega,\mc{A},\P;X)$ for all $p \in [1,\infty)$.
For $p = \infty$, since $L^\infty(\Omega;X)$ is lacking in convenient dense subspaces, we can't argue so abstractly.
Here is a somewhat ad-hoc argument for contractivity on $L^\infty$.

\begin{prop}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  Then the conditional expectation $\E_X^{\mc{B}}$, which \emph{a priori} maps
  \begin{equation*}
    \map{\E_X^{\mc{B}}}{L^\infty(\Omega,\mc{A},\P;X) \subset L^1(\Omega,\mc{A},\P;X)}{L^1(\Omega,\mc{B},\P;X)},
  \end{equation*}
  is a contraction on $L^\infty(\Omega,\mc{A},\P;X)$.
\end{prop}

\begin{proof}
  Let $f \in L^\infty(\Omega,\mc{A},P;X)$.
  Since $L^\infty(\Omega,\mc{A},\P;X) \subset L^2(\Omega,\mc{A},\P;X)$, $E^{\mc{B}} f = \td{P^{\mc{B}}} f$ can be seen as the tensor extension of the orthogonal projection of $L^2(\Omega,\mc{A},\P)$ onto $L^2(\Omega, \mc{B},\P)$ (as in the proof of Theorem \ref{thm:conditional-expectation-EU}).
  By Proposition \ref{prop:bochner-preduality}, we can test $\|\E_X^{\mc{B}} f\|_\infty$ by duality, using that $L^2(\Omega,\mc{B}; X^*)$ is dense in $L^1(\Omega, \mc{B}; X^*)$.
  For all nonzero $g \in L^2(\Omega, \mc{B}; X^*)$ we have
\begin{equation*}
  \begin{aligned}
    | \langle \E_X^{\mc{B}} f, g \rangle |
    =  | \langle \td{P^{\mc{B}}} f, g \rangle | 
    \stackrel{(*)}{=} | \langle f, \td{P^{\mc{B}}} g \rangle | 
    &=  | \langle f, \E_X^{\mc{B}} g \rangle | \\
    &\leq  \|f\|_{L^\infty(\Omega,\mc{A},\P;X)} \|\E_X^{\mc{B}} g\|_{L^1(\Omega,\mc{A},\P;X^*)} \\
    &\leq  \|f\|_{L^\infty(\Omega,\mc{A},\P;X)} \| g\|_{L^1(\Omega,\mc{A},\P;X^*)}.
  \end{aligned}
\end{equation*}
For the starred equality see Exercise \ref{ex:tensor-adjoint} in the previous chapter.
Taking the supremum over all such $g$ completes the proof.
\end{proof}


\todo{further properties of vector-valued conditional expectations}






\subsection{Filtrations and stopping times}

\begin{defn}
  A \emph{filtration} on a probability space $(\Omega,\mc{A},\P)$ is a monotone increasing (i.e. nondecreasing) sequence of $\sigma$-subalgebras
  \begin{equation*}
    \mc{A}_0 \subseteq \mc{A}_1 \subseteq \mc{A}_2 \subseteq \cdots \subseteq \mc{A}.
  \end{equation*}
\end{defn}

\begin{rmk}
  With obvious modifications one can talk about filtrations starting at an arbitrary index $(\mc{A}_n)_{n = n_0}^\infty$, finite filtrations $(\mc{A}_n)_{n = n_0}^N$, or filtrations with respect to arbitrary (total or partial) orders, for example continuously-indexed filtrations $(\mc{A}_t)_{t \geq 0}$.
  There is no need to be too pedantic about these definitions.
\end{rmk}

The most important interpretation of a filtration $(\mc{A}_n)_{n=0}^\infty$ is that each $\sigma$-subalgebra $\mc{A}_n \subset \mc{A}$ models the \emph{information available at time $n$} (in reference to some discrete time-dependent process).
The monotonicity assumption encapsulates the idea that the available information does not decrease as time progresses.
There are two equivalent ways of thinking about this: one is that at time $n$ one has access to all $\mc{A}_n$-measurable subsets; the other is that at time $n$ one has access to all $\mc{A}_n$-measurable functions.


\begin{defn}
  Given a filtration $(\mc{A}_n)_{n=0}^\infty$ on a probability space $(\Omega, \mc{A}, \P)$, a random variable $\map{T}{\Omega}{\N \cup \{\infty\}}$ is called a \emph{stopping time} (with respect to $(\mc{A}_n)$) if 
  \begin{equation*}
    \{\omega \in \Omega: T(\omega) \leq n\} \in \mc{A}_n \qquad \forall n \geq 0.
  \end{equation*}
  The stopping time $T$ is \emph{finite} if $T$ is almost surely finite.
\end{defn}

Generally stopping times $T$ are defined in terms of some kind of stochastic \emph{stopping condition}.
Interpreting the filtration $(\mc{A}_n)_{n \in \N}$ as modelling the available information at time $n$, $T$ being a stopping time says precisely that at time $n$, one can identify all events $\omega \in \Omega$ for which $T(\omega) \leq n$.
Said less precisely, if $T$ is a stopping time, then at time $n$, one can tell whether $T$ is less than $n$.
This is best understood by example.

\begin{example}
  Let $\Omega = \{H, T\}^\N$ equipped with the product $\sigma$-algebra and measure, where each factor $\{H, T\}$ has the full $\sigma$-algebra and the uniform probability measure.
  Let $\map{\pi_n}{\Omega}{\{H,T\}}$ be the $n$-th coordinate function. 
  Then the sequence of random variables $(\pi_n)_{n \in \N}$ represents an infinite sequence of fair coin tosses ($H$ is heads, $T$ is tails).
  Let $\mc{A}_n = \sigma(\pi_0,\ldots,\pi_n)$ be the $\sigma$-subalgebra generated by the first $n+1$ coordinates, so that $(\mc{A}_n)_{n = 0}^\infty$ is a filtration.
  Let
  \begin{equation*}
    T(\omega) := \inf\Big\{n \in \N : \pi_n(\omega) = H \Big\}
  \end{equation*}
  with the usual convention that $T(\omega) = \infty$ if $\pi_n(\omega) \neq H$ for all $n \in \N$.
  That is, $T(\omega)$ is the first time $n$ at which $\pi_n(\omega) = H$.
  At time $n$ (i.e. after the $n$-th coin flip) we obviously know whether or not one of the $m$-th coin flips, with $m \leq n$, showed an $H$, which indicates that $T$ should be a stopping time.
  Rigourously, one shows this by writing for all $n \in \N$
  \begin{equation*}
    \begin{aligned}
      \{\omega \in \Omega : T(\omega) \leq n\}
      &= \big\{\omega : \inf\{m : \pi_m(\omega) = H\} \leq n\big\} \\
      &= \{\omega  : \text{$\pi_m(\omega) = H$ for some $m \leq n$}\} \\
      &= \bigcup_{m = 0}^n \pi_m^{-1}(H) \in \sigma(\pi_0,\ldots,\pi_n) = \mc{A}_n.
    \end{aligned}
  \end{equation*}

  In fact, $T$ is a finite stopping time, which is one of the classic computations of probability:
  \begin{equation*}
    \P(T = \infty) = \P(\{\text{$\pi_m = H$ for all $m \in \N$} \}) = \prod_{m=0}^\infty \frac{1}{2} = 0.
  \end{equation*}
  
\end{example}


\begin{prop}
  first hitting time of a closed set is a stopping time, if needed\todo{write}
\end{prop}

\subsection{Definitions and key examples of martingales}

-stopped martingales
-Rademacher variables and sums

\begin{defn}
  limiting $\sigma$-algebra 
\end{defn}

\begin{thm}[$L^p$-convergence of martingales]
  %Pisier Theorem 1.14
  
\end{thm}

\subsection{Maximal inequalities and pointwise convergence}
% proofs from Pisier with little modifications to allow for infinite martingales


\begin{defn}
  definition of $\R$-valued submartingale
\end{defn}

As an example, consider a martingale $(f_n)_{n \in \N}$ taking values in a Banach space $X$.
Then by {\color{red} CITE PART OF POSITIVE OP EXTENSION HERE} we have for all $m \geq n$
\begin{equation*}
  \|f_n\|_X = \|\E^{\mc{F}_n} f_m\|_X \leq \E^{\mc{F}_n} \|f_m\|_X \qquad \text{almost surely},
\end{equation*}
so that the sequence $(\|f_n\|_X)_{n \in \N}$ is an $\R$-valued submartingale.

\begin{thm}[Doob's maximal inequalties]
  
\end{thm}

% i think we can do without this
% but include it if necessary
% \begin{thm}[Burkholder--Davis--Gundy inequality]
% \end{thm}

\begin{thm}[Martingale convergence theorem]
  
\end{thm}



\subsection{John--Nirenberg for adapted sequences and the Kahane--Khintchine inequalities}
% following HNVW1 section 3.2.c

Fix a Banach space $X$ and a $\sigma$-finite measure space $(S,\mc{A},\mu)$.
Consider a filtration $(\mc{F}_n)_{n \in \N}$ and a sequence $\phi = (\phi_n)_{n \in \N}$ of $X$-valued functions adapted to the filtration (i.e. $\phi_n$ is $\mc{F}_n$-measurable for all $n \in \N$).\todo{Strongly?}
For all $q \in (0,\infty)$ we consider the following measure of the oscillation of $\phi$:
\begin{equation*}
    \|\phi\|_{*,q} := \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{\substack{F \in \mc{F}_k \\ 0 < \mu(F) < \infty}} \Big( \fint_{F} \|(\phi_n - \phi_{k-1})(s)\|_{X}^{q} \, \dd\mu(s)\Big)^{1/q} .
\end{equation*}


\begin{thm}[John--Nirenberg inequality for adapted sequences]\label{thm:jn-adapted-sequences}
  With notation as above, for all $p,q \in (0,\infty)$ there exists a finite constant $c_{p,q}$ independent of $\phi$ such that
  \begin{equation*}
    \|\phi\|_{*,p} \leq c_{p,q} \|\phi\|_{*,q}.
  \end{equation*}
\end{thm}

We will prove this as a consequence of a series of lemmas, but before proving this, we demonstrate an important application to Rademacher sums.

\begin{thm}[Kahane--Khintchine inequality]\label{thm:kk}
  Let $X$ be a Banach space and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega,\mc{F},\P)$.
  Then for all $p,q \in (0,\infty)$ there exists a finite constant $\kappa_{p,q}$ such that for all finite sequences $(\mb{x}_n)_{n=1}^N$ in $X$,
  \begin{equation*}
    \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} \leq \kappa_{p,q} \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
  \end{equation*}
  That is, for all $p \in (0,\infty)$, the $L^p$-norms of a Rademacher sum are pairwise equivalent.  
\end{thm}

Since $\Omega$ is a probabibility space, H\"older's inequality yields the case $p \leq q$ with constant $\kappa_{p,q} = 1$, so we only need to consider the case $q < p$.

\begin{proof}[Proof, assuming the John--Nirenberg inequality]
  Consider the filtration and adapted sequence \todo{make sure to discuss filtrations generated by functions, particularly Rademacher variables} 
  \begin{equation*}
    \mc{F}_n := \sigma(\{\varepsilon_j : 1 \leq j \leq n\}), \qquad \phi_n := \sum_{j=1}^n \varepsilon_j \mb{x}_j.
  \end{equation*}
  We claim that for all $q \in [1,\infty)$ we have
  \begin{equation}\label{eq:kk-claim}
    \|\phi\|_{*,q} = \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^q(\Omega;X)}.
  \end{equation}
  Assuming this for the moment, the John--Nirenberg inequality yields a finite constant $c_{p,q}$ such that
  \begin{equation*}
    \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^p(\Omega;X)}
    = \|\phi\|_{*,p}
    \leq c_{p,q} \|\phi\|_{*,q}
    = \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^q(\Omega;X)}
  \end{equation*}
  whenever $1 \leq q < p$.
  If $q < 1 \leq p$, we fix $\theta \in (0,1)$ so that $1/p = \theta/q + (1-\theta)/2p$, and log-convexity of $L^p$-norms gives
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)}
    \leq \| \phi_N \|_{L^q(\Omega;X)}^{\theta} \| \phi_N \|_{L^{2p}(\Omega;X)}^{1 - \theta}
    \leq \| \phi_N \|_{L^q(\Omega;X)}^{\theta} (c_{2p,p} \| \phi_N \|_{L^{p}(\Omega;X)})^{1 - \theta},
  \end{equation*}
  which yields
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)} \leq c_{2p,p}^{(1 - \theta)/\theta} \| \phi_N \|_{L^q(\Omega;X)}.
  \end{equation*}
  Finally, if $q < p < 1$, we simply estimate
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)} \leq \| \phi_N \|_{L^1(\Omega;X)} \leq c_{1,q} \| \phi_N \|_{L^q(\Omega;X)}.
  \end{equation*}
  This covers all nontrivial cases, and it remains to prove the claimed equality \eqref{eq:kk-claim}.

  First recall that
  \begin{equation*}
    \|\phi\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{\substack{F \in \mc{F}_k \\ \mu(F) > 0}} \Big( \fint_{F} \|(\phi_n - \phi_{k-1})(s)\|_{X}^{q} \, \dd\P(s)\Big)^{1/q}.
  \end{equation*}
  Fix $k \leq n$ and a set $F \in \mc{F}_k$ of positive measure.
  We will compute
  \begin{equation*}
    \fint_{F} \|\1_{F} (\phi_n - \phi_{k-1})(s)\|_{X}^q \, \dd\P(s) = \P(F)^{-1} \E \Big( \1_{F} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big).
  \end{equation*}
  Observe that for all $\omega \in \Omega$
  \begin{equation*}
    \Big\| \sum_{j=k}^{n} \varepsilon_j(\omega) \mb{x}_j\Big\|_{X} = \Big\| \varepsilon_k(\omega) \Big( \mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime}(\omega) \mb{x}_j \Big) \Big\|_{X} = \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}
  \end{equation*}
  where
  \begin{equation*}
    \varepsilon_{j}^{\prime} :=
    \begin{cases}
      \varepsilon_{j} & j \leq k \\
      \varepsilon_{k} \varepsilon_{j} & k+1 \leq j,
    \end{cases}
  \end{equation*}
  since $\varepsilon_j$ is $\pm 1$-valued.
  Now note that the $\sigma$-algebra $\mc{F}^\prime_{k+1,n} := \sigma(\{\varepsilon_j^\prime : k+1 \leq j \leq n\})$ is independent of $\mc{F}_k$, since for all $1 \leq j \leq k$ and $k+1 \leq j' \leq n$ we have by independence of the original Rademacher sequence
  \begin{equation*}
    \E(\varepsilon_j \varepsilon_{j'}^\prime) = \E(\varepsilon_j \varepsilon_k \varepsilon_{j'})
    =
    \begin{cases}
      \E(\varepsilon_j) \E(\varepsilon_k) \E(\varepsilon_{j'}) & \text{if $j < k$} \\
       \E(\varepsilon_{j'}) & \text{if $j = k$}
    \end{cases}
    \quad = 0.
  \end{equation*}
  Thus, since $F \in \mc{F}_k$, we have by independence of $\mc{F}_k$ and $\mc{F}^{\prime}_{k+1, n}$
  \begin{equation*}
    \begin{aligned}
      \P(F)^{-1} \E \Big( \1_{F} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big)
      &=  \P(F)^{-1} \E \Big( \1_{F} \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} \Big) \\
      &=  \E \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} 
      =  \E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}. \\
    \end{aligned}
  \end{equation*}
  Now letting $\mc{F}_{k,n} := \sigma(\{\varepsilon_j : k \leq j \leq n\})$, we have by the $L^q$-contraction property of conditional expectations
  \begin{equation*}
    \E \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
    = \E \Big\|\E^{\mc{F}_{k,n}} \Big(\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big)\Big\|_{X}^{q}
    \leq \E \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
  \end{equation*}
  with equality when $k=1$ and $n=N$.
  Thus
  \begin{equation*}
    \|\phi\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}}  \Big(\E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q} = \Big(\E  \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q}
  \end{equation*}
  which proves the claimed equality \eqref{eq:kk-claim} and completes the proof. %mk
\end{proof}

In the setting of Hilbert-valued functions (and in particular, scalar-valued functions), the Kahane--Khintchine inequality leads to the classical Khintchine inequalities.

\begin{cor}[Khintchine's inequalities]
  Let $H$ be a Hilbert space, and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega, \mc{F}, \P)$.
  Then for all $p \in (0,\infty)$ there exist finite constants $A_p$ and $B_p$ such that for all finite sequences $(\mb{h}_n)_{n=1}^N$ in $H$,
  \begin{equation*}
    A_p \Big( \sum_{n=1}^N \|\mb{h}_n\|_H^2 \Big)^{1/2} \leq \Big\| \sum_{n=1}^N \varepsilon_n \mb{h}_n \Big\|_{L^p(\Omega;H)} \leq B_p \Big( \sum_{n=1}^N \|\mb{h}_n\|_H^2 \Big)^{1/2}.
  \end{equation*}
\end{cor}

\begin{proof}
  By independence of the Rademacher variables we have
  \begin{equation}
    \begin{aligned}
      \Big\| \sum_{n=1}^N \varepsilon_n \mb{h}_n \Big\|_{L^2(\Omega;H)}^2
      &= \Big\langle \sum_{n=1}^N \varepsilon_n \mb{h}_n , \sum_{m=1}^N \varepsilon_m \mb{h}_m \Big\rangle \\
      &= \sum_{n,m = 1}^N \E(\varepsilon_n \varepsilon_m) \langle \mb{h}_n, \mb{h}_m \rangle 
      = \sum_{n=1}^N \|\mb{h}_{n}\|_{H}^2,
    \end{aligned}
  \end{equation}
  so the result is true for $p=2$ with $A_2 = B_2 = 1$.
  Now use Kahane--Khintchine to extend the result to general $p \in (0,\infty)$.
\end{proof}

\subsubsection{Proof of the John--Nirenberg inequality}

We return to our analysis of a sequence $\phi = (\phi_n)_{n \in \N}$ of $X$-valued functions adapted to a filtration $(\mc{F}_n)_{n \in \N}$ on a $\sigma$-finite measure space $(S,\mc{A},\mu)$.
We prove the John--Nirenberg inequality via a series of lemmas in which we obtain increasingly fine control on the oscillation of $\phi$.

\begin{lem}\label{lem:JN-proof-1}
  For all $k \leq n$, $F \in \mc{F}_k$, and $\alpha > 0$,
  \begin{equation}\label{eq:JN-proof-1-est}
    \mu(F \cap \{ \|\phi_n - \phi_{k-1}\|_{X} > \alpha \}) \leq \Big( \frac{\|\phi\|_{*,q}}{\alpha} \Big)^{q} \mu(F).
  \end{equation}
\end{lem}

\begin{proof}
  We can assume that $0 < \mu(F) < \infty$, otherwise there is nothing to prove.
  The left hand side of \eqref{eq:JN-proof-1-est} is bounded by
  \begin{equation*}
    \int_{F} \Big( \frac{\|\phi_n - \phi_{k-1}\|_{X}}{\alpha} \Big)^{q} \, \dd\mu \leq \mu(F) \Big( \frac{\|\phi\|_{*,q}}{\alpha} \Big)^{q}
  \end{equation*}
  since $F \in \mc{F}_k$ and $k \leq n$, by the definition of $\|\phi\|_{*,q}$.
\end{proof}

Next we show that oscillation control of the form above extends to more general stopping times.

\begin{lem}
  Suppose that there exist $\alpha > 0$ and $\eta > 0$ such that
  \begin{equation*}
    \mu(F \cap \{ \| \phi_n - \phi_{k-1} \| > \alpha \} ) \leq \eta \mu(F) \qquad \forall k \leq n, F \in \mc{F}_k.
  \end{equation*}
  Then for all $k \in \N$, $F \in \mc{F}_k$, and all stopping times $\nu$ such that $\nu \geq k$ on $F$,
  \begin{equation}\label{eq:jn-stoptime}
    \mu(F \cap \{\nu < \infty\} \cap \{ \| \phi_{\nu} - \phi_{k-1} \| > 2\alpha \} ) \leq 2\eta \mu(F).
  \end{equation}
\end{lem}

\begin{proof}
  Sum over all possible values of the stopping time:
  \begin{equation*}
      \mu(F \cap \{\nu < \infty\} \cap \{ \| \phi_{\nu} - \phi_{k-1} \| > 2\alpha \} ) 
      = \lim_{N \to \infty} \sum_{n = k}^{N} \mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ),
  \end{equation*}
  where $F_{n} := F \cap \{\nu = n\}$.
  For fixed $N \geq n > k$, since $F_n \in \mc{F}_n \subset \mc{F}_{n+1}$, we have by assumption
  \begin{equation*}
    \begin{aligned}
      &\mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ) \\
      &\leq \mu(F_n \cap \{ \| \phi_{n} - \phi_{N} \| > \alpha \} )
      + \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \} ) \\
      &\leq \eta \mu(F_n) + \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \} ).
    \end{aligned}
  \end{equation*}
  Thus we have
  \begin{equation*}
    \begin{aligned}
      &\lim_{N \to \infty} \sum_{n = k}^{N} \mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ) \\
      &\leq \lim_{N \to \infty} \Big( \eta \sum_{n=k}^N \mu(F_n) + \sum_{n=k}^N \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \}) \Big) \\
      &\leq \eta \mu(F) + \lim_{N \to \infty} \mu(F \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \})  
      \leq 2\eta\mu(F)
    \end{aligned}
  \end{equation*}
  using the assumption and $F \in \mc{F}_k$ in the last estimate.
\end{proof}
  
In the following lemma we make use of the \emph{started sequence}
\begin{equation*}
  {}^{k-1}\phi = (\phi_n - \phi_{k-1})_{n \geq k-1}
\end{equation*}
and its maximal function
\begin{equation*}
  ({}^{k-1} \phi)^{*}(s) := \sup_{n \geq k-1} \|({}^{k-1}\phi)_n(s)\|_X = \sup_{n \geq k} \|(\phi_n - \phi_{k-1})(s)\|_X.
\end{equation*}

\begin{lem}\label{lem:jn-mf}
  Suppose that $\phi$ satisfies \eqref{eq:jn-stoptime} for all $k \in \N$, $F \in \mc{F}_k$, and all stopping times $\nu$ such that $\nu \geq k$ on $F$.
  Then for all $\lambda > 0$,
  \begin{equation}\label{eq:jn-mf-eq}
    \mu(F \cap \{ ({}^{k-1} \phi)^{*} > \lambda + 2\alpha \}) \leq 2\eta \mu(F \cap \{  ({}^{k-1} \phi)^{*} > \lambda \}) \qquad \forall k \in \N, F \in \mc{F}_{k}.
  \end{equation}
\end{lem}

\begin{proof}
  Fix $k \in \N$ and consider the stopping times
  \begin{equation*}
    \begin{aligned}
      \rho &:= \inf\{n \geq k : \|\phi_n - \phi_{k-1}\| > \lambda\}, \\
      \nu &:= \inf\{n \geq k : \|\phi_n - \phi_{k-1}\| > \lambda + 2\alpha\}.
    \end{aligned}
  \end{equation*}
  Then $k \leq \rho \leq \nu$, and \eqref{eq:jn-mf-eq} can be rewritten as
  \begin{equation*}
    \mu(F \cap \{\nu < \infty\}) \leq 2\eta \mu(F \cap \{\rho < \infty\}).
  \end{equation*}
  Now fix $n \geq k$ and let $F_n := F \cap \{\rho = n\} \in \mc{F}_n$.
  On $\{F_n \cap \{\nu < \infty\}\}$ we have
  \begin{equation*}
    \|\phi_{\nu} - \phi_{n-1}\| \geq \|\phi_{\nu} - \phi_{k-1}\| - \|\phi_{n-1} - \phi_{k-1}\| > (\lambda + 2\alpha) - \lambda = 2\alpha,
  \end{equation*}
  so
  \begin{equation*}
    \mu(F_n \cap \{\nu < \infty\}) = \mu(F_n \cap \{\nu < \infty\} \cap \{\|\phi_{\nu} - \phi_{n-1}\| > 2\alpha\} )
    \leq 2\eta \mu(F_n).
  \end{equation*}
  Summing over $n \geq k$ completes the proof.
\end{proof}

\begin{lem}
  Suppose that $f$ is a non-negative function supported in $F \in \mc{A}$, satisfying
  \begin{equation*}
    \mu(f > \lambda + \alpha) \leq \eta \mu(f > \lambda) \qquad \forall \lambda > 0
  \end{equation*}
  for some $\eta \in (0,1)$ and $\alpha > 0$.
  Then for all $p \in [1,\infty)$,
  \begin{equation*}
    \|f\|_p \leq \frac{1 + \eta^{1/p}}{1 - \eta^{1/p}} \alpha \mu(F)^{1/p}.
  \end{equation*}
\end{lem}

\begin{proof}
  {\color{red} WRITE PROOF}
\end{proof}

\todo{UP TO HERE. HAVE TO COMPLETE PROOF OF JOHN-NIRENBERG.}

{\color{blue}

\begin{equation*}
  \|\phi\|_{**,q} := \sup_{k \in \N} \sup_{\substack{F \in \mc{F}_k \\ 0 < \mu(F) < \infty}} \Big( \fint_{F} ({}^{k-1} \phi)^{*}(s)^q \, \dd\mu(s) \Big)^{1/q},
\end{equation*}

}



\subsection{Gundy's decomposition}

\subsection{The Radon--Nikodym property, martingale convergence, and Bochner space duality}

\subsection*{Exercises}

\begin{exercise}
  test
\end{exercise}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
