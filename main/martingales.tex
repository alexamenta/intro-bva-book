In the previous chapter we introduced the problem of whether a bounded operator $T \in \Lin(L^p)$ on scalar-valued functions has a bounded $X$-valued extension.
We saw that this depends on both the operator $T$ and the Banach space $X$.
To answer this question for various classes of operators, we need to be able to describe different geometric properties a Banach space $X$ may have.
Basic notions from functional analysis, such as separability and reflexivity, are not fine-grained enough to answer these questions.

One useful way of quantifying the geometry of a Banach space $X$ is to consider various properties of $X$-valued stochastic processes.
It is certainly not obvious that this would be useful, but many years of research has shown this to be the case.
By looking at particular classes of $X$-valued stochastic processes (for example, \emph{martingales}) and particular properties that they may or may not have (for example, \emph{pointwise convergence} or \emph{unconditionality} properties), one is led to useful Banach space properties (in the preceding examples, the \emph{Radon--Nikodym property} or the \emph{UMD property}).
Banach-valued probability is a very interesting field in its own right, and we will only scratch the surface; for a proper overview see for example \cite{LT91}.

Throughout these notes I will assume some knowledge of measure-theoretic probability theory; key results are collected in Section \ref{sec:probability} in the Appendices.

I want you to imagine the following betting game, on offer at the Banach-valued \emph{Spielhalle}.
At each turn, you bet on the outcome of a coin toss.
The quantities that you can bet are taken from a Banach space $X$.
The initial state of your wallet, $\mb{s}_{-1}$, is the zero vector
\begin{equation*}
  \mb{s}_{-1} = \mb{0} \in X.
\end{equation*}
At each time $n \in \N = \{0,1,\ldots\}$, you choose a vector $\mb{x}_n \in X$ to wager.
I then flip a fair coin, which shows either Heads or Tails, and the state of your wallet becomes
\begin{equation*}
  \mb{s}_n =
  \begin{cases}
    \mb{s}_{n-1} + \mb{x}_n & \text{if the coin shows Heads} \\
    \mb{s}_{n-1} - \mb{x}_n & \text{if the coin shows Tails.}
  \end{cases}
\end{equation*}
The Banach space $X$ is not ordered, so there is no canonical notion of $\mb{s}_n$ being `more' or `less' than $\mb{s}_{n-1}$. Thus the game is not about winning or losing (the true winner of the game is Functional Analysis).
This game is a good model for many of the probabilistic concepts introduced in this chapter, and we will come back to it at various points.

\begin{rmk}
  It will become clear in these notes that I am not an expert in probability, and there will probably be inaccuracies in these notes.
  Please let me know if you find any mistakes!
\end{rmk}

\section{Random variables, filtrations, and stochastic processes}

First we will set up some basic probabilistic language in Banach spaces.

\begin{defn}\label{defn:RV}
  Let $X$ be a Banach space and $(\Omega,\mc{A},\P)$ a probability space.
  An \emph{$X$-valued random variable} is a strongly measurable function $\map{\mb{f}}{\Omega}{X}$.
  If $\mb{f} \in L^1(\Omega;X)$ is an integrable random variable, we define the \emph{expectation} to be the Bochner integral
  \begin{equation*}
    \E \mb{f} = \int_{\Omega} \mb{f}(\omega) \, \dd\P(\omega).
  \end{equation*}
  When $X$ is the scalar field, this coincides with the usual definition of the expectation of a scalar-valued random variable.
\end{defn}

\begin{defn}
  A \emph{filtration} on a probability space $(\Omega,\mc{A},\P)$ is a monotone increasing sequence of $\sigma$-subalgebras $(\mc{A}_{n})_{n \in \N}$ of $\mc{A}$, i.e.
  \begin{equation*}
    \mc{A}_0 \subseteq \mc{A}_1 \subseteq \mc{A}_2 \subseteq \cdots \subseteq \mc{A}. 
  \end{equation*}
  We will sometimes use the notation $\mc{A}_{\bullet} := (\mc{A}_{n})_{n \in \N}$.
\end{defn}

\begin{example}\label{eg:dyadic-filtration}
  Consider the unit interval $[0,1) \subset \R$ with Borel $\sigma$-algebra and Lebesgue measure.
  For each $n \in \N$, let $\mc{A}_n$ be the $\sigma$-algebra generated by the \emph{dyadic intervals of length $2^{-n}$}, i.e. intervals of the form
  \begin{equation*}
    [2^{-n}k, 2^{-n}(k+1)) \qquad k = 0, 1, 2, \ldots, 2^{n}- 1.
  \end{equation*}
  Then $(\mc{A}_n)_{n \in \N}$ is a filtration, which we call the \emph{(standard) dyadic filtration}.
  A Banach-valued function $\map{\mb{f}}{[0,1)}{X}$ is $\mc{A}_n$-measurable if and only if it is constant on each dyadic interval of length $2^{-n}$.  
\end{example}

\begin{example}\label{eg:coordinate-filtration}
  Let $\{-1,1\}$ be a two-point space with uniform probability measure, and consider the infinite product
  \begin{equation*}
    \Omega := \prod_{n \in \N} \{-1,1\} = \{-1,1\}^\N,
  \end{equation*}
  with the product $\sigma$-algebra and measure.
  Elements of $\Omega$ are sequences $\omega = (\omega_n)_{n \in \N}$ with $\omega_n = \pm 1$.
  Suppose $n \in \N$, fix a vector $\eta = (\eta_0, \eta_1, \ldots \eta_n) \in \{-1,1\}^{n+1}$ of length $n+1$, and define the set
  \begin{equation*}
    A_{\eta} := \{\omega \in \Omega : \omega_k = \eta_k \; \forall k = 0,1,\ldots,n\};
  \end{equation*}
  that is, a point $\omega \in \Omega$ belongs to $A_{\eta}$ if its first $n+1$ components are given by $\eta$.
  For each $n \in \N$, let $\mc{A}_n$ be the $\sigma$-algebra generated by all sets $A_{\eta}$ with $\eta$ of length $n+1$.
  Then $(\mc{A}_n)_{n \in \N}$ is a filtration, which we call the \emph{coordinate filtration}.
  A Banach-valued function $\map{\mb{f}}{\Omega}{X}$ is $\mc{A}_n$-measurable if and only if $\mb{f}(\omega) = \mb{f}(\omega_0,\ldots,\omega_n)$ only depends on the first $n+1$ coordinates of its argument.
\end{example}

\begin{rmk}
  Example \ref{eg:coordinate-filtration} encodes the same information as Example \ref{eg:dyadic-filtration}.
  Each dyadic interval $I \subset [0,1]$ has exactly two dyadic subintervals, and every vector $\eta \in \{-1,1\}^{n+1}$ can be extended in exactly two ways to a vector in $\{-1,1\}^{n+2}$.
  Equivalently, each infinite sequence $\omega \in \{0,1\}^\N$ corresponds to the binary expansion of a number $t \in [0,1)$, and this correspondence is bijective up to a measure zero subset (the set of $t \in [0,1)$ with non-unique binary expansions; i.e. the dyadic numbers).
  The set of sequences $\omega' \in \{0,1\}^\N$ whose first $n+1$ entries coincide with those of $\omega$ then corresponds to the set $A_{(\omega_0,\ldots,\omega_n)}$, which corresponds to the unique dyadic interval of length $2^{-(n+1)}$ containing $t$. 
\end{rmk}

\begin{defn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  A \emph{(discrete-time) $X$-valued stochastic process} on $(\Omega,\mc{A},\P)$ is a sequence of $\mc{A}$-measurable random variables $\map{\mb{f}_n}{\Omega}{X}$, $n \in \N$.
  We will sometimes use the notation $\mb{f}_{\bullet} := (\mb{f}_{n})_{n \in \N}$, as with our notation for filtrations.
  The \emph{difference sequence} of $\mb{f}_{\bullet}$ is the stochastic process $d\mb{f}_{\bullet}$ defined by
  \begin{equation*}
    d\mb{f}_n := \mb{f}_{n} - \mb{f}_{n-1} \qquad \forall n \in \N,
  \end{equation*}
  with the convention that $\mb{f}_{-1} := \mb{0}$.
  
  Given a filtration $\mc{A}_{\bullet}$, a stochastic process $\mb{f}_{\bullet}$ is
  \begin{itemize}
  \item \emph{adapted to $\mc{A}_{\bullet}$} if each $\mb{f}_{n}$ is $\mc{A}_{n}$-measurable, and
  \item \emph{predictable (with respect to $\mc{A}_{\bullet}$)} if each $\mb{f}_n$ is $\mc{A}_{n-1}$-measurable (with the convention that $\mc{A}_{-1} = \{\varnothing, \Omega\}$).
  \end{itemize}
\end{defn}

\begin{rmk}
  With obvious modifications one can talk about filtrations and stochastic processes starting at an arbitrary index, finite filtrations/processes, or filtrations/processes with respect to arbitrary (total or partial) orders, for example with a continuous time index.
  In this course we will only consider discrete indexing sets contained in $\N$.
\end{rmk}

One should think of a filtration $\mc{A}_{\bullet}$ as representing the progression of available information over time, usually in relation to a stochastic process.
Each $\sigma$-subalgebra $\mc{A}_n \subset \mc{A}$ represents the information available at time $n$.
There are two equivalent ways of thinking about the availability of information: one is that at time $n$ one has access to all $\mc{A}_n$-measurable subsets; the other is that at time $n$ one has access to all $\mc{A}_n$-measurable functions.
The monotonicity assumption says that no information is lost as time progresses.
Predictability of a stochastic process $\mb{f}_{\bullet}$ with respect to the filtration $\mc{A}_{\bullet}$ thus says the following: if the available information is represented by $\mc{A}_{\bullet}$, then at each time $n$, one already `knows' $\mb{f}_{n+1}$.

\begin{example}\label{eg:filtration-generated-by-process}
  Let $(\Omega,\mc{A},\P)$ be a probability space, $X$ a Banach space, and let $\mb{f}_{\bullet}$ be an $X$-valued stochastic process on $(\Omega,\mc{A},\P)$.
  The \emph{filtration $\mc{A}_{\bullet}$ generated by $\mb{f}_{\bullet}$} is given by
  \begin{equation*}
    \mc{A}_{n} := \sigma(\mb{f}_0,\mb{f}_1,\ldots,\mb{f}_n) \qquad \forall n \in \N.
  \end{equation*}
  The information-theoretic intuition says that at time $n \in \N$, one `knows' the functions $\mb{f}_0, \mb{f}_1, \ldots \mb{f}_n$, as these are in $\mc{A}_n$.
  Furthermore, one also knows all `functions of $\mb{f}_{0}, \ldots, \mb{f}_{1}$', in the sense that one knows all functions of the form
  \begin{equation*}
    g \circ (\mb{f}_0, \mb{f}_1, \ldots, \mb{f}_n) \colon \omega \mapsto g(\mb{f}_0(\omega),\mb{f}_1(\omega),\ldots,\mb{f}_n(\omega))
  \end{equation*}
  where $\map{g}{X^{n+1}}{S}$ is a measurable function mapping into a measurable space $(S, \mc{A}')$ (as such compositions are automatically measurable).
\end{example}

\begin{example}\label{eg:gambling-filtrations}
  Consider the game we introduced in at the start of this chapter.
  At each time $n \in \N$ I flip a fair coin, which comes up Heads ($H$) or Tails $(T)$ with equal probability.
  The natural probability space on which to base this game is the infinite product $\Omega = \{-1,+1\}^{\N}$ (see Example \ref{eg:coordinate-filtration}).
  The value $-1$ represents Tails, while $+1$ represents Heads.
  For each $n \in \N$ let $\map{\pi_n}{\Omega}{\{-1,+1\}}$ be the $n$-th coordinate function, which represents the outcome of the $n$-th coin toss.
  The sequence $\pi_{\bullet}$ is a scalar-valued stochastic process, and the filtration $\mc{A}_{\bullet}$ it generates is precisely the coordinate filtration discussed in Example \ref{eg:coordinate-filtration}.

  
  Your wager at time $n$, the vector $\mb{x}_n \in X$, is allowed to depend on the outcomes $\pi_0, \pi_1, \ldots, \pi_{n-1}$: you do not need to register all your bets in advance.
  In probabilistic language, $\map{\mb{x}_n}{\Omega}{X}$ is $\mc{A}_{n-1}$-measurable, i.e. the stochastic process $\mb{x}_{\bullet}$ is predictable with respect to $\mc{A}_{\bullet}$.

  Now consider the stochastic process $(\mb{s}_n)_{n \in \N}$, representing the evolution of the state of your wallet.
  By definition we have
  \begin{equation*}
    \mb{s}_{n+1} = \mb{s}_n + \pi_{n+1} \mb{x}_{n+1} \qquad \forall n \in \N;
  \end{equation*}
  keep in mind that this is an equality of $X$-valued random variables, i.e. functions $\Omega \to X$.
  Since $\mb{s}_n$, $\pi_{n+1}$, and $\mb{x}_{n+1}$ are all $\mc{A}_{n+1}$-measurable, we find that $\mb{s}_{n+1}$ is $\mc{A}_{n+1}$-measurable (i.e. we know the state of our wallet $\mb{s}_{n+1}$ at time $n+1$).
  Heuristically, $\mb{s}_{n+1}$ should not be $\mc{A}_n$-measurable unless $\mb{x}_{n+1} \equiv 0$, as this would amount to predicting the future (which can only be done by wagering nothing).
  You should prove this rigourously (Exercise \ref{ex:winnings-unpredictability}).
\end{example}

\begin{defn}
  Given a filtration $\mc{A}_\bullet$ on a probability space $(\Omega, \mc{A}, \P)$, a random variable $\map{T}{\Omega}{\N \cup \{\infty\}}$ is called a \emph{stopping time} (with respect to $\mc{A}_{\bullet}$) if 
  \begin{equation*}
    \{\omega \in \Omega: T(\omega) \leq n\} \in \mc{A}_n \qquad \forall n \geq 0.
  \end{equation*}
  The stopping time $T$ is called \emph{finite} if $T$ is almost surely finite.
\end{defn}

Generally stopping times $T$ are defined in terms of stochastic \emph{stopping conditions}.
Interpreting the filtration $\mc{A}_\bullet$ as modelling the available information as time progresses, $T$ being a stopping time says that at time $n$, one `knows' the set of points $\omega \in \Omega$ for which $T(\omega) \leq n$.
Said less precisely, if $T$ is a stopping time, then at time $n$, one can determine whether or not $T \leq n$.

\begin{example}\label{eg:gambling-stoppingtimes}
  We return to our betting game.
  Let's suppose that our goal is to get the state of our wallet $\mb{s} \in X$ into a fixed Borel measurable set $K \subset X$, and that we intend to stop betting once this condition holds (i.e. from that point on we only wager the zero vector).
  Let
  \begin{equation*}
    T_K(\omega) := \inf\{n \in \N : \mb{s}_n(\omega) \in K \}
  \end{equation*}
  with the usual convention that $T_K(\omega) = \infty$ if $\mb{s}_n(\omega) \notin K$ for all $n \in \N$.
  That is, $T_K$ is the first time $n$ at which $\mb{s}_n \in K$.
  Heuristically, at time $n$, we know whether or not our wallet satisfied $\mb{s}_m \in K$ for some $m \leq n$, which indicates that $T_K$ should be a stopping time with respect to the filtration $\mc{A}_{\bullet}$ generated by the stochastic process $\pi_{\bullet}$.
  Rigourously, one shows this by writing for all $n \in \N$
  \begin{equation*}
    \begin{aligned}
      \{\omega \in \Omega : T_K(\omega) \leq n\}
      &= \big\{\omega : \inf\{m : \mb{s}_m(\omega) \in K\} \leq n\big\} \\
      &= \{\omega  : \text{$\mb{s}_m(\omega) \in K$ for some $m \leq n$}\} \\
      &= \bigcup_{m = 0}^n \mb{s}_m^{-1}(K),
    \end{aligned}
  \end{equation*}
  and noting that since each $\mb{s}_m$ is $\mc{A}_m$-measurable, the set above is $\mc{A}_n$-measurable.
  Thus $T_K$ is a stopping time with respect to $\mc{A}_{\bullet}$.
  Whether $T_K$ is a \emph{finite} stopping time depends on the set $K \subset X$, the wager vectors $(\mb{x}_n)_{n \in \N}$, and potentially even the geometry of $X$ (see Exercise \ref{ex:gambling-in-linfty}).
\end{example}

The proof above applies to more general stochastic processes.

\begin{prop}\label{prop:first-hitting-time}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space.
  Let $\mb{f}_{\bullet}$ be an $X$-valued stochastic process adapted to a filtration $\mc{A}_{\bullet}$, and $K \subset X$ a Borel measurable set.
  Then the function $\map{T_K}{\Omega}{\N \cup \{\infty\}}$ defined by
  \begin{equation*}
    T_K(\omega) := \inf\{n \in \N : \mb{f}_n(\omega) \in K \}
  \end{equation*}
  is a stopping time with respect to $\mc{A}_{\bullet}$.
\end{prop}

The stopping time $T_K$ defined above is called the \emph{first hitting time of $K$}.

\section{Conditional expectations}

Given a Banach-valued random variable $\mb{f}$ which is measurable with respect to a $\sigma$-algebra $\mc{A}$, and given a $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, it is natural to ask for the `best' $\mc{B}$-measurable approximation to $\mc{A}$.
One answer is given by the \emph{conditional expectation of $\mb{f}$ given $\mc{B}$}, $\E^{\mc{B}} \mb{f}$.
The information-theoretic interpretation is as follows: given an $\mc{A}$-measurable function $\mb{f}$, and given that we only have the information given by $\mc{B} \subset \mc{A}$, what do we \emph{expect} $\mb{f}$ to be? Again, the answer is $\E^{\mc{B}} \mb{f}$.
Here is the formal definition.

\begin{defn}\label{defn:conditional-expectation}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space.
  Let $\mb{f} \in L^1(\mc{A}; X)$ be an integrable, $\mc{A}$-measurable, $X$-valued random variable.
  Given a $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a \emph{conditional expectation of $f$ given $\mc{B}$} is a $\mc{B}$-measurable random variable $\E^{\mc{B}}f \in L^1(\mc{B}; X) \subset L^1(\mc{A};X)$ such that
  \begin{equation}\label{eq:conditional-expectation-property}
    \qquad \int_B \E^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P \qquad \text{for all $B \in \mc{B}$.}
  \end{equation}
\end{defn}

When the $\sigma$-algebra $\mc{B}$ is simple enough, conditional expectations can be computed explicitly.

\begin{example}\label{eg:atomic-CE}
  Let $(\Omega,\mc{A},\P)$ be a probability space and let $\mc{B} \subset \mc{A}$ be a sub-$\sigma$-algebra which is \emph{atomic}, in the sense that there is a collection of pairwise disjoint subsets $(B_\lambda)_{\lambda \in \Lambda}$ of $\mc{B}$ which generate $\mc{B}$, such that $\P(B_{\lambda}) > 0$ for all $\lambda$, and such that if $B_\lambda$ can be written as a disjoint union $B_\lambda = C \cup D$ for some sets $C,D \in \mc{B}$, then $\P(C) = 0$ or $\P(D) = 0$ (i.e. the sets $B_\lambda$ are \emph{atoms}).
  Let's compute \emph{the} conditional expectation $\E^{\mc{B}}\mb{f}$ of an integrable random variable $\mb{f} \in L^1(\mc{A};X)$ (it turns out there is only one).
  Since the atoms $(B_\lambda)_{\lambda}$ generate $\mc{B}$ and are pairwise disjoint, and since $\E^{\mc{B}}\mb{f}$ is $\mc{B}$-measurable, $\E^{\mc{B}}\mb{f}$ must be constant on each $B_\lambda$, so that
  \begin{equation*}
    \E^{\mc{B}} \mb{f} = \sum_{\lambda \in \Lambda} \1_{B_\lambda} \otimes \mb{x}_\lambda
  \end{equation*}
  for some vectors $\mb{x}_{\lambda} \in X$.
  Averaging over one of the atoms $B_{\lambda}$ and using \eqref{eq:conditional-expectation-property} tells us that
  \begin{equation*}
    \begin{aligned}
      \mb{x}_\lambda = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} \E^{\mc{B}} \mb{f} \, \dd\P = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} \mb{f} \, \dd\P =: \E^{B_{\lambda}} \mb{f}.
    \end{aligned}
  \end{equation*}
  Note that $\P(B_{\lambda}) > 0$ for all $\lambda$, so this makes sense.
  Thus we have
  \begin{equation*}
    \E^{\mc{B}} \mb{f} = \sum_{\lambda \in \Lambda} \1_{B_\lambda} \otimes \E^{B_{\lambda}} \mb{f}.
  \end{equation*}
\end{example}

This example shows that conditional expectations with respect to atomic $\sigma$-algebras exist and are unique.
The same is true for general $\sigma$-algebras, but since we can't decompose a general $\sigma$-algebra into atoms, this takes a few steps.
First we establish the uniqueness of conditional expectations.

\begin{prop}\label{prop:CE-uniqueness}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  For any $\mb{f} \in L^1(\mc{A};X)$ and any $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, if $\E^{\mc{B}}\mb{f}$ and $\td{\E}^{\mc{B}}\mb{f}$ are two conditional expectations of $\mb{f}$ given $\mc{B}$, then $\E^{\mc{B}}\mb{f} \aeeq \td{\E}^{\mc{B}} \mb{f}$.
\end{prop}

\begin{proof}
  First we consider the real one-dimensional case.
  Fix $f \in L^1(\mc{A};\R)$.
  For all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P - \int_B f \, \dd\P = 0.
  \end{equation*}
  Since $\E^{\mc{B}}f - \td{\E}^{\mc{B}}f$ is $\mc{B}$-measurable, the subsets
  \begin{equation*}
    B_+ := \{ \E^{\mc{B}}f - \td{\E}^{\mc{B}}f > 0\} \quad \text{and} \quad B_- := \{\E^{\mc{B}}f - \td{\E}^{\mc{B}}f < 0\}
  \end{equation*}
  are both in $\mc{B}$, so we get
  \begin{equation*}
    \int_\Omega |\E^{\mc{B}}f - \td{\E}^{\mc{B}}f | \, \dd\P
    = \int_{B_+}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P
    -  \int_{B_-}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P
    = 0,
  \end{equation*}
  establishing that $\E^{\mc{B}}f \aeeq \td{\E}^{\mc{B}}f$.

  Now let $X$ be any real Banach space and suppose $\mb{f} \in L^1(\mc{A};X)$.
  To show $\E^{\mc{B}} \mb{f} \aeeq \td{\E}^{\mc{B}} \mb{f}$, by Lemma \ref{lem:coordinatewise-equality-test} it suffices to show that
  \begin{equation}\label{eq:CE-testing-eq}
    \langle \E^{\mc{B}} \mb{f}, \mb{x}^* \rangle \aeeq \langle \td{\E}^{\mc{B}} \mb{f}, \mb{x}^* \rangle \qquad \text{for all $\mb{x}^* \in X^*$.}
  \end{equation}
  This will follow from the one-dimensional case by showing that $\langle \E^{\mc{B}} \mb{f}, \mb{x}^* \rangle$ and $\langle \td{\E}^{\mc{B}} \mb{f}, \mb{x}^* \rangle$ are both conditional expectations of $\langle \mb{f}, \mb{x}^* \rangle$ given $\mc{B}$.
  For all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \langle \E^{\mc{B}} \mb{f}, \mb{x}^* \rangle \, \dd\P
    = \Big\langle \int_B \E^{\mc{B}} \mb{f} \, \dd\P, \mb{x}^* \Big\rangle
    = \Big\langle \int_B \mb{f} \, \dd\P, \mb{x}^* \Big\rangle
    = \int_B \langle \mb{f}, \mb{x}^* \rangle \, \dd\P
  \end{equation*}
  using that $\E^{\mc{B}}\mb{f}$ is a conditional expectation of $\mb{f}$, and the same argument shows that
  \begin{equation*}
    \int_B \langle \td{\E}^{\mc{B}} \mb{f}, \mb{x}^* \rangle \, \dd\P = \int_B \langle \mb{f}, \mb{x}^* \rangle \, \dd\P.
  \end{equation*}
  Thus $\langle \E^{\mc{B}} \mb{f}, \mb{x}^* \rangle$ and $\langle \td{\E}^{\mc{B}} \mb{f}, \mb{x}^* \rangle$ are conditional expectations of $\langle \mb{f}, \mb{x}^* \rangle$ given $\mc{B}$, establishing \eqref{eq:CE-testing-eq}, and thus proving that $\E^{\mc{B}} \mb{f} \aeeq \td{\E}^{\mc{B}} \mb{f}$.

  Finally, if $X$ is a complex Banach space, the result follows by considering real and imaginary parts separately.
\end{proof}

Next we will establish existence, positivity, and $L^p$-nonexpansiveness of conditional expectations in the scalar-valued case.
The positivity in particular will let us deduce the existence of $X$-valued conditional expectations for every Banach space $X$.

\begin{thm}\label{thm:conditional-expectation-existence-scalar}
  Let $(\Omega,\mc{A},\P)$ be a probability space.
  Then for any $f \in L^1(\mc{A})$ and any $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a conditional expectation $\E^{\mc{B}}f$ exists.
  The operator $f \mapsto \E^{\mc{B}}f$ is linear, and for all $p \in [1,\infty]$, $\E^{\mc{B}}$ is a positive and nonexpansive map on $L^p(\mc{A})$: that is, if $f \in L^p(\mc{A})$, then
  \begin{equation*}
    \|\E^{\mc{B}}f\|_{p} \leq \|f\|_{p},
  \end{equation*}
  and if $f$ is nonnegative then so is $\E^{\mc{B}}f$.
\end{thm}

\begin{proof}
  We can prove positivity from the defining property \eqref{eq:conditional-expectation-property}, before we establish existence.
  Suppose $f \in L^1(\mc{A})$ is nonnegative.
  Then for all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P = \int_B f \, \dd\P \geq 0,
  \end{equation*}
  which implies that the $\mc{B}$-measurable function $\E^{\mc{B}}f$ is nonnegative.\footnote{This uses an exercise from measure theory: if $g$ is $\mc{B}$-measurable and $\int_B g \geq 0$ for all $\mc{B}$-measurable sets, then $g \stackrel{\mathrm{a.e.}}{\geq} 0$. Proof: the set $N := \{g(\omega) < 0\}$ is $\mc{B}$-measurable, and assuming it has positive measure leads to the contradiction $0 \leq \int_B g < 0$.}
  
  Now fix $p \in [1,\infty]$ and let $f \in L^p(\mc{A})$; we will construct a linear nonexpansive conditional expectation operator $\E^{\mc{B}}$ on $L^p(\mc{B};\K)$.

  \textbf{Mild case: $p > 1$.}
  In this case, $p' < \infty$.
  The inclusion map $\map{\iota}{L^{p'}(\mc{B})}{L^{p'}(\mc{A})}$ is nonexpansive, so its adjoint $\map{\E^{\mc{B}} := \iota^*}{L^p(\mc{A})}{L^p(\mc{B})}$ is also nonexpansive.\footnote{This uses that $L^p$ is the dual of $(L^{p'})^*$, which requires $p' < \infty$.}
  For all $f \in L^p(\mc{A})$ and $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P = \langle \iota^* f, \1_{B} \rangle = \langle f, \iota \1_{B} \rangle = \langle f, \1_{B} \rangle = \int_B f \, \dd\P,
  \end{equation*}
  so $\E^{\mc{B}} f \in L^p(\mc{B}) \subset L^1(\mc{B})$ is a conditional expectation of $f$ given $\mc{B}$.


  \textbf{(German) spicy case: $p = 1$.} In this case $L^1$ is strictly contained in the dual of $L^\infty$, so taking an adjoint of the inclusion $L^\infty(\mc{B}) \to L^\infty(\mc{A})$ is not so straightforward.\footnote{See Exercise \ref{ex:conditional-expectation-as-adjoint}.} 
  Instead we argue by density.
  We know that $L^2(\mc{A})$ is dense in $L^1(\mc{A})$, so we aim to extend the conditional expectation defined above (in the case $p=2$) by continuity.
  For $f \in L^2(\mc{A})$ and $g \in L^\infty(\mc{B})$ we have
  \begin{equation*}
    |\langle \E^{\mc{B}} f, g \rangle| = |\langle f, \iota g \rangle| = |\langle f, g \rangle| \leq \|f\|_1 \|g\|_\infty
  \end{equation*}
  using that $L^\infty(\mc{B}) \subset L^2(\mc{B})$.
  Taking the supremum over all nonzero $g \in L^\infty(\mc{B})$ proves that
  \begin{equation*}
    \|\E^{\mc{B}} f\|_1 \leq \|f\|_1,
  \end{equation*}
  so $\E^{\mc{B}}$ extends to a nonexpansive map $L^1(\mc{A}) \to L^1(\mc{B})$.
  For $f \in L^1(\mc{A})$ and $B \in \mc{B}$, using that integration on $B$ is a continuous linear functional on $L^1$, we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P
    = \lim_{n \to \infty} \int_B \E^{\mc{B}} f_n \, \dd\P
    = \lim_{n \to \infty} \int_B  f_n \, \dd\P
    = \int_B f \, \dd\P
  \end{equation*}
  where $f_n$ is a sequence in $L^2(\mc{A})$ converging to $f$ in $L^1(\mc{A})$.
  Thus $\E^{\mc{B}}f$ is a conditional expectation of $f$ given $\mc{B}$, and we are done.
\end{proof}

Note that the proof of the previous result also establishes the following adjoint relation, which can also be proven directly from the defining property \eqref{eq:conditional-expectation-property} (Exercise \ref{ex:CE-adjoint}).

\begin{prop}\label{prop:CE-adjoint}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$.
  For all $p \in (1,\infty]$, the conditional expectation $\E^{\mc{B}}$ on $L^p(\mc{A})$ is the adjoint of the corresponding conditional expectation on $L^{p'}(\mc{A})$.
\end{prop}

Now we can use the extension theorem for positive operators to show the existence of conditional expectations of Banach-valued random variables.

\begin{prop}
  Let $(\Omega,\mc{A},\P)$ be a probability space, let $\mc{B}$ be a $\sigma$-subalgebra of $\mc{A}$, and let $X$ be a Banach space.
  Then for any $\mb{f} \in L^1(\mc{A};X)$, a conditional expectation $\E_X^{\mc{B}}\mb{f}$ of $\mb{f}$ given $\mc{B}$ exists.
  Furthermore, for all $p \in [1,\infty]$, $\E^{\mc{B}}$ is nonexpansive on $L^p(\mc{A};X)$.
\end{prop}


\begin{proof}  
  First fix $p \in [1,\infty)$.
  Since the conditional expectation $\E^{\mc{B}}$ is a positive operator on $L^p(\mc{A})$, by Theorem \ref{thm:positive-extensions} it admits a bounded $X$-valued extension, which we denote by $\E_X^{\mc{B}}$.
  Since $\E^{\mc{B}}$ is nonexpansive, so is $\E_X^{\mc{B}}$.
  We just need to show that for all $\mb{f} \in L^p(\mc{A};X)$, $\E_X^{\mc{B}}\mb{f}$ is a conditional expectation of $f$ given $\mc{B}$; we will do this by scalarisation.
  For all $B \in \mc{B}$ and all functionals $\mb{x}^* \in X^*$, since the function $\langle \mb{f}, \mb{x}^* \rangle$ is in $L^1(\mc{A})$, we have
  \begin{equation*}
    \begin{aligned}
      \Big\langle \int_B \E_X^{\mc{B}} \mb{f} \, \dd\P, \mb{x}^* \Big\rangle
      &= \int_B \langle \E_X^{\mc{B}} \mb{f}, \mb{x}^* \rangle \, \dd\P \\
      &\stackrel{(*)}{=} \int_B \E^{\mc{B}} (\langle \mb{f}, \mb{x}^* \rangle) \, \dd\P
      = \int_B \langle \mb{f}, \mb{x}^* \rangle \, \dd\P
      = \Big\langle \int_B \mb{f} \, \dd\P, \mb{x}^* \Big\rangle.
    \end{aligned}
  \end{equation*}
  (see Exercise \ref{ex:tensor-extension-basic-props} for the starred equality).
  Since this holds for all $\mb{x}^* \in X^*$, we have
  \begin{equation*}
     \int_B \E_X^{\mc{B}} \mb{f} \, \dd\P = \int_B \mb{f} \, \dd\P,
   \end{equation*}
   which shows that $\E_X^{\mc{B}}$ is a conditional expectation of $\mb{f}$ given $\mc{B}$.

   Now we establish the result for $p = \infty$: let $\mb{f} \in L^\infty(\mc{A};X) \subset L^2(\mc{A};X)$.
   Then a conditional expectation $\E_X^{\mc{B}} \mb{f}$ of $\mb{f}$ given $\mc{B}$ is defined, but so far we only know that it is in $L^2(\mc{B};X)$: we just need to show that $\|\E_X^{\mc{B}} f\|_\infty \leq \|f\|_\infty$.
   We can test this by duality using Proposition \ref{prop:bochner-preduality} and that $L^2(\mc{B}; X^*)$ is dense in $L^1( \mc{B}; X^*)$.
   
   For all $\mb{g} \in L^2(\mc{B}; X^*)$, since the operator $\E^{\mc{B}} \in \Lin(L^2(\mc{A}))$ is self-adjoint, we have
\begin{equation*}
  \begin{aligned}
    | \langle \E_X^{\mc{B}} \mb{f}, \mb{g} \rangle |
    =  | \langle \td{\E^{\mc{B}}} \mb{f}, \mb{g} \rangle | 
    &\stackrel{(*)}{=} | \langle \mb{f}, \td{\E^{\mc{B}}} \mb{g} \rangle | \\
    &\leq  \|\mb{f}\|_{L^\infty(\mc{A};X)} \|\E_X^{\mc{B}} \mb{g}\|_{L^1(\mc{A};X^*)} \\
    &\leq  \|\mb{f}\|_{L^\infty(\mc{A};X)} \| \mb{g}\|_{L^1(\mc{A};X^*)}.
  \end{aligned}
\end{equation*}
For the starred equality see Exercise \ref{ex:tensor-adjoint}, particularly the identity \eqref{eq:tensor-adjoint-identity}.
Taking the supremum over all nonzero $\mb{g} \in L^2(\mc{B};X^*)$ completes the proof.
\end{proof} 

We will introduce further properties of conditional expectations as they are needed.
For now, we note a multiplication property for $\mc{B}$-measurable multipliers, with proof left as Exercise \ref{ex:CE-measurable-op}.

\begin{prop}\label{prop:CE-measurable-op}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$, and let $X$ and $Y$ be Banach spaces.
  Consider an operator-valued function $T \in L^\infty(\mc{B}; \Lin(X,Y))$.
  For $\mb{f} \in L^1(\mc{A}; X)$ define the function $T\mb{f} \in L^1(\mc{A}; Y)$ by
  \begin{equation*}
    T\mb{f}(\omega) := T(\omega) \mb{f}(\omega).
  \end{equation*}
  Then $\E^{\mc{B}}(T\mb{f}) = T \E^{\mc{B}}\mb{f}$ for all $\mb{f} \in L^1(\mc{A};X)$.
\end{prop}

\begin{rmk}
  We have only considered conditional expectations $\E^{\mc{B}}$ on probability spaces, but the concept can be extended to general measure spaces $(S,\mc{A},\mu)$ provided that the measure $\mu$ is $\sigma$-finite on the $\sigma$-subalgebra $\mc{B} \subset \mc{A}$ (although the arguments require a fair bit of modification).
  This approach is taken in \cite{HNVW16}.
\end{rmk}

\section{Martingales and martingale transforms}

\begin{defn}
  Let $(\Omega, \mc{A}, \P)$ be a probability space with a filtration $\mc{A}_{\bullet}$, and let $X$ be a Banach space.
  A stochastic process $\mb{f}_{\bullet}$ with $\mb{f}_n \in L^1(\Omega; X)$ for all $n \in \N$ is called a \emph{martingale} with respect to $\mc{A}_{\bullet}$ if 
  \begin{equation}\label{eq:mgale-defining-property}
    \mb{f}_n = \E^{\mc{A}_n} \mb{f}_{n+1} \qquad \forall n \in \N.
  \end{equation}
  Note that in particular $\mb{f}_{\bullet}$ is adapted to $\mc{A}_{\bullet}$.
\end{defn}

\begin{rmk}
  Now is a good time to complete Exercise \ref{ex:martingale-elementary-properties}, establishing a few elementary properties of martingales. 
\end{rmk}

Martingales are stochastic processes that are `balanced' in the following sense: at time $n$, the best estimate of the state of the process at time $n+1$ is precisely the current state of the process.

\begin{example}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $\mc{A}_{\bullet}$ a filtration.
  Let $X$ be a Banach space and $\mb{f} \in L^1(\Omega;X)$.
  For each $n \in \N$ define $\mb{f}_n := \E^{\mc{A}_n}\mb{f} \in L^1(\Omega;X)$.
  Then by the monotonicity property of conditional expectations (see Exercise \ref{ex:CE-monotonicity})
  \begin{equation*}
    \E^{\mc{A}_n} \mb{f}_{n+1} = \E^{\mc{A}_n} \E^{\mc{A}_{n+1}} \mb{f} = \E^{\mc{A}_n} \mb{f} = \mb{f}_n,
  \end{equation*}
  so $\mb{f}_{\bullet}$ is a martingale, called the \emph{martingale associated with $\mb{f}$ (and $\mc{A}_{\bullet}$)}. 

  When $\Omega = [0,1)$ is the unit interval with Borel $\sigma$-algebra and Lebesgue measure, and when we consider the dyadic filtration $\mc{A}_{\bullet}$ as in Example \ref{eg:dyadic-filtration}, the martingale associated with a function $\mb{f} \in L^1([0,1);X)$ is given by
  \begin{equation*}
    \mb{f}_n = \sum_{I \in \mc{D}_n} \1_I \otimes \langle \mb{f} \rangle_I
  \end{equation*}
  where $\mc{D}_n$ is the set of dyadic intervals $I \subset [0,1)$ of length $2^{-n}$ and $\langle \mb{f} \rangle_I = \fint_I \mb{f}(t) \, \dd t$ is the average of $\mb{f}$ on $I$ (we did this computation in Example \ref{eg:atomic-CE}).
  Each dyadic interval $I \in \mc{D}_{n-1}$ can be `halved', i.e. $I = I_{-} \cup I_{+}$, where $I_{\pm} \in \mc{D}_{n}$ and $I_-$ is to the left of $I_+$ (i.e. $\sup_{I_-} = \inf_{I_+}$).
  Let's compute the difference $d\mb{f}_n$ on an interval $I \in \mc{D}_{n-1}$:
  \begin{equation*}
    \begin{aligned}
      \1_{I}(d\mb{f}_{n}) &= \1_{I}(\mb{f}_{n} - \mb{f}_{n-1}) \\
      &= \1_{I_{-}} \otimes \langle \mb{f} \rangle_{I_{-}} + \1_{I_{+}} \otimes \langle \mb{f} \rangle_{I_{+}} - (\1_{-} + \1_{+}) \otimes \langle \mb{f} \rangle_{I} \\
      &= \1_{I_{-}} \otimes \Big( \frac{2}{|I|}\int_{I_{-}} \mb{f} - \frac{1}{|I|}\int_{I} \mb{f} \Big) + \1_{I_{+}} \otimes \Big( \frac{2}{|I|}\int_{I_{+}} \mb{f} - \frac{1}{|I|}\int_{I} \mb{f} \Big) \\
      &= \1_{I_{-}} \otimes \Big( \frac{1}{|I|}\int_{I_{-}} \mb{f} - \frac{1}{|I|}\int_{I_{+}} \mb{f} \Big) - \1_{I_{+}} \otimes \Big( \frac{1}{|I|}\int_{I_{-}} \mb{f} - \frac{1}{|I|}\int_{I_{+}} \mb{f} \Big) \\
      &= h_I \otimes \langle \mb{f}, h_I \rangle
    \end{aligned}
  \end{equation*}
  where
  \begin{equation*}
    h_I := \frac{1}{|I|^{1/2}} (\1_{I_-} - \1_{I_+})
  \end{equation*}
  is the ($L^2$-normalised) \emph{Haar function} associated with the dyadic interval $I$.
  Thus the representation of $\mb{f}$ in terms of martingale differences corresponds to its Haar expansion (ignoring the issue of whether the sums converge):
  \begin{equation*}
    \begin{aligned}
      \mb{f} &= \mb{f}_0 + \sum_{n \geq 1} d\mb{f}_{n} \\
      &= \langle \mb{f} \rangle_{[0,1)} + \sum_{n \geq 1} \sum_{I \in \mc{D}_{n-1}} h_I \otimes \langle \mb{f}, h_I \rangle \\
     &= \langle \mb{f} \rangle_{[0,1)} + \sum_{I \in \mc{D}}h_I \otimes \langle \mb{f}, h_I \rangle,
   \end{aligned}
  \end{equation*}
\end{example}
where $\mc{D} = \cup_{n \in \N} \mc{D}_{n}$ is the set of all dyadic intervals.

In general, given a function $\mb{f} \in L^1(\Omega;X)$ and a filtration $\mc{A}_{\bullet}$ as above, we can formally write\footnote{This is assuming that $\mb{f}$ is $\mc{A}_{\infty}$-measurable, in the notation of Theorem \ref{thm:mgale-conv-Lp}. The filtration $\mc{A}_{\bullet}$ may not contain enough information to recover $\mc{A}$.}
\begin{equation*}
  \mb{f} = \mb{f}_{0} + \sum_{n \geq 1} d\mb{f}_{n} = \lim_{n \to \infty} \E^{\mc{A}_{n}} \mb{f}.
\end{equation*}
In what sense can we take this limit, and thus represent $\mb{f}$ in terms of its associated martingale?
First we will show convergence in $L^p$-norm; this will be extended to almost sure convergence in Theorem \ref{thm:mgale-pw-conv}. 

\begin{thm}\label{thm:mgale-conv-Lp}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $\mc{A}_{\bullet}$ a filtration.
  Let $\mc{A}_{\infty} \subset \mc{A}$ be the $\sigma$-subalgebra generated by $\cup_{n \in \N} \mc{A}_{n}$.
  Let $X$ be a Banach space and $p \in [1,\infty)$.
  Then for all $\mb{f} \in L^p(\Omega;X)$ we have $\E^{\mc{A}_{n}} \mb{f} \to \E^{\mc{A}_\infty} \mb{f}$ in $L^p$.
\end{thm}

\begin{proof}
  Since $\E^{\mc{A}_{n}} \mb{f} = \E^{\mc{A}_{n}} \E^{\mc{A}_{\infty}} \mb{f}$ for all $n \in \N$, it suffices to assume that $\mb{f} = \E^{\mc{A}_{\infty}} \mb{f}$, i.e. that $f$ is $\mc{A}_{\infty}$-measurable.
  We will reduce to showing that
  \begin{equation*}
    \bigcup_{n \in \N} L^p(\mc{A}_{n};X)
  \end{equation*}
  is dense in $L^p(\mc{A}_{\infty};X)$.
  Assuming this is true for the moment, given $\varepsilon > 0$, there exists $n \in \N$ and $\mb{g} \in L^p(\mc{A}_{n};X)$ such that $\|\mb{f} - \mb{g}\|_p < \varepsilon$.
  We also have $\E^{\mc{A}_{m}} \mb{g} = \mb{g}$ for all $m > n$, so for all such $m$ we have
  \begin{equation*}
    \begin{aligned}
      \|\E^{\mc{A}_{m}} \mb{f} - \mb{f}\|_p
      &\leq \|\E^{\mc{A}_m}(\mb{f} - \mb{g})\|_p + \|\E^{\mc{A}_{m}} \mb{g} - \mb{f}\|_p \\
      &\leq 2\|\mb{f} - \mb{g}\|_{p} < 2\varepsilon.
    \end{aligned}
  \end{equation*}
  Taking $m \to \infty$ and noting that $\varepsilon$ was arbitrary, we find that $\E^{\mc{A}_{m}} \mb{f} \to \mb{f}$ in $L^p$.

  It remains to prove the density statement, and by Exercise \ref{ex:general-density} it suffices to do this in the scalar case $X = \K$.
  Consider the collection of sets
  \begin{equation*}
    \mc{C} := \Big\{A \in \mc{A}_{\infty} : \1_{A} \in \overline{\bigcup_{n \in \N} L^p(\mc{A}_{n})} \Big\} \subset \mc{A}_{\infty},
  \end{equation*}
  where the closure is in $L^p(\mc{A}_{\infty})$.
  Then $\mc{C}$ is a $\sigma$-algebra which contains $\mc{A}_{n}$ for each $n \in \N$, so that $\mc{A}_{\infty} = \mc{C}$, which implies that all $\mc{A}_{\infty}$-simple functions are contained in $\overline{\bigcup_{n \in \N} L^p(\mc{A}_{n})}$, and thus that this closure is $L^p(\mc{A}_{\infty})$.  
\end{proof}

\begin{example}\label{eg:sum-process}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, $X$ a Banach space, and let $(\mb{g}_n)_{n \in \N}$ be a sequence of integrable $X$-valued random variables which are mutually independent (see Section \ref{sec:probability} in the appendices), such that $\E\mb{g}_{n} = 0$ for all $n \geq 1$.
  Let $\mc{A}_{\bullet}$ be the filtration generated by the process $\mb{g}_{\bullet}$, and for each $n \in \N$ let $\mb{\sigma}_n := \sum_{m=0}^n \mb{g}_m$ be the sum of the first $n+1$ random variables.
  Then we have
  \begin{equation*}
    \begin{aligned}
      \E^{\mc{A}_n} \mb{\sigma}_{n+1} &= \E^{\mc{A}_n}\Big( \sum_{m=0}^n\mb{g}_m \Big) + \E^{\mc{A}_n} \mb{g}_{n+1} \\
      &=\sum_{m=0}^n \mb{g}_m + \E\mb{g}_{n+1} = \mb{\sigma}_n
    \end{aligned}
  \end{equation*}
  since the random variables $(\mb{g}_m)_{m=0}^n$ are $\mc{A}_n$-measurable and $\mb{g}_{n+1}$ is independent of $\mc{A}_n$ (Exercise \ref{ex:ce-ind}).
  Thus the sum process $\mb{\sigma}_{\bullet}$ is a martingale.
\end{example}

An important class of operators in stochastic analysis are \emph{martingale transforms}.\footnote{These are the discrete-time analogues of stochastic integrals.}
These are defined by acting termwise on a martingale's difference sequence.
They turn out to be an important model for many operators in harmonic analysis.

\begin{defn}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, $X$ a Banach space, and let $\mb{f}_{\bullet}$ be an $X$-valued martingale with respect to a filtration $\mc{A}_{\bullet}$.
  Let $Y$ be another Banach space, and let $(T_n)_{n \in \N}$ be a sequence of operators in $L^\infty(\Omega; \Lin(X,Y))$ which is predictable with respect to $\mc{A}_{\bullet}$.
  The \emph{martingale transform of $\mb{f}_{\bullet}$ by $T_{\bullet}$} is the $Y$-valued martingale $(T \cdot \mb{f})_{\bullet}$ defined by
  \begin{equation*}
    (T \cdot \mb{f})_n := \sum_{m=0}^n T_m d\mb{f}_m.
  \end{equation*}
\end{defn}

To see that the martingale transform $(T \cdot \mb{f})_{\bullet}$ is itself a martingale, first note that integrability of each $(T \cdot \mb{f})_n$ follows from that of each $\mb{f}_m$ and the a.e. uniform boundedness of each $T_m$.
It then suffices to show that each $d(T\cdot \mb{f})_{n}$ is $\mc{A}_{n}$-measurable, and that
\begin{equation}\label{eq:transform-independent-increments}
  \E^{\mc{A}_n} d(T \cdot \mb{f})_{n+1} = 0 \qquad \forall n \in \N
\end{equation}
(see Exercise \ref{ex:martingale-elementary-properties}).
Since
\begin{equation*}
  d(T \cdot \mb{f})_{n} = T_{n} d\mb{f}_{n},
\end{equation*}
the assumptions immediately give $\mc{A}_{n}$-measurability.
As for \eqref{eq:transform-independent-increments}, the predictability assumption on $T_{\bullet}$ and the fact that $\mb{f}_{\bullet}$ is a martingale gives us
\begin{equation*}
  \E^{\mc{A}_n} d(T \cdot \mb{f})_{n+1} = \E^{\mc{A}_n} (T_{n+1} d\mb{f}_{n+1}) \stackrel{(*)}{=} T_{n+1} \E^{\mc{A}_n} d\mb{f}_{n+1} = 0,
\end{equation*}
using Proposition \ref{prop:CE-measurable-op} to give the starred equality.

\begin{example}\label{eg:betting-game-martingale}
  We return once more to our betting game, with notation given in Example \ref{eg:gambling-filtrations}.
  Consider the stochastic process $\mb{s}_\bullet$ representing the evolution of the state of your wallet: recall that
  \begin{equation*}
    d\mb{s}_{n+1} = \mb{s}_{n+1} - \mb{s}_{n} = \pi_{n+1} \mb{x}_{n+1} \qquad \forall n \in \N,
  \end{equation*}
  where $\pi_{n+1}$ is the outcome of the coin toss and $\mb{x}_{n+1}$ is the vector wagered at time $n+1$.
  The process $\pi_{\bullet}$ generates the filtration $\mc{A}_{\bullet}$.
  Since the random variables $\pi_{\bullet}$ are mutually independent, the sum process $\sigma_{\bullet}$ given by
  \begin{equation*}
    \sigma_{n} := \sum_{m=0}^n \pi_{m}
  \end{equation*}
  is a martingale (see Example \ref{eg:sum-process}).
  Now suppose that the wager vectors $\mb{x}_n \in L^1(\Omega;X)$ are integrable.
  We assumed that this sequence is predictable with respect to $\mc{A}_{\bullet}$ (meaning that we place the $n$-th bet before knowing the outcome of the $n$-th coin toss), so the martingale transform $(\mb{x} \cdot \sigma)_{\bullet}$ is defined.\footnote{Technically we are identifying $X$ with $\Lin(\C;X)$ here. Given a vector $\mb{y} \in X$, the associated linear operator $\C \to X$ maps $\lambda \in \C$ to $\lambda \mb{y} \in X$.}
  The difference sequence of $(\mb{x} \cdot \sigma)_{\bullet}$ is
  \begin{equation*}
    d(\mb{x} \cdot \sigma)_{n+1} = \mb{x}_{n+1} d\sigma_{n+1} = \pi_{n+1} \mb{x}_{n+1} = d\mb{s}_{n+1}, 
  \end{equation*}
  with initial term
  \begin{equation*}
    (\mb{x} \cdot \sigma)_0 = \mb{x}_{0} \pi_{0} = \mb{s}_{0},
  \end{equation*}
  so we have the equality of martingales $\mb{s}_{\bullet} = (\mb{x} \cdot \sigma)_{\bullet}$.
  That is, the state of your wallet $\mb{s}_{\bullet}$ is a martingale, and it is given by the martingale transform of the sum of coin flips $\sigma_{\bullet}$ by the wager vectors $\mb{x}_{\bullet}$.
  This says that the outcome of our $X$-valued betting game is given by a martingale transform of the outcome of the simple coin-toss game.
\end{example}


\section{Maximal inequalities and pointwise convergence}

In Theorem \ref{thm:mgale-conv-Lp} we proved the $L^p$-limit $\E^{\mc{A}_{n}} \mb{f} \to \E^{\mc{A}_{\infty}} \mb{f}$ for a function $\mb{f} \in L^p(X)$.
In this section we will consider almost-everywhere pointwise convergence.
A general principle usually attributed to Banach says that a.e. pointwise convergence results can be proven by combining $L^p$-convergence results (which we have) with $L^p$-bounds for an appropriate \emph{maximal operator}.
The maximal operator relevant to our problem is defined as follows.

\begin{defn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  Given an $X$-valued stochastic process $\mb{f}_{\bullet}$ we define the \emph{Doob maximal function}
  \begin{equation*}
    \mc{M}(\mb{f}_{\bullet})(\omega) := \sup_{n \in \N} \|\mb{f}_{n}(\omega)\|_{X} \qquad \forall \omega \in \Omega.
  \end{equation*}
  We call $\mc{M}$ the \emph{Doob maximal operator}.
\end{defn}

Note that the Doob maximal function is scalar-valued (and in fact non-negative), and that it is implicitly defined in terms of the scalar-valued process $\|\mb{f}_{\bullet}\|_{X}$.
In fact, the theory we will present concerning the maximal function is essentially scalar-valued.
If you're familiar with stochastic processes, you would have seen it before.

\begin{defn}
  Let $(\Omega, \mc{A}, \P)$ be a probability space with a filtration $\mc{A}_{\bullet}$.
  A real-valued stochastic process $f_{\bullet}$ with each $f_n \in L^1(\Omega; \R)$ is called a \emph{submartingale} with respect to $\mc{A}_{\bullet}$ if 
  \begin{equation}\label{eq:mgale-defining-property}
    f_n \stackrel{\ae}{\leq} \E^{\mc{A}_n} f_{n+1} \qquad \forall n \in \N.
  \end{equation}
\end{defn}

The submartingales we consider are all defined in terms of vector-valued martingales.
Consider a martingale $\mb{f}_{\bullet}$ taking values in a Banach space $X$, with respect to a filtration $\mc{A}_{\bullet}$.
Then by the pointwise estimate \eqref{eq:positive-pw-est} in Theorem \ref{thm:positive-extensions} we have
\begin{equation*}
  \|\mb{f}_n\|_X = \|\E^{\mc{A}_n} \mb{f}_{n+1}\|_X \stackrel{\ae}{\leq} \E^{\mc{A}_n} \|\mb{f}_{n+1}\|_X, \qquad \forall n \in \N,
\end{equation*}
so that the real-valued process $(\|\mb{f}_{n}\|_X)_{n \in \N}$ is a submartingale.
In fact, it is a non-negative submartingale, and to make things simpler we will only consider non-negative submartingales in what follows.\footnote{Actual probabilists will probably be screaming at me at this point. I am cutting corners here, but it shouldn't have any impact on the vector-valued theory. Or will it? Let's hope not. }

\begin{thm}[Doob's maximal inequalities]\label{thm:doob}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and let $f_{\bullet}$ be a non-negative submartingale with respect to a filtration $\mc{A}_{\bullet}$.
  Then for all $t > 0$ we have
  \begin{equation*}
    t\P(\{\mc{M}(f_{\bullet}) > t\}) \leq   \sup_{n \in \N} \int_{\{\mc{M}(f_{\bullet}) > t\}} f_{n} \, \dd\P,
  \end{equation*}
  and for all $p \in (1,\infty)$ we have
  \begin{equation*}
    \|\mc{M}(f_{\bullet})\|_{L^p(\Omega)} \leq p' \sup_{n \in \N} \|f_{n}\|_{L^p(\Omega)}.
  \end{equation*}
\end{thm}

Before proving Doob's maximal inequalities, we state a quick consequence for $X$-valued martingales, which as we saw before induce non-negative submartingales by taking the norm pointwise.
The first inequality is written in terms of the \emph{Lorentz space} (or \emph{weak $L^1$ space}) $L^{1,\infty}$.

\begin{cor}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space, and let $\mb{f}_{\bullet}$ be an $X$-valued martingale.
  Then we have
  \begin{equation*}
    \|\mc{M}(\mb{f}_{\bullet})\|_{L^{1,\infty}(\Omega)} :=  \sup_{t > 0} t\P(\{\mc{M}(\mb{f}_{\bullet}) > t\}) \leq  \sup_{n \in \N} \|\mb{f}_{n}\|_{L^1(\Omega;X)},
  \end{equation*}
  and for all $p \in (1,\infty)$ we have
  \begin{equation*}
    \|\mc{M}(\mb{f}_{\bullet})\|_{L^p(\Omega;X)} \leq p' \sup_{n \in \N} \|\mb{f}_{n}\|_{L^p(\Omega;X)}.
  \end{equation*}
\end{cor}

\begin{proof}[Proof of Theorem \ref{thm:doob}]
  Fix $t > 0$ and define the random variable
  \begin{equation*}
    T :=  \inf\{n \in \N : f_n > t\}.
  \end{equation*}
  By Proposition \ref{prop:first-hitting-time}, $T$ is a stopping time relative to the filtration $\mc{A}_{\bullet}$ (it is the first hitting time of the set $(t,\infty)$).
  Then we have
  \begin{equation*} 
    \begin{aligned}
      t\P(\{\mc{M}(f_{\bullet}) > t\})
      = t\P(\{T < \infty\})
      &= \lim_{N \to \infty} \sum_{n = 0}^{N} t\P(\{T = n\}) \\
      &\leq \lim_{N \to \infty} \sum_{n = 0}^{N} \int_{\{T = n\}} f_n \, \dd\P  \\ 
      &\leq \lim_{N \to \infty} \sum_{n = 0}^{N} \int_{\{T = n\}} \E^{\mc{A}_{n}} f_N \, \dd\P 
    \end{aligned}
  \end{equation*}
  using that $f_n > t$ on the set $\{T = n\}$ (this uses that $f_{n}$ is nonnegative), and that $f_{\bullet}$ is a submartingale.
  Since $T$ is a stopping time with respect to $\mc{A}_{\bullet}$ we have $\{T = n\} \in \mc{A}_{n}$, so for all $N \in \N$ we have
  \begin{equation*}
    \begin{aligned}
     \sum_{n = 0}^{N} \int_{\{T = n\}} \E^{\mc{A}_{n}} f_N \, \dd\P 
    =  \sum_{n = 0}^{N} \int_{\{T = n\}} f_{N} \, \dd\P 
    &=  \int_{\{T \leq N\}} f_{N} \, \dd\P \\
    &\leq \sup_{N \in \N} \int_{\{\mc{M}(f_{\bullet}) > t\}} f_{N} \, \dd\P
  \end{aligned}
  \end{equation*}
  proving the first inequality.

  Before moving on to the second inequality, we observe that we have proven something slightly stronger: fix $N \in \N$ and consider the `stopped submartingale' $f_{\bullet}^{N}$, defined by $f_{n}^{N} := f_{\min(n,N)}^{N}$ for all $n \in \N$.\footnote{$f_{\bullet}^{N}$ is indeed a submartingale: this can be checked directly by separating the cases $n < N$ and $n \geq N$.}
  Then by stopping at the second last line of the previous proof, we find that
  \begin{equation}\label{eq:doob-stopped}
    t\P(\{\mc{M}(f_{\bullet}^{N}) > t\}) \leq \int_{\{\mc{M}(f_{\bullet}^{N}) > t\}} f_{N} \, \dd \P.
  \end{equation}
  This `stopped' version of the previous estimate can be extrapolated to $L^p$-estimates.

  Now fix $1 < p < \infty$ and $N \in \N$, and use the `stopped' estimate \eqref{eq:doob-stopped} to deduce
  \begin{equation*}
    \begin{aligned}
      \|\mc{M}(f_{\bullet}^{N})\|_{p}^{p}
      &= \int_0^\infty pt^{p-1} \P(\{\mc{M}(f_{\bullet}^{N}) > t\}) \, \dd t \\
      &\leq \int_0^\infty pt^{p-2} \int_{\{\mc{M}(f_{\bullet}^{N}) > t\}} f_{N}(\omega) \, \dd\P(\omega) \, \dd t \\
      &=  \int_{\Omega} f_{N}(\omega) \Big( \int_0^{\mc{M}(f_{\bullet}^{N})(\omega)} pt^{p-2} \, \dd t \Big) \, \dd\P(\omega) \\
      &=  \int_{\Omega} \frac{p}{p-1} f_{N}(\omega) \mc{M}(f_{\bullet}^{N})(\omega)^{p-1} \, \dd\P(\omega) \\
      &\leq  p' \|f_{N}\|_{p} \|\mc{M}(f_\bullet^{N})\|_{p}^{p-1}
    \end{aligned}
  \end{equation*}
  using H\"older's inequality in the last step.
  Dividing through by $\|\mc{M}(f_{\bullet}^{N})\|_{p}^{p-1}$ yields
  \begin{equation*}
    \|\mc{M}(f_{\bullet}^{N})\|_{p} \leq p' \|f_{N}\|_{p} \leq p' \sup_{n \in \N} \|f_{n}\|_{p}.
  \end{equation*}
  Finally, note that the functions $\mc{M}(f_{\bullet}^{N})$ are monotonically increasing in $N$, and that
  \begin{equation*}
    \lim_{N \to \infty} \mc{M}(f_{\bullet}^{N})(\omega) = \lim_{N \to \infty} \sup_{1 \leq n \leq N} |f_{n}(\omega)| = \mc{M}(f_{\bullet})(\omega).
  \end{equation*}
  Thus by the monotone convergence theorem we have
  \begin{equation*}
    \|\mc{M}(f_{\bullet})\|_{p} \leq p' \sup_{n \in \N} \|f_{n}\|_{p},
  \end{equation*}
  as claimed.
\end{proof}

By combining Doob's maximal inequality with the $L^p$-convergence result of Theorem \ref{thm:mgale-conv-Lp}, one (i.e. you) can prove the following pointwise convergence theorem.

\begin{thm}\label{thm:mgale-pw-conv} 
  Let $(\Omega, \mc{A}, \P)$ be a probability space with a filtration $\mc{A}_{\bullet}$.
  Let $\mc{A}_{\infty} \subset \mc{A}$ be the $\sigma$-subalgebra generated by $\cup_{n \in \N} \mc{A}_{n}$.
  Let $X$ be a Banach space and $p \in [1,\infty)$.
  Then for all $\mb{f} \in L^p(\Omega;X)$ we have $\E^{\mc{A}_{n}} \mb{f} \to \E^{\mc{A}_\infty} \mb{f}$ pointwise almost everywhere.
\end{thm}

\begin{proof}
  See Exercise \ref{ex:mgale-conv}.
\end{proof}

Now we turn to a kind of converse question, which will lead to an interesting Banach space property.
Given a Banach space $X$, which $X$-valued martingales $\mb{f}_{\bullet}$ are of the form $\mb{f}_{n} = \E^{\mc{A}_{n}} \mb{f}$ for some function $\mb{f} \in L^p(X)$?
First we will need an evidently necessary $L^p$-boundedness condition.

\begin{defn}
  Let $X$ be a Banach space.
  An $X$-valued stochastic process $\mb{f}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$ is called \emph{$L^p$-bounded} if
  \begin{equation*}
    \sup_{n \in \N} \|\mb{f}_n\|_{L^p(\Omega;X)} < \infty.
  \end{equation*}
\end{defn}

Given a general $L^p$-bounded $X$-valued martingale $(\mb{f}_n)_{n \in \N}$, does it automatically hold that $\mb{f}_n = \E^{\mc{A}_n} \mb{f}$ for some $\mb{f} \in L^p(\Omega;X)$?
The answer turns out to depend on the geometry of $X$, and we will discuss this in the next section.
For now we will quickly settle the scalar case.
When $p=1$ we will need an additional condition to guarantee relative weak compactness.

\begin{defn}\label{defn:UI}
  Let $(\Omega,\mc{A},\P)$ be a probability space.
  A bounded subset $\mc{F} \subset L^1(\Omega)$ is \emph{uniformly integrable} (or \emph{equi-integrable}) if for all $\varepsilon > 0$, there exists $\delta > 0$ such that
  \begin{equation*}
    \P(A) < \delta \Rightarrow \sup_{f \in \mc{F}} \int_{A} |f(\omega)| \, \dd\P(\omega) < \varepsilon \qquad \forall A \in \mc{A}.
  \end{equation*}
  For a Banach space $X$, we say that an $X$-valued stochastic process $\mb{f}_{\bullet}$ is uniformly integrable if the set $\{\|\mb{f}_n\|_{X} : n \in \N\} \subset L^1(\Omega)$ is uniformly integrable. 
\end{defn}

A bounded subset $\mc{F} \subset L^1(\Omega)$ is uniformly integrable if and only if it is weakly relatively compact (see \cite[Theorem 5.2.9]{AK06}).
For $p \in (1,\infty)$, since $L^p(\Omega)$ is reflexive, every bounded subset $\mc{F} \subset L^p(\Omega)$ is weakly relatively compact (see Corollary \ref{cor:reflexive-iff-weakcpt} of the Banach--Alaoglu theorem).
In both cases, the Eberlein--Smulian theorem (Theorem \ref{thm:eberlein-smulian}) says that every bounded sequence in $L^p(\Omega)$ (with the additional assumption of uniform integrability if $p=1$) has a weakly convergent subsequence.
We use this to prove the following theorem.

\begin{thm}\label{thm:mgale-cv-repn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and fix $p \in [1,\infty)$.
  Let $f_{\bullet}$ be an $L^p$-bounded scalar-valued martingale with respect to a filtration $\mc{A}_{\bullet}$.
  If $p = 1$, suppose also that $\mb{f}_{\bullet}$ is uniformly integrable.
  Then there exists a function $f_\infty \in L^p(\Omega,\mc{A},\P)$ such that $f_n = \E^{\mc{A}_n} f_\infty$ for all $n \in \N$.
\end{thm}

Note that by Theorems \ref{thm:mgale-conv-Lp} and \ref{thm:mgale-pw-conv} we have $f_n \to f_\infty$ almost everywhere and in $L^p$.
In particular, the martingale $f_{\bullet}$ has an almost everywhere pointwise limit.

\begin{proof}
  By the discussion above, there is a subsequence $(f_{n_k})_{k \in \N}$ which converges weakly to a limit $f_\infty \in L^p(\Omega)$.
  For all $n \in \N$ and $A \in \mc{A}_n$,
  \begin{equation*}
    \int_A f_\infty \, \dd\P = \lim_{k \to \infty} \int_A f_{n_k} \, \dd\P,
  \end{equation*}
  and whenever $k$ is so large that $n_k \geq n$ we have
  \begin{equation*}
    \int_A f_{n_k} \, \dd\P = \int_A \E^{\mc{A}_n} f_{n_k} \, \dd \P = \int_A f_{n} \, \dd\P
  \end{equation*}
  by the martingale property.
  Thus for all $A \in \mc{A}_n$ we have
  \begin{equation*}
    \int_A f_\infty \, \dd\P = \int_A f_n,
  \end{equation*}
  which implies that $f_n = \E^{\mc{F}_n} f_\infty$.
\end{proof}

\begin{rmk}\label{rmk:mgale-cv-p1}
  When $p = 1$ the assumption of uniform integrability can be removed, and the following is true: every scalar-valued $L^1$-bounded martingale converges almost everywhere (but not necessarily in $L^1$).
  This is proven in \cite[Theorem 1.34]{gP16}, and the proof isn't particularly difficult, but we'll skip it so that we have more time to spend with Banach spaces.
\end{rmk}

Later on we are going to need a corresponding result for submartingales.

\begin{thm}\label{thm:submartingale-convergence}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $f_{\bullet}$ a real-valued submartingale on $\Omega$ which is $L^1$-bounded and uniformly integrable.
  Then $f_\bullet$ converges almost everywhere, and in $L^1$.
\end{thm}

\begin{proof}
  We will write $f_{n} = \tilde{f}_{n} + a_{n}$ as a sum of an $L^1$-bounded uniformly integrable martingale $\tilde{f}_{\bullet}$ and a monotone increasing `drift' process $a_{\bullet}$.\footnote{This is the \emph{Doob decomposition}.}
  By Theorem \ref{thm:mgale-cv-repn} to $\tilde{f}_{\bullet}$, this reduces us to showing that $a_{\bullet}$ has the desired convergence properties.

  Define $g_{n} := df_{n} - \E^{\mc{A}_{n-1}}(df_{n})$ for all $n \geq 1$ and $g_{0} := f_{0}$, and let $\tilde{f}_{\bullet}$ be the process with difference sequence $d\tilde{f}_{\bullet} = g_{\bullet}$.
  By construction $\E^{\mc{A}_{n-1}} g_{n} = 0$ for all $n \geq 1$, so $\tilde{f}_{\bullet}$ is a martingale.
  Now note that for all $N \geq 1$
  \begin{equation*}
    f_{N} = \sum_{n=0}^{N} df_{n} = \sum_{n=0}^{N} d\tilde{f}_{n} + \sum_{n=1}^{N} \E^{\mc{A}_{n-1}}(df_{n}),
  \end{equation*}
  so that $f_{\bullet} = \tilde{f}_{\bullet} + a_{\bullet}$, with
  \begin{equation*}
    a_{N} := \sum_{n=1}^{N} \E^{\mc{A}_{n-1}}(df_{n}). 
  \end{equation*}
  Since $f_{\bullet}$ is a submartingale we have $\E^{\mc{A}_{n-1}}(df_{n}) \geq 0$ for all $n \geq 1$, so $a_{\bullet}$ is non-decreasing.
  We have
  \begin{equation*}
    \|a_{N}\|_{L^1(\Omega)} = \E \sum_{n=1}^{N} \E^{\mc{A}_{n-1}}(df_{n}) = \sum_{n=1}^{N} \E(df_{n}) = \E(f_{N}) - E(f_{0})
  \end{equation*}
  by telescoping, thus the $L^1$-boundedness of $a_{\bullet}$ follows from that of $f_{\bullet}$.
  By monotonicity $a_{\bullet}$ converges almost surely and in $L^1$, and furthermore $a_{\bullet}$ is uniformly integrable.
  The $L^1$-boundedness and uniform integrability of $\tilde{f}_{\bullet} = f_{\bullet} - a_{\bullet}$ finally follows from that of $f_{\bullet}$ and $a_{\bullet}$.
  
\end{proof}

\section{Martingale convergence as a Banach space property}

With Theorem \ref{thm:mgale-cv-repn} as inspiration, we make the following definition.

\begin{defn}
  For $p \in [1,\infty]$, we say that a Banach space $X$ has the \emph{$p$-martingale convergence property} (or \emph{$p$-MCP}) if every $X$-valued $L^p$-bounded martingale (with the added assumption of uniform integrability when $p=1$) converges almost everywhere.
  We say that $X$ has the $p$-MCP with respect to a probability space $(\Omega,\mc{A},\P)$ if the previous property holds for every $X$-valued martingale on $(\Omega,\mc{A},\P)$.
\end{defn}

\begin{rmk}
  This is not a standard definition, because it turns out to be equivalent to the more familiar \emph{Radon--Nikodym property}, which we will discuss in the next chapter. But for now, it will help to give the property a more martingaley name.
\end{rmk}

Formally, the $1$-MCP is the strongest of these properties, and the $\infty$-MCP is the weakest.
As stated in the remark above, it will turn out that these properties are equivalent, but we don't know that yet.
For the moment we will investigate the $p$-MCP na\"ively, without invoking this equivalence.

\begin{prop}
  Let $1 \leq p < q \leq \infty$.
  If a Banach space $X$ has the $p$-MCP, then it also has the $q$-MCP.
\end{prop}

\begin{proof}
  For $p > 1$ this follows from the continuous inclusion $L^q(\Omega) \subset L^p(\Omega)$ for probability spaces $\Omega$: an $L^q$-bounded martingale is also $L^p$-bounded, and one can then invoke the $p$-MCP to derive the $q$-MCP.
  For $p = 1$ the same argument applies once we show that a bounded subset $\mc{F} \subset L^q(\Omega)$ is uniformly integrable.
  To see this, for all $f \in \mc{F}$ and measurable $A \subset \Omega$ use H\"older's inequality to estimate
  \begin{equation*}
    \int_{A} |f(\omega)| \, \dd\P(\omega) \leq \|f\|_{q} \P(A)^{1/q'}.
  \end{equation*}
  Thus for all $\varepsilon > 0$, if $\P(A) < (\varepsilon/\sup\{\|f\|_q : f \in \mc{F}\})^{q'}$ then
  \begin{equation*}
    \int_{A} |f(\omega)| \, \dd\P(\omega) < \varepsilon,
  \end{equation*}
  so $\mc{F}$ is uniformly integrable.
\end{proof}

Theorem \ref{thm:mgale-cv-repn} says that the scalar field $\K$ has the $1$-MCP, and arguing coordinatewise shows that every finite dimensioal Banach space also has this property.
In the following two examples we will show that the Banach spaces $c_0$ and $L^1(\Omega)$ do not have the $\infty$-MCP (and hence do not have the $p$-MCP for any $p \in [1,\infty]$). 

\begin{example}\label{eg:c0-noMCP}
  Consider the product space $\Omega = \{-1,1\}^{\N}$ with its usual product probability measure, and let $\map{\pi_n}{\Omega}{\{-1,1\}}$ be the $n$-th coordinate function.
  Consider the Banach space $c_0$ of scalar-valued sequences $(a_n)_{n \in \N}$ satisfying $\lim_{n \to \infty} a_n = 0$, equipped with the $\ell^\infty$-norm.
  Let $(\mb{e}_n)_{n \in \N}$ be the canonical basis of $c_0$, i.e.
  \begin{equation*}
    \mb{e}_n(m) = \begin{cases} 1 & m=n \\ 0 & m \neq n. \end{cases}
  \end{equation*}
  Define a $c_0$-valued martingale $\mb{f}_\bullet$ with respect to the filtration generated by $\pi_{\bullet}$ by
  \begin{equation*}
    \mb{f}_n := \sum_{k = 0}^{n} \pi_k \otimes \mb{e}_k.
  \end{equation*} 
  (As shown in Example \ref{eg:betting-game-martingale}, this is a martingale; in fact, it is just a special case of our betting game, in which one wagers the vector $\mb{e}_n$ at time $n$, regardless of the previous outcomes.)
  Then we have
  \begin{equation*}
    \|\mb{f}_n\|_{L^\infty(\Omega;c_0)} = \esssup_{\omega \in \Omega} \|(\pi_k(\omega))_{k = 0}^n\|_{c_0} = 1,
  \end{equation*}
  so the martingale $\mb{f}_{\bullet}$ is $L^\infty$-bounded.
  But for all $\omega$ in $\Omega$ and all $n < m$, we have
  \begin{equation*}
    \|\mb{f}_n(\omega) - \mb{f}_m(\omega)\|_{c_0} = \max_{n < k \leq m} |\pi_k(\omega)| = 1,
  \end{equation*}
  so the sequence $(\mb{f}_n(\omega))_{n \in \N}$ is not Cauchy in $c_0$ and hence not convergent.
  Thus $\mb{f}_{\bullet}$ cannot converge anywhere.
\end{example}

\begin{example}\label{eg:L1-noMCP}
  With the notation of the previous example, let $X = L^1(\Omega)$ and consider the $X$-valued martingale
  \begin{equation}\label{eq:L1-example-mgale}
    \mb{g}_n(\omega) := \prod_{k \leq n} (1 + \pi_{k}(\omega)\pi_k)
  \end{equation}
  (Exercise \ref{ex:mgale-check} asks you to show that this is indeed a martingale).
  For all $\omega \in \Omega$ and $n \in \N$ we have
  \begin{equation*}
    \begin{aligned}
      \|\mb{g}_n(\omega)\|_{X} &= \int_{\Omega} \prod_{k \leq n} (1 + \pi_k(\omega)\pi_k(\eta))  \, \dd\P(\eta) \\
      &= \prod_{k \leq n} \int_{\Omega}  (1 + \pi_k(\omega)\pi_k(\eta)) \, \dd\P(\eta)
      = 1
    \end{aligned}
  \end{equation*}
  by mutual independence of the random variables $\pi_k$, so the martingale $\mb{g}_{\bullet}$ is $L^\infty$-bounded.
  However, we also have
  \begin{equation*}
    \begin{aligned}
      &\|\mb{g}_{n}(\omega) - \mb{g}_{n+1}(\omega)\|_{X} \\
      &= \int_{\Omega} \Big| \Big(1 - (1 + \pi_{n+1}(\omega)\pi_{n+1}(\eta))\Big)\prod_{k \leq n} (1 + \pi_k(\omega)\pi_k(\eta)) \Big|\, \dd\P(\eta) \\
      &= \int_{\Omega} |\pi_{n+1}(\omega) \pi_{n+1}(\eta)| \, \dd\P(\eta) \prod_{k \leq n} \int_{\Omega} 1 + \pi_k(\omega)\pi_k(\eta) \, \dd\P(\eta) = 1,
    \end{aligned}
  \end{equation*}
  which shows that the sequence $(\mb{g}_n(\omega))_{n \in \N}$ cannot be Cauchy in $X$.
  Hence $\mb{g}_{\bullet}$ is not convergent anywhere.
\end{example}

It is not surprising that the Banach spaces $c_0$ and $L^1(\Omega)$ fail the $\infty$-MCP, as these are classically `bad' spaces.
Most spaces that arise in practice are better behaved.

\begin{thm}\label{thm:MCP-sepdual}
  If $X$ is a separable dual space (i.e. $X$ is separable and $X = Y^*$ for some Banach space $Y$), then $X$ has the $\infty$-MCP.
\end{thm}

\begin{proof}
  Let $\mb{f}_{\bullet}$ be an $X$-valued $L^\infty$-bounded martingale on a probability space $\Omega$.
  By homogeneity we may assume without loss of generality that each $\mb{f}_n$ is valued in the closed unit ball of $X$, which by Banach--Alaoglu is weak-* compact.
  For each $\omega \in \Omega$, let $\mb{f}(\omega)$ be a weak-* limit point of the sequence $(\mb{f}_n(\omega))_{n \in \N}$.
  
  Since $X = Y^*$ is separable, so is $Y$, and we can choose a countable dense subset $D \subset \overline{B_{Y}}$ of the unit ball of $Y$.
  For each $\mb{y} \in D$, the scalar-valued martingale $(\langle \mb{y}, \mb{f}_n \rangle)_{n \in \N}$ is $L^\infty$-bounded and thus converges almost everywhere (Theorem \ref{thm:mgale-cv-repn}), and the limit must be $\langle \mb{y}, \mb{f} \rangle$.
  For each $n \in \N$ and $\mb{y} \in D$ let $N_{\mb{y}} \subset \Omega$ denote the null set on which this convergence fails, let $N = \bigcup_{\mb{y} \in D} N_{\mb{y}}$ so that $\P(N) = 0$ (since $D$ is countable), and observe that
  \begin{equation*}
    \langle \mb{y}, \mb{f}_n(\omega) \rangle \to \langle \mb{y}, \mb{f}(\omega) \rangle \qquad \forall \omega \in \Omega \sm N \quad \forall \mb{y} \in D.
  \end{equation*}
  Since $D$ is dense in the unit ball of $Y$ we have this convergence (away from $N$) for all $\mb{y} \in Y$, and since the closed unit ball of $Y$ is weak-* dense in that of $Y^{**}$ (see Theorem \ref{thm:goldstine} in the appendices) we have
    \begin{equation*}
      \langle \mb{f}_n(\omega), \mb{y}^{**} \rangle \to \langle \mb{f}(\omega), \mb{y}^{**} \rangle \qquad \forall \omega \in \Omega \sm N \quad \forall \mb{y}^{**} \in Y^{**} = X^{*}.
  \end{equation*}
  Since the functions $\mb{f}_n$ are all $\mc{A}_{\infty}$-measurable, this shows that $\map{\mb{f}}{\Omega \sm N}{X}$ is weakly $\mc{A}_{\infty}$-measurable, and since $X$ is separable, Pettis tells us that $\mb{f}$ is strongly $\mc{A}_{\infty}$-measurable.

  It remains to show that $\mb{f}_n \to \mb{f}$ on $\Omega \sm N$ in the norm topology on $X$.
  For all $\omega \in \Omega \sm N$ and $\mb{x} \in X$ we have, using that for each $\mb{y} \in D$ the scalar-valued martingale $\langle \mb{y}, \mb{x} - \mb{f}_\bullet \rangle$ converges to $\langle \mb{y}, \mb{x} - \mb{f}\rangle$ on $\Omega \sm N$, 
  \begin{equation*}
    \|\mb{x} - \mb{f}_n(\omega)\|_{X}
    = \sup_{\mb{y} \in D} |\langle \mb{y}, \mb{x} - \mb{f}_n(\omega) \rangle| 
    = \sup_{\mb{y} \in D} |\E^{\mc{A}_n} (\langle \mb{y}, \mb{x} - \mb{f} \rangle)(\omega) | 
    \leq \E^{\mc{A}_n}( \|\mb{x} - \mb{f}\|_{X})(\omega)
  \end{equation*}
  using \eqref{eq:positive-pw-est}.
  Since the function $\|\mb{x} - \mb{f}\|_{X}$ is bounded and $\mc{A}_{\infty}$-measurable, we have
  \begin{equation*}
    \limsup_{n \to \infty} \|\mb{x} - \mb{f}_n(\omega)\|_{X} \leq \limsup_{n \to \infty} \E^{\mc{A}_n}( \|\mb{x} - \mb{f}\|_{X})(\omega) = \|\mb{x} - \mb{f}(\omega)\|_{X}.
  \end{equation*}
  Now taking $\mb{x} = \mb{f}(\omega)$ shows that $\mb{f}_n(\omega) \to \mb{f}(\omega)$ in $X$.
  The proof is complete.  
\end{proof}

Many useful Banach spaces are separable duals, and thus have the $\infty$-MCP: this includes the spaces $L^p([0,1])$ for $p \in (1,\infty)$ as well as the sequence space $\ell^1$ (which is the dual of $c_0$).
The sequence space $\ell^\infty$ contains $c_0$, which does not have $p$-MCP for any $p$ as shown above, so $\ell^\infty$ doesn't have any of these properties either.
Of course, $\ell^\infty$ is a dual space, but it isn't separable.
Separability is not necessary for the $p$-MCP; rather, for a Banach space $X$ to have the $p$-MCP, it is sufficient to inspect the separable closed subspaces of $X$ individually.

\begin{lem}\label{lem:MCP-sepdet}
  For all $p \in [1,\infty]$, the $p$-MCP is separably determined: that is, a Banach space $X$ has the $p$-MCP if and only if every separable closed subspace $Y \subset X$ has the $p$-MCP.
\end{lem}

\begin{proof}
  The `only if' direction is immediate, as a $Y$-valued martingale can be seen as an $X$-valued martingale, and if a martingale converges a.s. in $X$, then since $Y$ is closed it must also converge a.s. in $Y$.

  On the other hand, suppose that every separable closed subspace $Y \subset X$ has the $p$-MCP, and let $\mb{f}_{\bullet}$ be an $X$-valued $L^p$-bounded martingale.
  Each $\mb{f}_n$ is strongly measurable and hence separably-valued by the Pettis theorem (Theorem \ref{thm:Pettis-measurability}), so there is a sequence of separable closed subspaces $Y_n \subset X$ such that $\mb{f}_n$ takes values in $Y_n$.
  The (countable!) union of these subspaces generates a separable closed subspace $Y$.
  The martingale $\mb{f}_{\bullet}$ then takes values in $Y$, and since $Y$ has the $p$-MCP by assumption, $\mb{f}_{\bullet}$ is almost everywhere convergent.
\end{proof}

\begin{cor}\label{cor:MCP-reflexive}
  If $X$ is reflexive, then $X$ has the $\infty$-MCP.
\end{cor}

\begin{proof}
  By the previous lemma, it suffices to show that every separable closed subspace $Y \subset X$ has the $\infty$-MCP, and by Theorem \ref{thm:MCP-sepdual} we just need to show that every such $Y$ is a dual space.
  
  Consider the annihilator
  \begin{equation*}
    Y^{\perp} := \{\mb{x}^* \in X^* :  \text{$\langle \mb{y}, \mb{x}^* \rangle = 0$ for all $y \in Y$}\},
  \end{equation*}
  and the double annihilator
  \begin{equation*}
    Y^{\perp\perp} = (Y^\perp)^\perp = \{\mb{x}^{**} \in X^{**} : \text{$\langle \mb{z}, \mb{x}^{**} \rangle = 0$ for all $\mb{z} \in Y^\perp$}\}.
  \end{equation*}
  Then $Y^{\perp\perp}$ is isometrically isomorphic to the dual space $(X^*/Y^\perp)^{*}$ (see Proposition \ref{prop:duality-subspace-quotient} in the appendices), so it suffices to show that $j(Y) = Y^{\perp\perp}$, where $\map{j}{X}{X^{**}}$ is the canonical inclusion.
  The containment $j(Y) \subset Y^{\perp\perp}$ is a direct consequence of the definition.
  To show the reverse inclusion, suppose that $\mb{x}^{**} \notin j(Y)$; we will conclude that $\mb{x}^{**} \notin Y^{\perp\perp}$.
  To do this we need to find a functional $\mb{x}^* \in Y^{\perp}$ such that $\langle\mb{x}^*, \mb{x}^{**}\rangle \neq 0$.
  Since $X$ is reflexive, $\mb{x}^{**} = j(\mb{x})$ for some $\mb{x} \in X \sm Y$.
  By Hahn--Banach there exists a functional $\mb{x}^* \in X^*$ such that $\langle \mb{x}, \mb{x}^* \rangle = 1$ and $\mb{x}^* \in Y^\perp$.
  Since $\langle \mb{x}, \mb{x}^* \rangle = \langle \mb{x}^*, j(\mb{x}) \rangle = \langle \mb{x}^*, \mb{x}^{**} \rangle$, the functional $\mb{x}^*$ does exactly what we want.
\end{proof}

Thus reflexive spaces have the $\infty$-MCP, even if they are not separable (for example, the Hilbert space $\ell^2(\Lambda)$ over an uncountable set $\Lambda$ with counting measure).


\section*{Exercises}

\begin{exercise}
  Let $\mb{f}_{\bullet}$ be a stochastic process on a probability space $(\Omega,\mc{A},\P)$.
  Suppose that $\mb{f}_{\bullet}$ is predictable with respect to the filtration that it generates (see Example \ref{eg:filtration-generated-by-process}).
  Show that the process is deterministic, in the sense that each $\mb{f}_n$ is constant.
\end{exercise}

\begin{exercise}\label{ex:winnings-unpredictability}
  In the setting of Example \ref{eg:gambling-filtrations}, show that the random variable $\mb{s}_{n+1}$ is $\mc{F}_n$-measurable if and only if $\mb{x}_{n+1} \equiv 0$.
\end{exercise}

\begin{exercise}\label{ex:gambling-in-linfty}
  This exercise takes place in the setting of Example \ref{eg:gambling-stoppingtimes}.
  \begin{itemize}
  \item
    Let $X = \ell^\infty(\N)$.
    Suppose that the wager vectors $\map{\mb{x}_n}{\Omega}{\ell^\infty(\N)}$ are such that for all $\omega \in \Omega$, the vectors $(\mb{x}_n(\omega))_{n \in \N}$ are pairwise distinct standard basis vectors (i.e. $\{0,1\}$-valued sequences, zero for all but one index).
    Fix $\lambda > 0$ and let $K = \{\mb{a} \in \ell^\infty(\N) : \|\mb{a}\|_\infty \geq \lambda\} = \ell^\infty(\N) \sm B_\lambda(0)$.
    Show that the stopping time
    \begin{equation*}
      T_K(\omega) := \inf\{n \in \N : \mb{s}_n(\omega) \in K\} 
    \end{equation*}
    is finite if and only if $\lambda \leq 1$.
  \item
    As above, but now let $X = \ell^2(\N)$, and show that the stopping time $T_K$ is finite for all $\lambda > 0$.
  \end{itemize}
\end{exercise}

\begin{exercise}\label{ex:CE-monotonicity}
  Let $(\Omega,\mc{A},\P)$ is a probability space, $X$ a Banach space, and $\mb{f} \in L^1(\Omega;X)$.
  Let $\mc{B}$ and $\mc{B}'$ be $\sigma$-subalgebras of $\mc{A}$ with $\mc{B}' \subset \mc{B}$.
  Show that
  \begin{equation*}
    \E^{\mc{B}'} \E^{\mc{B}} \mb{f} = \E^{\mc{B}'} \mb{f}
  \end{equation*}
  in two ways: first by using the defining property of conditional expectations, and then by using its construction as the adjoint of an inclusion map.
\end{exercise}

\begin{exercise}\label{ex:CE-adjoint}
  Use the defining property \eqref{eq:conditional-expectation-property} of conditional expectations (i.e. do not use details of its construction) to prove Proposition \ref{prop:CE-adjoint}.
\end{exercise}

\begin{exercise}\label{ex:conditional-expectation-as-adjoint}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$.
  Using that $L^1(\mc{A}) \subsetneq L^\infty(\mc{A})^*$, show that the adjoint of the inclusion map $\map{\iota}{L^\infty(\mc{B})}{L^\infty(\mc{A})}$, which \emph{a priori} maps $L^1(\mc{A}) \to L^\infty(\mc{B})^* \supsetneq L^1(\mc{B})$, actually maps into $L^1(\mc{B})$ \emph{without invoking the existence of a conditional expectation operator on $L^1$.}
\end{exercise}

\begin{exercise}\label{ex:CE-measurable-op}
  Prove Proposition \ref{prop:CE-measurable-op}.
\end{exercise}

\begin{exercise}\label{ex:ce-ind}
  Let $(\Omega,\mc{A},\P)$ be a probability space, $X$ a Banach space, and $\mb{f} \in L^1(\Omega;X)$ an integrable random variable.
  Let $\mc{B}$ be a $\sigma$-subalgebra of $\mc{A}$ which is independent of the $\sigma$-subalgebra $\sigma(\mb{f})$.
  Show that $\E^{\mc{B}} \mb{f} \aeeq \E \mb{f}.$
\end{exercise}

\begin{exercise}\label{ex:martingale-elementary-properties}
  Let $(\Omega,\mc{A},\P)$ be a probability space with a filtration $\mc{A}_{\bullet}$, $X$ a Banach space, and let $\mb{f}_{\bullet}$ be an $X$-valued stochastic process with $\mb{f}_{n} \in L^1(\Omega;X)$ for all $n \in \N$.
  \begin{itemize}
  \item
    Show that $\mb{f}_{\bullet}$ is a martingale if and only if $\mb{f}_n = \E^{\mc{A}_n} \mb{f}_m$ for all $n,m \in \N$ with $m > n$.
  \item
    Show that $\mb{f}_{\bullet}$ is a martingale if and only if the difference sequence $d\mb{f}_{n}$ is adapted to $\mc{A}_{\bullet}$ and $\E^{\mc{A}_{n-1}} (d\mb{f}_{n}) = 0$ for all $n \geq 1$.
    Conclude that an $X$-valued stochastic process $\mb{g}_{\bullet}$ is the difference sequence of a martingale if and only if it is $\mc{A}_{\bullet}$-adapted, $\mb{g}_{n} \in L^1(\Omega;X)$ for all $n \in \N$, and $\E^{\mc{A}_{n-1}}(\mb{g}_{n}) = 0$ for all $n \geq 1$.
  \item
    If $\mb{f}_{\bullet}$ is a martingale and $p \in [1,\infty]$, show that $\|\mb{f}_{n}\|_{L^p(\Omega;X)}$ is monotonically increasing in $n$.
  \end{itemize}
\end{exercise}

\begin{exercise}\label{ex:mgale-conv}
  Prove the pointwise convergence theorem for martingales, Theorem \ref{thm:mgale-pw-conv}, as a consequence of Doob's maximal inequality and the $L^p$-convergence theorem (Theorem \ref{thm:mgale-conv-Lp}).
\end{exercise}

\begin{exercise}\label{ex:UI-characterisation}
  Let $(\Omega,\mc{A},\P)$ be a probability space.
  Show that a bounded subset $\mc{F} \subset L^1(\Omega)$ is uniformly integrable if and only if
  \begin{equation*}
    \lim_{t \to \infty} \sup_{f \in \mc{F}} \int_{\{|f| > t\}} |f(\omega)| \, \dd\P(\omega) = 0.
  \end{equation*}
\end{exercise}

\begin{exercise}\label{ex:mgale-check}
  Show that the $L^1(\Omega)$-valued stochastic process defined in \eqref{eq:L1-example-mgale} is a martingale. 
\end{exercise}

\begin{exercise}\label{ex:L1-noMCP-var}
  Modify Example \ref{eg:L1-noMCP} to show that $L^1([0,1])$ does not have the $\infty$-martingale convergence property. (Do not simply use that $L^1([0,1])$ is isometrically isomorphic to $L^1(\Omega)$---construct a `bad' martingale directly.)
\end{exercise}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
