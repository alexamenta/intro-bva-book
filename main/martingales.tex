
\todo{Write expository introduction}

\subsection{Gambling in Banach spaces}\label{sec:gambling}

To motivate the theory in this section, we're going to imagine a betting game.
At each turn, you bet on the outcome of a coin toss.
The quantities that you can bet are taken from a Banach space $X$.
The initial state of your wallet, $\mb{s}_{-1}$, is the zero vector
\begin{equation*}
  \mb{s}_{-1} = 0 \in X.
\end{equation*}
At each time $n \in \N = \{0,1,\ldots\}$, you choose a vector $\mb{x}_n \in X$ to wager.
I then flip a fair coin, which shows either Heads or Tails, and the state of your wallet becomes
\begin{equation*}
  \mb{s}_n =
  \begin{cases}
    \mb{s}_{n-1} + \mb{x}_n & \text{if the coin shows Heads} \\
    \mb{s}_{n-1} - \mb{x}_n & \text{if the coin shows Tails.}
  \end{cases}
\end{equation*}
The Banach space $X$ is not ordered, so there is no canonical notion of $\mb{s}_n$ being `more' or `less' than $\mb{s}_{n-1}$. Thus the game is not about winning or losing (the true winner of the game is Functional Analysis).
In this chapter we will discuss various probabilistic concepts that can be well-understood in the context of this game.


\subsection{Filtrations and stochastic processes}

\begin{defn}
  A \emph{filtration} on a probability space $(\Omega,\mc{A},\P)$ is a monotone increasing (i.e. nondecreasing) sequence of $\sigma$-subalgebras
  \begin{equation*}
    \mc{A}_0 \subseteq \mc{A}_1 \subseteq \mc{A}_2 \subseteq \cdots \subseteq \mc{A}.
  \end{equation*}
\end{defn}

\begin{example}\label{eg:dyadic-filtration}
  Consider the unit interval $[0,1) \subset \R$ with Borel $\sigma$-algebra and Lebesgue measure.
  For each $n \in \N$, let $\mc{F}_n$ be the $\sigma$-algebra generated by the \emph{dyadic intervals of length $2^{-n}$}, i.e. intervals of the form
  \begin{equation*}
    [2^{-n}k, 2^{-n}(k+1)) \qquad k = 0, 1, 2, \ldots, 2^{n}- 1.
  \end{equation*}
  Then $(\mc{F}_n)_{n \in \N}$ is a filtration, which we call the \emph{(standard) dyadic filtration}.
  A Banach-valued function $\map{f}{[0,1)}{X}$ is $\mc{F}_n$ measurable if and only if it is constant on each dyadic interval of length $2^{-n}$.  
\end{example}

\begin{example}\label{eg:coordinate-filtration}
  Let $\{-1,1\}$ be a two-point space with uniform probability measure, and consider the infinite product
  \begin{equation*}
    \Omega := \prod_{n \in \N} \{-1,1\} = \{-1,1\}^\N,
  \end{equation*}
  which (equipped with the product $\sigma$-algebra and measure) is a probability space.
  Elements of $\Omega$ are sequences $\omega = (\omega_n)_{n \in \N}$ where each $\omega_n = \pm 1$.
  Suppose $n \in \N$, fix a vector $\eta = (\eta_0, \eta_1, \ldots \eta_n) \in \{-1,1\}$ of length $n+1$, and define the set
  \begin{equation*}
    A_{\nu} := \{\omega \in \Omega : \omega_k = \eta_k \; \forall k = 0,1,\ldots,n\};
  \end{equation*}
  that is, a point $\omega \in \Omega$ belongs to $A_{\nu}$ if its first $n+1$ components are given by $\nu$.
  For each $n \in \N$, let $\mc{F}_n$ be the $\sigma$-algebra generated by all sets $A_{\nu}$ with $\nu \in \{-1,1\}^n$.
  Then $(\mc{F}_n)_{n \in \N}$ is a filtration, which we call the \emph{coordinate filtration}.
  A Banach-valued function $\map{f}{\Omega}{X}$ is $\mc{F}_n$-measurable if and only if $f(\omega) = f(\omega_0,\ldots,\omega_n)$ only depends on the first $n+1$ coordinates of its input variable.
\end{example}

\begin{rmk}
  Example \ref{eg:coordinate-filtration} encodes the same information as Example \ref{eg:dyadic-filtration}.
  Each dyadic interval $I \subset [0,1]$ has exactly two dyadic subintervals, and every vector $\nu \in \{-1,1\}^n$ can be extended in exactly two ways to a vector in $\{-1,1\}^{n+1}$.
  Equivalently, each infinite sequence $\omega \in \{0,1\}^\N$ corresponds to the binary expansion of a number $t \in [0,1)$, and this correspondence is bijective up to a measure zero subset (corresponding to the negligible non-uniqueness of binary expansions).
  The set of sequences $\omega' \in \{0,1\}^\N$ whose first $n+1$ entries coincide with those of $\omega$ then corresponds to the set $A_{(\omega_0,\ldots,\omega_n)}$, which corresponds to the unique dyadic interval of length $2^{-(n+1)}$ containing $t$. 
\end{rmk}

Filtrations are closely linked with stochastic processes.
While we don't plan on saying anything really serious about these in this course, it will be useful to keep the core concept in mind, as it guides a lot of probabilistic intuition.

\begin{defn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  A \emph{(discrete-time) $X$-valued stochastic process} on $(\Omega,\mc{A},\P)$ is a sequence of $\mc{A}$-measurable random variables $\map{f_n}{\Omega}{X}$, $n \in \N$.
  Given a filtration $(\mc{A}_n)_{n \in \N}$, a stochastic process $(f_n)_{n \in \N}$ is called \emph{predictable} (with respect to the filtration) if each $f_n$ is $\mc{A}_{n-1}$-measurable (with the convention that $\mc{A}_{-1} = \{\varnothing, \Omega\}$).
\end{defn}

\begin{rmk}
  With obvious modifications one can talk about filtrations and stochastic processes starting at an arbitrary index, finite filtrations/processes, or filtrations/processes with respect to arbitrary (total or partial) orders, for example with a continuous time index.
  In this course we will only consider discrete indexing sets contained in $\N$.
\end{rmk}

One should think of a filtration $(\mc{A}_n)_{n=0}^\infty$ as representing the progression of available information over time, usually in relation to a stochastic process.
Each $\sigma$-subalgebra $\mc{A}_n \subset \mc{A}$ represents the information available at time $n$.
There are two equivalent ways of thinking about the availability of information: one is that at time $n$ one has access to all $\mc{A}_n$-measurable subsets; the other is that at time $n$ one has access to all $\mc{A}_n$-measurable functions.
The monotonicity assumption says that no information is lost as time progresses.
Predictability of a stochastic process $(f_n)_{n \in \N}$ with respect to the filtration $(\mc{A}_n)_{n \in \N}$ thus says the following: if the available information is represented by $(\mc{A}_n)_{n \in \N}$, then at each time $n$, one already has access to the $\mc{A}_n$-measurable function $f_{n+1}$.

\begin{example}\label{eg:filtration-generated-by-process}
  Let $(\Omega,\mc{A},\P)$ be a probability space, $X$ a Banach space, and let $(f_n)_{n \in \N}$ be an $X$-valued stochastic process on $(\Omega,\mc{A},\P)$.
  The \emph{filtration generated by the process $(f_n)_{n \in \N}$} is given by
  \begin{equation*}
    \mc{F}_n := \sigma(f_0,f_1,\ldots,f_n) \qquad \forall n \in \N.
  \end{equation*}
  The information-theoretic intuition says that at time $n \in \N$, one `knows' the functions $f_0, f_1, \ldots f_n$, as these are in $\mc{F}_n$, and one also knows all functions of the form
  \begin{equation*}
    g \circ (f_0, f_1, \ldots, f_n) \colon \omega \mapsto g(f_0(\omega),f_1(\omega),\ldots,f_n(\omega))
  \end{equation*}
  where $\map{g}{X^{n+1}}{\C}$ is measurable (as such compositions are automatically measurable).
  In fact, all $\mc{F}_n$-measurable functions $\Omega \to \C$ are of this form.\todo{cite a reference}
\end{example}

\begin{example}\label{eg:gambling-filtrations}
  Consider the game we introduced in Section \ref{sec:gambling}.
  At each time $n \in \N$ I flip a fair coin, which comes up Heads ($H$) or Tails $(T)$ with equal probability.
  The natural probability space on which to base this game is the infinite product $\Omega = \{-1,+1\}^{\N}$ (see Example \ref{eg:coordinate-filtration}).
  The value $-1$ represents Tails, while $+1$ represents Heads.
  For each $n \in \N$ let $\map{\pi_n}{\Omega}{\{-1,+1\}}$ be the $n$-th coordinate function, which represents the outcome of the $n$-th coin toss.
  The sequence $(\pi_n)_{n \in \N}$ is a scalar-valued stochastic process, and the filtration it generated is precisely the coordinate filtration discussed in Example \ref{eg:coordinate-filtration}.

  
  Your bet at time $n$, the vector $\mb{x}_n \in X$, is allowed to depend on the outcomes $\pi_0, \pi_1, \ldots, \pi_{n-1}$: you do not need to register all your bets in advance.
  In probabilistic language, $\map{\mb{x}_n}{\Omega}{X}$ is $\mc{F}_{n-1}$-measurable, i.e. the sequence $(\mb{x}_n)_{n \in \N}$ is a stochastic process which is predictable with respect to the filtration $(\mc{F}_n)_{n \in \N}$.

  Now consider the stochastic process $(\mb{s}_n)_{n \in \N}$, representing the evolution of the state of your wallet.
  By definition we have
  \begin{equation*}
    \mb{s}_{n+1} = \mb{s}_n + \pi_{n+1} \mb{x}_{n+1} \qquad \forall n \in \N;
  \end{equation*}
  keep in mind that this is an equality of $X$-valued random variables, i.e. functions $\Omega \to X$.
  Since $\mb{s}_n$, $\pi_{n+1}$, and $\mb{x}_{n+1}$ are all $\mc{F}_{n+1}$-measurable, we find that $\mb{s}_{n+1}$ is $\mc{F}_{n+1}$-measurable (i.e. we know the state of our wallet $\mb{s}_{n+1}$ at time $n+1$).
  Heuristically, $\mb{s}_{n+1}$ should not be $\mc{F}_n$-measurable unless $\mb{x}_{n+1} \equiv 0$, as this would amount to predicting the future (which can only be done by wagering nothing).
  You should prove this rigourously (Exercise \ref{ex:winnings-unpredictability}).
\end{example}


\begin{defn}
  Given a filtration $(\mc{A}_n)_{n=0}^\infty$ on a probability space $(\Omega, \mc{A}, \P)$, a random variable $\map{T}{\Omega}{\N \cup \{\infty\}}$ is called a \emph{stopping time} (with respect to $(\mc{A}_n)$) if 
  \begin{equation*}
    \{\omega \in \Omega: T(\omega) \leq n\} \in \mc{A}_n \qquad \forall n \geq 0.
  \end{equation*}
  The stopping time $T$ is \emph{finite} if $T$ is almost surely finite.
\end{defn}

Generally stopping times $T$ are defined in terms of some kind of stochastic \emph{stopping condition}.
Interpreting the filtration $(\mc{A}_n)_{n \in \N}$ as modelling the available information at time $n$, $T$ being a stopping time says precisely that at time $n$, one `knows' the set of points $\omega \in \Omega$ for which $T(\omega) \leq n$.
Said less precisely, if $T$ is a stopping time, then at time $n$, one can determine whether or not $T \leq n$.

\begin{example}\label{eg:gambling-stoppingtimes}
  We return to the betting game of Section \ref{sec:gambling}, elaborated upon in Example \ref{eg:gambling-filtrations}.
  Let's suppose that our goal is to get the state of our wallet $\mb{s} \in X$ into a fixed Borel measurable set $K \subset X$, and that we intend to stop betting once this condition holds (i.e. from that point on we only wager the zero vector).
  
  Let
  \begin{equation*}
    T_K(\omega) := \inf\{n \in \N : \mb{s}_n(\omega) \in K \}
  \end{equation*}
  with the usual convention that $T_K(\omega) = \infty$ if $\mb{s}_n(\omega) \notin K$ for all $n \in \N$.
  That is, $T_K$ is the first time $n$ at which $\mb{s}_n \in K$.
  At time $n$ we heuristically know whether or not our wallet satisfied $\mb{s}_m \in K$ for some $m \leq n$, which indicates that $T_K$ should be a stopping time with respect to the filtration $(\mc{F}_n)_{n \in \N}$ associated with the stochastic process $(\pi_n)_{n \in \N}$.
  Rigourously, one shows this by writing for all $n \in \N$
  \begin{equation*}
    \begin{aligned}
      \{\omega \in \Omega : T_K(\omega) \leq n\}
      &= \big\{\omega : \inf\{m : \mb{s}_m(\omega) \in K\} \leq n\big\} \\
      &= \{\omega  : \text{$\mb{s}_m(\omega) \in K$ for some $m \leq n$}\} \\
      &= \bigcup_{m = 0}^n \mb{s}_m^{-1}(K),
    \end{aligned}
  \end{equation*}
  and noting that since each $\mb{s}_m$ is $\mc{F}_m$-measurable, the set above is $\mc{F}_n$-measurable.
  Thus $T_K$ is a stopping time.
  Of course, whether $T_K$ is a finite stopping time depends on the set $K \subset X$, the wager vectors $(\mb{x}_n)_{n \in \N}$, and potentially even the geometry of $X$ (see Exercise \ref{ex:gambling-in-linfty}).
\end{example}

The proof of the following proposition was already done in the previous exercise for the a particular stochastic process, but the proof is identical for a general stochastic process.

\begin{prop}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space.
  Let $(f_n)_{n \in \N}$ be an $X$-valued stochastic process and $K \subset X$ a Borel measurable set.
  Then the function $\map{T_K}{\Omega}{\N \cup \{\infty\}}$ defined by
  \begin{equation*}
    T_K(\omega) := \inf\{n \in \N : f_n(\omega) \in K \}
  \end{equation*}
  is a stopping time.
\end{prop}

The stopping time $T_K$ defined above is called the \emph{first hitting time of $K$}.

\subsection{Conditional expectations}

\todo{a bit of exposition; include defn of $\E$ and talk about `best approximation with given information'}

\begin{defn}\label{defn:conditional-expectation}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space.
  Let $f \in L^1(\Omega, \mc{A}; X)$ be a integrable $X$-valued random variable.
  Given a $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a \emph{conditional expectation of $f$ given $\mc{B}$} is a $\mc{B}$-measurable random variable $\E^{\mc{B}}f \in L^1(\Omega, \mc{B}; X) \subset L^1(\Omega,\mc{A};X)$ such that
  \begin{equation}\label{eq:conditional-expectation-property}
    \qquad \int_B \E^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P \qquad \text{for all $B \in \mc{B}$.}
  \end{equation}
\end{defn}

\begin{example}\label{eg:atomic-CE}
  Let $(\Omega,\mc{A},\P)$ be a probability space and let $\mc{B} \subset \mc{A}$ be a sub-$\sigma$-algebra which is \emph{atomic}, in the sense that there is a collection of pairwise disjoint subsets $(B_\lambda)_{\lambda \in \Lambda}$ of $\mc{B}$ which generate $\mc{B}$, such that $\P(B_{\lambda}) > 0$ for all $\lambda$, and such that if $B_\lambda$ can be written as a disjoint union $B_\lambda = C \cup D$ for some sets $C,D \in \mc{B}$, then $\P(C) = 0$ or $\P(D) = 0$ (i.e. the sets $B_\lambda$ are \emph{atoms}).
  Let's compute \emph{the} conditional expectation $\E^{\mc{B}}f$ of an integrable random variable $f \in L^1(\mc{A};X)$ (it turns out there is only one).
  Since the atoms $(B_\lambda)_{\lambda}$ generate $\mc{B}$ and are pairwise disjoint, and since $\E^{\mc{B}}f$ is $\mc{B}$-measurable, $\E^{\mc{B}}f$ must be constant on each $B_\lambda$, so that
  \begin{equation*}
    \E^{\mc{B}} f = \sum_{\lambda \in \Lambda} \1_{B_\lambda} \otimes \mb{x}_\lambda
  \end{equation*}
  for some vectors $\mb{x}_{\lambda} \in X$.
  Averaging over one of the atoms $B_{\lambda}$ and using \eqref{eq:conditional-expectation-property} tells us that
  \begin{equation*}
    \begin{aligned}
      \mb{x}_\lambda = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} \E^{\mc{B}} f \, \dd\P = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} f \, \dd\P.
    \end{aligned}
  \end{equation*}
  In probabilistic terms, the quantity on the right hand side is the conditional expectation of $f$ given $B_\lambda$, which exists since $\P(B_\lambda) > 0$.
\end{example}

The previous example shows that conditional expectations with respect to atomic $\sigma$-algebras exist and are unique.
The same is true for general $\sigma$-algebras, but proving this will take a few steps.
First we establish the uniqueness of conditional expectations.

\begin{prop}\label{prop:CE-uniqueness}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  For any $f \in L^1(\mc{A};X)$ and any $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, if $\E^{\mc{B}}f$ and $\td{\E}^{\mc{B}}f$ are two conditional expectations of $f$ given $\mc{B}$, then $\E^{\mc{B}}f = \td{\E}^{\mc{B}}f$ almost surely.
\end{prop}

\begin{proof}
  First we consider the real one-dimensional case.
  Fix $f \in L^1(\mc{A};\R)$.
  For all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P - \int_B f \, \dd\P = 0.
  \end{equation*}
  Since $\E^{\mc{B}}f - \td{\E}^{\mc{B}}f$ is $\mc{B}$-measurable, the subsets
  \begin{equation*}
    B_+ := \{ \E^{\mc{B}}f - \td{\E}^{\mc{B}}f > 0\} \quad \text{and} \quad B_- := \{\E^{\mc{B}}f - \td{\E}^{\mc{B}}f < 0\}
  \end{equation*}
  are both in $\mc{B}$, so we get
  \begin{equation*}
    \int_\Omega |\E^{\mc{B}}f - \td{\E}^{\mc{B}}f | \, \dd\P
    = \int_{B_+}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P
    -  \int_{B_-}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P
    = 0,
  \end{equation*}
  establishing that $\E^{\mc{B}}f = \td{\E}^{\mc{B}}f$ almost surely.

  Now let $X$ be any real Banach space and suppose $f \in L^1(\mc{A};X)$.
  We aim to show that $\E^{\mc{B}} f = \td{\E}^{\mc{B}} f$ almost surely.
  By Lemma \ref{lem:coordinatewise-equality-test} it suffices to show that
  \begin{equation}\label{eq:CE-testing-eq}
    \langle \E^{\mc{B}} f, \mb{x}^* \rangle \aseq \langle \td{\E}^{\mc{B}} f, \mb{x}^* \rangle \qquad \text{for all $\mb{x}^* \in X^*$.}
  \end{equation}
  This will follow from showing that $\langle \E^{\mc{B}} f, \mb{x}^* \rangle$ and $\langle \td{\E}^{\mc{B}} f, \mb{x}^* \rangle$ are both conditional expectations of the $\R$-valued function $\langle f, \mb{x}^* \rangle$ given $\mc{B}$, thanks to the uniqueness result already established in the one-dimensional case.
  For all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \langle \E^{\mc{B}} f, \mb{x}^* \rangle \, \dd\P
    = \Big\langle \int_B \E^{\mc{B}} f \, \dd\P, \mb{x}^* \Big\rangle
    = \Big\langle \int_B f \, \dd\P, \mb{x}^* \Big\rangle
    = \int_B \langle f, \mb{x}^* \rangle \, \dd\P
  \end{equation*}
  using that $\E^{\mc{B}}f$ is a conditional expectation of $f$, and the same argument shows that
  \begin{equation*}
    \int_B \langle \td{\E}^{\mc{B}} f, \mb{x}^* \rangle \, \dd\P = \int_B \langle f, \mb{x}^* \rangle \, \dd\P.
  \end{equation*}
  Thus $\langle \E^{\mc{B}} f, \mb{x}^* \rangle$ and $\langle \td{\E}^{\mc{B}} f, \mb{x}^* \rangle$ are conditional expectations of of $\langle f, \mb{x}^* \rangle$ given $\mc{B}$, establishing \eqref{eq:CE-testing-eq}, and thus proving that $\E^{\mc{B}} f \aseq \td{\E}^{\mc{B}} f$.

  Finally, if $X$ is a complex Banach space, the result follows by considering real and imaginary parts separately.
\end{proof}

Next we will establish existence, positivity, and $L^p$-contractivity of conditional expectations in the scalar-valued case.

\begin{thm}\label{thm:conditional-expectation-existence-scalar}
  Let $(\Omega,\mc{A},\P)$ be a probability space.
  For any $f \in L^1(\mc{A})$ and any $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a conditional expectation $\E^{\mc{B}}f$ exists.
  The operator $f \mapsto \E^{\mc{B}}f$ is linear, and for all $p \in [1,\infty]$, $\E^{\mc{B}}$ is a positive contraction on $L^p(\mc{A})$: that is, if $f \in L^p(\mc{A})$, then
  \begin{equation*}
    \|\E^{\mc{B}}f\|_{p} \leq \|f\|_{p},
  \end{equation*}
  and if $f$ is a.s. nonnegative then so is $\E^{\mc{B}}f$.
\end{thm}

\begin{proof}

  We can prove positivity from the defining property \eqref{eq:conditional-expectation-property}, before we establish existence.
  Suppose $f \in L^1(\mc{A})$ is a.s. nonnegative.
  Then for all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P = \int_B f \, \dd\P \geq 0,
  \end{equation*}
  which implies that the $\mc{B}$-measurable function $\E^{\mc{B}}f$ is a.s. nonnegative.\footnote{This uses an exercise from measure theory: if $g$ is $\mc{B}$-measurable and $\int_B g \geq 0$ for all $\mc{B}$-measurable sets, then $g \geq 0$ a.e.. Proof: the set $N := \{g(\omega) < 0\}$ is $\mc{B}$-measurable, and assuming it has positive measure leads to the contradiction $0 \leq \int_B g < 0$.}
  
  Now fix $p \in [1,\infty]$ and let $f \in L^p(\mc{A})$; we will construct a linear contractive conditional expectation operator $\E^{\mc{B}}$ on $L^p(\mc{B};\K)$ directly.

  \textbf{Mild case: $p > 1$}, so most importantly $p' < \infty$.
  The inclusion map $\map{\iota}{L^{p'}(\mc{B})}{L^{p'}(\mc{A})}$ is  contractive, so (using that $L^p$ is the dual of $(L^{p'})^*$, which requires $p' < \infty$) its adjoint $\map{\E^{\mc{B}} := \iota^*}{L^p(\mc{A})}{L^p(\mc{B})}$ is also contractive.
  For all $f \in L^p(\mc{A})$ and $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P = \langle \iota^* f, \1_{B} \rangle = \langle f, \iota \1_{B} \rangle = \langle f, \1_{B} \rangle = \int_B f \, \dd\P,
  \end{equation*}
  so $\E^{\mc{B}} f \in L^p(\mc{B}) \subset L^1(\mc{B})$ is a conditional expectation of $f$ given $\mc{B}$.


  \textbf{(German) spicy case: $p = 1$.} The difficulty here is that $L^1$ is \emph{strictly} contained in the dual of $L^\infty$, so taking an adjoint of the inclusion $L^\infty(\mc{B}) \to L^\infty(\mc{A})$ is not so straightforward.\footnote{See Exercise \ref{ex:conditional-expectation-as-adjoint}.} 
  Instead we argue by density.
  We have that $L^2(\mc{A})$ is dense in $L^1(\mc{A})$, so we aim to extend the conditional expectation defined above (in the case $p=2$) by continuity.
  For $f \in L^2(\mc{A})$ and $g \in L^\infty(\mc{B})$ we have
  \begin{equation*}
    |\langle \E^{\mc{B}} f, g \rangle| = |\langle f, \iota g \rangle| = |\langle f, g \rangle| \leq \|f\|_1 \|g\|_\infty
  \end{equation*}
  using that $L^\infty(\mc{B}) \subset L^2(\mc{B})$.
  Taking the supremum over all nonzero $g \in L^\infty(\mc{B})$ proves that
  \begin{equation*}
    \|\E^{\mc{B}} f\|_1 \leq \|f\|_1,
  \end{equation*}
  so $\E^{\mc{B}}$ extends to a contraction $L^1(\mc{A}) \to L^1(\mc{B})$.
  For $f \in L^1(\mc{A})$ and $B \in \mc{B}$, using that integration on $B$ is a continuous linear functional on $L^1$, we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P
    = \lim_{n \to \infty} \int_B \E^{\mc{B}} f_n \, \dd\P
    = \lim_{n \to \infty} \int_B  f_n \, \dd\P
    = \int_B f \, \dd\P
  \end{equation*}
  where $f_n$ is a sequence in $L^2(\mc{A})$ converging to $f$ in $L^1(\mc{A})$.
  Thus $\E^{\mc{B}}f$ is a conditional expectation of $f$ given $\mc{B}$, and we are done.
\end{proof}

Note that the proof of the previous result also establishes the following adjoint relation, which can also be proven directly from the defining property \eqref{eq:conditional-expectation-property} (Exercise \ref{ex:CE-adjoint}).

\begin{prop}\label{prop:CE-adjoint}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$.
  For all $p \in (1,\infty]$, the conditional expectation $\E^{\mc{B}}$ on $L^p(\mc{A})$ is the adjoint of the corresponding conditional expectation on $L^{p'}(\mc{A})$.
\end{prop}

Now we can use the extension theorem for positive operators to show the existence of conditional expectations of Banach-valued random variables.

\begin{prop}
  Let $(\Omega,\mc{A},\P)$ be a probability space, let $\mc{B}$ be a $\sigma$-subalgebra of $\mc{A}$, and let $X$ be a Banach space.
  Then for any $f \in L^1(\mc{A};X)$, a conditional expectation $\E_X^{\mc{B}}f$ of $f$ given $\mc{B}$ exists.
  Furthermore, for all $p \in [1,\infty]$, $\E^{\mc{B}}$ is a contraction on $L^p(\mc{A};X)$.
\end{prop}


\begin{proof}  
  First fix $p \in [1,\infty)$.
  Since the conditional expectation $\E^{\mc{B}}$ is a positive operator on $L^p(\mc{A})$, by Theorem \ref{thm:positive-extensions} it admits a bounded $X$-valued extension, which we denote by $\E_X^{\mc{B}}$.
  Since $\E^{\mc{B}}$ is contractive, so is $\E_X^{\mc{B}}$.
  We just need to show that for all $f \in L^p(\mc{A};X)$, $\E_X^{\mc{B}}f$ is a conditional expectation of $f$ given $\mc{B}$; we will do this by scalarisation.
  For all $B \in \mc{B}$ and all functionals $\mb{x}^* \in X^*$, since the function $\langle f, \mb{x}^* \rangle$ is in $L^1(\mc{A})$, we have
  \begin{equation*}
    \begin{aligned}
      \Big\langle \int_B \E_X^{\mc{B}} f \, \dd\P, \mb{x}^* \Big\rangle
      &= \int_B \langle \E_X^{\mc{B}} f, \mb{x}^* \rangle \, \dd\P \\
      &\stackrel{(*)}{=} \int_B \E^{\mc{B}} (\langle f, \mb{x}^* \rangle) \, \dd\P
      = \int_B \langle f, \mb{x}^* \rangle \, \dd\P
      = \Big\langle \int_B f \, \dd\P, \mb{x}^* \Big\rangle.
    \end{aligned}
  \end{equation*}
  (see Exercise \ref{ex:tensor-extension-basic-props} for the starred equality).
  Since this holds for all $\mb{x}^* \in X^*$, we have
  \begin{equation*}
     \int_B \E_X^{\mc{B}} f \, \dd\P = \int_B f \, \dd\P,
   \end{equation*}
   which shows that $\E_X^{\mc{B}}$ is a conditional expectation of $f$ given $\mc{B}$.

   Now we establish the result for $p = \infty$: let $f \in L^\infty(\mc{A};X) \subset L^2(\mc{A};X)$.
   Then a conditional expectation $\E_X^{\mc{B}} f$ of $f$ given $\mc{B}$ is defined as an element of $L^2(\mc{B};X)$: we just need to show that $\|\E_X^{\mc{B}} f\|_\infty \leq \|f\|_\infty$.
   We can test this by duality using Proposition \ref{prop:bochner-preduality} and that $L^2(\mc{B}; X^*)$ is dense in $L^1( \mc{B}; X^*)$.
   
   For all $g \in L^2(\mc{B}; X^*)$, since the operator $\E^{\mc{B}} \in \Lin(L^2(\mc{A}))$ is self-adjoint, we have
\begin{equation*}
  \begin{aligned}
    | \langle \E_X^{\mc{B}} f, g \rangle |
    =  | \langle \td{\E^{\mc{B}}} f, g \rangle | 
    &\stackrel{(*)}{=} | \langle f, \td{\E^{\mc{B}}} g \rangle | \\
    &\leq  \|f\|_{L^\infty(\mc{A};X)} \|\E_X^{\mc{B}} g\|_{L^1(\mc{A};X^*)} \\
    &\leq  \|f\|_{L^\infty(\mc{A};X)} \| g\|_{L^1(\mc{A};X^*)}.
  \end{aligned}
\end{equation*}
For the starred equality see Exercise \ref{ex:tensor-adjoint} in the previous chapter, particularly the identity \eqref{eq:tensor-adjoint-identity}.
Taking the supremum over all nonzero $g \in L^2(\mc{B};X^*)$ completes the proof.
\end{proof}


\todo{further properties of vector-valued conditional expectations, as needed}

\begin{prop}\label{prop:CE-measurable-op}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$, and let $X$ and $Y$ be Banach spaces.
  Consider an operator-valued function $T \in L^\infty(\Omega, \mc{B}; \Lin(X,Y))$.
  For $f \in L^1(\Omega, \mc{A}; X)$ define the function $Tf \in L^1(\Omega, \mc{A}; Y)$ by
  \begin{equation*}
    Tf(\omega) := T(\omega) f(\omega).
  \end{equation*}
  Then the identity
  \begin{equation*}
    \E^{\mc{B}}(Tf) = T \E^{\mc{B}}f
  \end{equation*}
  holds.
\end{prop}

\begin{proof}
  Exercise \ref{ex:CE-measurable-op}. %mk
\end{proof}

\begin{rmk}
  We have only considered conditional expectations $\E^{\mc{B}}$ on probability spaces, but the concept can be extended to general measure spaces $(S,\mc{A},\mu)$ provided that the measure $\mu$ is $\sigma$-finite on the $\sigma$-subalgebra $\mc{B} \subset \mc{A}$ (although the arguments require a fair bit of modification).
  This approach is taken in \cite{HNVW16}.
\end{rmk}


\subsection{Martingales and martingale transforms}

\begin{defn}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, let $(\mc{A}_n)_{n \in \N}$ be a filtration on this space,  and let $X$ be a Banach space.
  \begin{itemize}
  \item
    A stochastic process $(M_n)_{n \in \N}$ with each $M_n \in L^1(\Omega; X)$ is called an \emph{martingale} with respect to $(\mc{A}_n)_{n \in \N}$ if 
    \begin{equation}\label{eq:mgale-defining-property}
      M_n = \E^{\mc{A}_n} M_{n+1} \qquad \forall n \in \N.
    \end{equation}
    Note that in particular each $M_n$ is $\mc{A}_n$-measurable.

  \item
    Let $(M_n)_{n \in \N}$ be a martingale as above.
    The associated \emph{martingale difference sequence} is the process $(dM_n)_{n \in \N}$ in $L^1(\Omega; X)$ defined by
    \begin{equation*}
      dM_0 := M_0, \qquad dM_k := M_k - M_{k-1} \quad (k \geq 1)
    \end{equation*}
    (the difference equation can be made to hold for all $k \geq 0$ by defining $M_{-1} := 0$).
  \end{itemize}
\end{defn}

\begin{rmk}
  Now is a good time to complete Exercise \ref{ex:martingale-elementary-properties}, establishing a few elementary properties of martingales. 
\end{rmk}

Martingales are stochastic processes that are `balanced': at time $n$, the best estimate of the state of the process at time $n+1$ is precisely the current state of the process.
They are intricately linked with many topics in harmonic and functional analysis.

\begin{example}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $(\mc{A}_n)_{n \in \N}$ a filtration.
  Let $X$ be a Banach space and $f \in L^1([0,1);X)$.
  For each $n \in \N$ define $f_n := \E^{\mc{A}_n}f \in L^1(\Omega;X)$.
  Then by the monotonicity property of conditional expectations,
  \begin{equation*}
    \E^{\mc{A}_n} f_{n+1} = \E^{\mc{A}_n} \E^{\mc{A}_{n+1}} f = \E^{\mc{A}_n} f = f_n,
  \end{equation*}
  so $(f_n)_{n \in \N}$ is a martingale.
  This is called the \emph{martingale associated with $f$}.
\end{example}

\begin{example}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, $X$ a Banach space, and let $(g_n)_{n \in \N}$ be a sequence of random variables in $L^1(\Omega;X)$ which are mutually independent.
  Let $(\mc{F}_n)_{n \in \N}$ be the filtration generated by the process $(g_n)_{n \in \N}$, and for each $n \in \N$ let $s_n := \sum_{m=0}^n g_n$ be the sum of the first $n+1$ random variables.
  Then we have
  \begin{equation*}
    \begin{aligned}
      \E^{\mc{F}_n} s_{n+1} &= \E^{\mc{F}_n}\Big( \sum_{m=0}^ng_m \Big) + \E^{\mc{F}_n} g_{n+1} \\
      &=\sum_{m=0}^n g_m = s_n
    \end{aligned}
  \end{equation*}
  since the random variables $(g_m)_{m=0}^n$ are $\mc{F}_n$-measurable and $g_{n+1}$ is independent of $\mc{F}_n$.\todo{this needs to be an exercise or a note}
  Thus the sum process $(s_n)_{n \in \N}$ is a martingale.
\end{example}

\begin{prop}\label{prop:mgale-transforms}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, $X$ a Banach space, and let $(M_n)_{n \in \N}$ be a martingale (valued in $X$) with respect to a filtration $(\mc{A}_n)_{n \in \N}$.
  Let $Y$ be another Banach space, and let $(T_n)_{n \in \N}$ be a sequence of operators in $L^\infty(\Omega; \Lin(X,Y))$ which is predictable with respect to $(\mc{A}_n)_{n \in \N}$.
  For each $n \in \N$ and $\omega \in \Omega$ define a function $T_n M_n \in L^1(\Omega; Y)$ by
  \begin{equation*}
    (T \cdot M)_n := \sum_{m=0}^n T_m dM_m \in Y.
  \end{equation*}
  Then $(T \cdot M)_{n \in \N}$ is a ($Y$-valued) martingale with respect to $(\mc{A}_n)_{n \in \N}$.
\end{prop}

\begin{proof}
  Integrability of each $(T \cdot M)_n$ follows from that of each $M_m$ and the a.s. uniform boundedness of each $T_m$.
  To see that $(T \cdot M)_{n \in \N}$ is a martingale with respect to $(\mc{A}_n)_{n \in \N}$, it suffices to show\todo{pur a prop. allowing for this} that
  \begin{equation}\label{eq:transform-independent-increments}
    \E^{\mc{A}_n} d(T \cdot M)_{n+1} = 0
  \end{equation}
  for all $n \in \N$.
  By Proposition \ref{prop:CE-measurable-op}, since $T_{n+1}$ is $\mc{A}_n$-measurable by the predictability assumption and $dM_{n+1}$ is independent of $\mc{A}_n$ (Exercise \ref{}), we have %mk
  \begin{equation*}
    \E^{\mc{A}_n} d(T \cdot M)_{n+1} = \E^{\mc{A}_n} (T_{n+1} dM_{n+1}) = T_{n+1} \E^{\mc{A}_n} dM_{n+1} = 0,
  \end{equation*}
  proving \eqref{eq:transform-independent-increments} as required.
\end{proof}

\begin{example}
  \todo{return to this AFTER doing martingale transofrms - this is the $\mb{x}$-transform of the sum process of $\pi$}
  We return once more to our betting game, with notation given in Example \ref{eg:gambling-filtrations}.
  Consider the stochastic process $(\mb{s}_n)_{n \in \N}$ representing the evolution of the state of your wallet: recall that
  \begin{equation*}
    \mb{s}_{n+1} = \mb{s}_n + \pi_{n+1} \mb{x}_{n+1} \qquad \forall n \in \N,
  \end{equation*}
  where $\pi_{n+1}$ is the outcome of the coin toss and $\mb{x}_{n+1}$ is the vector wagered at time $n+1$.
  The process $(\pi_n)_{n \in \N}$ generates the filtration $(\mc{F}_n)_{n \in \N}$.
  To keep everything integrable, let us assume that each of the random variables $\mb{x}_n \in L^1(\Omega,X)$ are integrable.
  Then $\mb{s}_n$, being a sum of the vectors $\mb{x}_1, \ldots, \mb{x}_n$, is also integrable, and since both $\mb{s}_n$ and $\mb{x}_{n+1}$ are $\mc{F}_n$-measurable, we can compute
  \begin{equation*}
    \begin{aligned}
      \E^{\mc{F}_n} \mb{s}_{n+1} &= \E^{\mc{F}_n} \mb{s}_n + \E^{\mc{F}_n}( \pi_{n+1} \mb{x}_{n+1} ) \\
      &= \mb{s}_n + \E^{\mc{F}_n}(\pi_{n+1}) \mb{x}_{n+1} \\
      &= \mb{s}_n.
    \end{aligned}
  \end{equation*}
  In the last step ...
\end{example}

-stopped martingales
-Rademacher variables and sums

\todo{stopped martingales}

\begin{defn}
  limiting $\sigma$-algebra 
\end{defn}

\begin{thm}[$L^p$-convergence of martingales]
  %Pisier Theorem 1.14
  
\end{thm}

\subsection{Maximal inequalities and pointwise convergence}
% proofs from Pisier with little modifications to allow for infinite martingales


\begin{defn}
  definition of $\R$-valued submartingale
\end{defn}

As an example, consider a martingale $(f_n)_{n \in \N}$ taking values in a Banach space $X$.
Then by {\color{red} CITE PART OF POSITIVE OP EXTENSION HERE} we have for all $m \geq n$
\begin{equation*}
  \|f_n\|_X = \|\E^{\mc{F}_n} f_m\|_X \leq \E^{\mc{F}_n} \|f_m\|_X \qquad \text{almost surely},
\end{equation*}
so that the sequence $(\|f_n\|_X)_{n \in \N}$ is an $\R$-valued submartingale.

\begin{thm}[Doob's maximal inequalties]
  
\end{thm}

% i think we can do without this
% but include it if necessary
% \begin{thm}[Burkholder--Davis--Gundy inequality]
% \end{thm}

\begin{thm}[Martingale convergence theorem]
  
\end{thm}



\subsection{John--Nirenberg for adapted sequences and the Kahane--Khintchine inequalities}
% following HNVW1 section 3.2.c

Fix a Banach space $X$ and a $\sigma$-finite measure space $(S,\mc{A},\mu)$.\todo{put this all on a probability space}
Consider a filtration $(\mc{F}_n)_{n \in \N}$ and a sequence $\phi = (\phi_n)_{n \in \N}$ of $X$-valued functions adapted to the filtration (i.e. $\phi_n$ is $\mc{F}_n$-measurable for all $n \in \N$).\todo{Strongly?}
For all $q \in (0,\infty)$ we consider the following measure of the oscillation of $\phi$:
\begin{equation*}
    \|\phi\|_{*,q} := \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{\substack{F \in \mc{F}_k \\ 0 < \mu(F) < \infty}} \Big( \fint_{F} \|(\phi_n - \phi_{k-1})(s)\|_{X}^{q} \, \dd\mu(s)\Big)^{1/q} .
\end{equation*}


\begin{thm}[John--Nirenberg inequality for adapted sequences]\label{thm:jn-adapted-sequences}
  With notation as above, for all $p,q \in (0,\infty)$ there exists a finite constant $c_{p,q}$ independent of $\phi$ such that
  \begin{equation*}
    \|\phi\|_{*,p} \leq c_{p,q} \|\phi\|_{*,q}.
  \end{equation*}
\end{thm}

We will prove this as a consequence of a series of lemmas, but before proving this, we demonstrate an important application to Rademacher sums.

\begin{thm}[Kahane--Khintchine inequality]\label{thm:kk}
  Let $X$ be a Banach space and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega,\mc{F},\P)$.
  Then for all $p,q \in (0,\infty)$ there exists a finite constant $\kappa_{p,q}$ such that for all finite sequences $(\mb{x}_n)_{n=1}^N$ in $X$,
  \begin{equation*}
    \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^p(\Omega;X)} \leq \kappa_{p,q} \Big\|\sum_{n=1}^{N} \varepsilon_{n} \mb{x}_{n} \Big\|_{L^q(\Omega;X)}.
  \end{equation*}
  That is, for all $p \in (0,\infty)$, the $L^p$-norms of a Rademacher sum are pairwise equivalent.  
\end{thm}

Since $\Omega$ is a probability space, H\"older's inequality yields the case $p \leq q$ with constant $\kappa_{p,q} = 1$, so we only need to consider the case $q < p$.

\begin{proof}[Proof, assuming the John--Nirenberg inequality]
  Consider the filtration and adapted sequence \todo{make sure to discuss filtrations generated by functions, particularly Rademacher variables} 
  \begin{equation*}
    \mc{F}_n := \sigma(\{\varepsilon_j : 1 \leq j \leq n\}), \qquad \phi_n := \sum_{j=1}^n \varepsilon_j \mb{x}_j.
  \end{equation*}
  We claim that for all $q \in [1,\infty)$ we have
  \begin{equation}\label{eq:kk-claim}
    \|\phi\|_{*,q} = \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^q(\Omega;X)}.
  \end{equation}
  Assuming this for the moment, the John--Nirenberg inequality yields a finite constant $c_{p,q}$ such that
  \begin{equation*}
    \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^p(\Omega;X)}
    = \|\phi\|_{*,p}
    \leq c_{p,q} \|\phi\|_{*,q}
    = \Big\| \sum_{j=1}^N \varepsilon_j \mb{x}_j \Big\|_{L^q(\Omega;X)}
  \end{equation*}
  whenever $1 \leq q < p$.
  If $q < 1 \leq p$, we fix $\theta \in (0,1)$ so that $1/p = \theta/q + (1-\theta)/2p$, and log-convexity of $L^p$-norms gives
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)}
    \leq \| \phi_N \|_{L^q(\Omega;X)}^{\theta} \| \phi_N \|_{L^{2p}(\Omega;X)}^{1 - \theta}
    \leq \| \phi_N \|_{L^q(\Omega;X)}^{\theta} (c_{2p,p} \| \phi_N \|_{L^{p}(\Omega;X)})^{1 - \theta},
  \end{equation*}
  which yields
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)} \leq c_{2p,p}^{(1 - \theta)/\theta} \| \phi_N \|_{L^q(\Omega;X)}.
  \end{equation*}
  Finally, if $q < p < 1$, we simply estimate
  \begin{equation*}
    \| \phi_N \|_{L^p(\Omega;X)} \leq \| \phi_N \|_{L^1(\Omega;X)} \leq c_{1,q} \| \phi_N \|_{L^q(\Omega;X)}.
  \end{equation*}
  This covers all nontrivial cases, and it remains to prove the claimed equality \eqref{eq:kk-claim}.

  First recall that
  \begin{equation*}
    \|\phi\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}} \sup_{\substack{F \in \mc{F}_k \\ \mu(F) > 0}} \Big( \fint_{F} \|(\phi_n - \phi_{k-1})(s)\|_{X}^{q} \, \dd\P(s)\Big)^{1/q}.
  \end{equation*}
  Fix $k \leq n$ and a set $F \in \mc{F}_k$ of positive measure.
  We will compute
  \begin{equation*}
    \fint_{F} \|\1_{F} (\phi_n - \phi_{k-1})(s)\|_{X}^q \, \dd\P(s) = \P(F)^{-1} \E \Big( \1_{F} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big).
  \end{equation*}
  Observe that for all $\omega \in \Omega$
  \begin{equation*}
    \Big\| \sum_{j=k}^{n} \varepsilon_j(\omega) \mb{x}_j\Big\|_{X} = \Big\| \varepsilon_k(\omega) \Big( \mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime}(\omega) \mb{x}_j \Big) \Big\|_{X} = \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}
  \end{equation*}
  where
  \begin{equation*}
    \varepsilon_{j}^{\prime} :=
    \begin{cases}
      \varepsilon_{j} & j \leq k \\
      \varepsilon_{k} \varepsilon_{j} & k+1 \leq j,
    \end{cases}
  \end{equation*}
  since $\varepsilon_j$ is $\pm 1$-valued.
  Now note that the $\sigma$-algebra $\mc{F}^\prime_{k+1,n} := \sigma(\{\varepsilon_j^\prime : k+1 \leq j \leq n\})$ is independent of $\mc{F}_k$, since for all $1 \leq j \leq k$ and $k+1 \leq j' \leq n$ we have by independence of the original Rademacher sequence
  \begin{equation*}
    \E(\varepsilon_j \varepsilon_{j'}^\prime) = \E(\varepsilon_j \varepsilon_k \varepsilon_{j'})
    =
    \begin{cases}
      \E(\varepsilon_j) \E(\varepsilon_k) \E(\varepsilon_{j'}) & \text{if $j < k$} \\
       \E(\varepsilon_{j'}) & \text{if $j = k$}
    \end{cases}
    \quad = 0.
  \end{equation*}
  Thus, since $F \in \mc{F}_k$, we have by independence of $\mc{F}_k$ and $\mc{F}^{\prime}_{k+1, n}$
  \begin{equation*}
    \begin{aligned}
      \P(F)^{-1} \E \Big( \1_{F} \Big\| \sum_{j=k}^{n} \varepsilon_j \mb{x}_j\Big\|_{X}^q \Big)
      &=  \P(F)^{-1} \E \Big( \1_{F} \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} \Big) \\
      &=  \E \Big\|\mb{x}_k + \sum_{j=k+1}^{n} \varepsilon_{j}^{\prime} \mb{x}_j \Big\|_{X}^{q} 
      =  \E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}. \\
    \end{aligned}
  \end{equation*}
  Now letting $\mc{F}_{k,n} := \sigma(\{\varepsilon_j : k \leq j \leq n\})$, we have by the $L^q$-contraction property of conditional expectations
  \begin{equation*}
    \E \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
    = \E \Big\|\E^{\mc{F}_{k,n}} \Big(\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big)\Big\|_{X}^{q}
    \leq \E \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}
  \end{equation*}
  with equality when $k=1$ and $n=N$.
  Thus
  \begin{equation*}
    \|\phi\|_{*,q} =  \sup_{\substack{k,n \in \N \\ k \leq n}}  \Big(\E  \Big\|\sum_{j=k}^{n} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q} = \Big(\E  \Big\|\sum_{j=1}^{N} \varepsilon_{j} \mb{x}_j \Big\|_{X}^{q}\Big)^{1/q}
  \end{equation*}
  which proves the claimed equality \eqref{eq:kk-claim} and completes the proof. 
\end{proof}

In the setting of Hilbert-valued functions (and in particular, scalar-valued functions), the Kahane--Khintchine inequality leads to the classical Khintchine inequalities.

\begin{cor}[Khintchine's inequalities]
  Let $H$ be a Hilbert space, and let $(\varepsilon_{n})_{n \in \N}$ be a Rademacher sequence on a probability space $(\Omega, \mc{F}, \P)$.
  Then for all $p \in (0,\infty)$ there exist finite constants $A_p$ and $B_p$ such that for all finite sequences $(\mb{h}_n)_{n=1}^N$ in $H$,
  \begin{equation*}
    A_p \Big( \sum_{n=1}^N \|\mb{h}_n\|_H^2 \Big)^{1/2} \leq \Big\| \sum_{n=1}^N \varepsilon_n \mb{h}_n \Big\|_{L^p(\Omega;H)} \leq B_p \Big( \sum_{n=1}^N \|\mb{h}_n\|_H^2 \Big)^{1/2}.
  \end{equation*}
\end{cor}

\begin{proof}
  By independence of the Rademacher variables we have
  \begin{equation}
    \begin{aligned}
      \Big\| \sum_{n=1}^N \varepsilon_n \mb{h}_n \Big\|_{L^2(\Omega;H)}^2
      &= \Big\langle \sum_{n=1}^N \varepsilon_n \mb{h}_n , \sum_{m=1}^N \varepsilon_m \mb{h}_m \Big\rangle \\
      &= \sum_{n,m = 1}^N \E(\varepsilon_n \varepsilon_m) \langle \mb{h}_n, \mb{h}_m \rangle 
      = \sum_{n=1}^N \|\mb{h}_{n}\|_{H}^2,
    \end{aligned}
  \end{equation}
  so the result is true for $p=2$ with $A_2 = B_2 = 1$.
  Now use Kahane--Khintchine to extend the result to general $p \in (0,\infty)$.
\end{proof}

\subsubsection{Proof of the John--Nirenberg inequality}

We return to our analysis of a sequence $\phi = (\phi_n)_{n \in \N}$ of $X$-valued functions adapted to a filtration $(\mc{F}_n)_{n \in \N}$ on a $\sigma$-finite measure space $(S,\mc{A},\mu)$.
We prove the John--Nirenberg inequality via a series of lemmas in which we obtain increasingly fine control on the oscillation of $\phi$.

\begin{lem}\label{lem:JN-proof-1}
  For all $k \leq n$, $F \in \mc{F}_k$, and $\alpha > 0$,
  \begin{equation}\label{eq:JN-proof-1-est}
    \mu(F \cap \{ \|\phi_n - \phi_{k-1}\|_{X} > \alpha \}) \leq \Big( \frac{\|\phi\|_{*,q}}{\alpha} \Big)^{q} \mu(F).
  \end{equation}
\end{lem}

\begin{proof}
  We can assume that $0 < \mu(F) < \infty$, otherwise there is nothing to prove.
  The left hand side of \eqref{eq:JN-proof-1-est} is bounded by
  \begin{equation*}
    \int_{F} \Big( \frac{\|\phi_n - \phi_{k-1}\|_{X}}{\alpha} \Big)^{q} \, \dd\mu \leq \mu(F) \Big( \frac{\|\phi\|_{*,q}}{\alpha} \Big)^{q}
  \end{equation*}
  since $F \in \mc{F}_k$ and $k \leq n$, by the definition of $\|\phi\|_{*,q}$.
\end{proof}

Next we show that oscillation control of the form above extends to more general stopping times.

\begin{lem}
  Suppose that there exist $\alpha > 0$ and $\eta > 0$ such that
  \begin{equation*}
    \mu(F \cap \{ \| \phi_n - \phi_{k-1} \| > \alpha \} ) \leq \eta \mu(F) \qquad \forall k \leq n, F \in \mc{F}_k.
  \end{equation*}
  Then for all $k \in \N$, $F \in \mc{F}_k$, and all stopping times $\nu$ such that $\nu \geq k$ on $F$,
  \begin{equation}\label{eq:jn-stoptime}
    \mu(F \cap \{\nu < \infty\} \cap \{ \| \phi_{\nu} - \phi_{k-1} \| > 2\alpha \} ) \leq 2\eta \mu(F).
  \end{equation}
\end{lem}

\begin{proof}
  Sum over all possible values of the stopping time:
  \begin{equation*}
      \mu(F \cap \{\nu < \infty\} \cap \{ \| \phi_{\nu} - \phi_{k-1} \| > 2\alpha \} ) 
      = \lim_{N \to \infty} \sum_{n = k}^{N} \mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ),
  \end{equation*}
  where $F_{n} := F \cap \{\nu = n\}$.
  For fixed $N \geq n > k$, since $F_n \in \mc{F}_n \subset \mc{F}_{n+1}$, we have by assumption
  \begin{equation*}
    \begin{aligned}
      &\mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ) \\
      &\leq \mu(F_n \cap \{ \| \phi_{n} - \phi_{N} \| > \alpha \} )
      + \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \} ) \\
      &\leq \eta \mu(F_n) + \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \} ).
    \end{aligned}
  \end{equation*}
  Thus we have
  \begin{equation*}
    \begin{aligned}
      &\lim_{N \to \infty} \sum_{n = k}^{N} \mu(F_n \cap \{ \| \phi_{n} - \phi_{k-1} \| > 2\alpha \} ) \\
      &\leq \lim_{N \to \infty} \Big( \eta \sum_{n=k}^N \mu(F_n) + \sum_{n=k}^N \mu(F_n \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \}) \Big) \\
      &\leq \eta \mu(F) + \lim_{N \to \infty} \mu(F \cap \{ \| \phi_{k-1} - \phi_{N} \| > \alpha \})  
      \leq 2\eta\mu(F)
    \end{aligned}
  \end{equation*}
  using the assumption and $F \in \mc{F}_k$ in the last estimate.
\end{proof}
  
In the following lemma we make use of the \emph{started sequence}
\begin{equation*}
  {}^{k-1}\phi = (\phi_n - \phi_{k-1})_{n \geq k-1}
\end{equation*}
and its maximal function
\begin{equation*}
  ({}^{k-1} \phi)^{*}(s) := \sup_{n \geq k-1} \|({}^{k-1}\phi)_n(s)\|_X = \sup_{n \geq k} \|(\phi_n - \phi_{k-1})(s)\|_X.
\end{equation*}

\begin{lem}\label{lem:jn-mf}
  Suppose that $\phi$ satisfies \eqref{eq:jn-stoptime} for all $k \in \N$, $F \in \mc{F}_k$, and all stopping times $\nu$ such that $\nu \geq k$ on $F$.
  Then for all $\lambda > 0$,
  \begin{equation}\label{eq:jn-mf-eq}
    \mu(F \cap \{ ({}^{k-1} \phi)^{*} > \lambda + 2\alpha \}) \leq 2\eta \mu(F \cap \{  ({}^{k-1} \phi)^{*} > \lambda \}) \qquad \forall k \in \N, F \in \mc{F}_{k}.
  \end{equation}
\end{lem}

\begin{proof}
  Fix $k \in \N$ and consider the stopping times
  \begin{equation*}
    \begin{aligned}
      \rho &:= \inf\{n \geq k : \|\phi_n - \phi_{k-1}\| > \lambda\}, \\
      \nu &:= \inf\{n \geq k : \|\phi_n - \phi_{k-1}\| > \lambda + 2\alpha\}.
    \end{aligned}
  \end{equation*}
  Then $k \leq \rho \leq \nu$, and \eqref{eq:jn-mf-eq} can be rewritten as
  \begin{equation*}
    \mu(F \cap \{\nu < \infty\}) \leq 2\eta \mu(F \cap \{\rho < \infty\}).
  \end{equation*}
  Now fix $n \geq k$ and let $F_n := F \cap \{\rho = n\} \in \mc{F}_n$.
  On $\{F_n \cap \{\nu < \infty\}\}$ we have
  \begin{equation*}
    \|\phi_{\nu} - \phi_{n-1}\| \geq \|\phi_{\nu} - \phi_{k-1}\| - \|\phi_{n-1} - \phi_{k-1}\| > (\lambda + 2\alpha) - \lambda = 2\alpha,
  \end{equation*}
  so
  \begin{equation*}
    \mu(F_n \cap \{\nu < \infty\}) = \mu(F_n \cap \{\nu < \infty\} \cap \{\|\phi_{\nu} - \phi_{n-1}\| > 2\alpha\} )
    \leq 2\eta \mu(F_n).
  \end{equation*}
  Summing over $n \geq k$ completes the proof.
\end{proof}

\begin{lem}
  Suppose that $f$ is a non-negative function supported in $F \in \mc{A}$, satisfying
  \begin{equation*}
    \mu(f > \lambda + \alpha) \leq \eta \mu(f > \lambda) \qquad \forall \lambda > 0
  \end{equation*}
  for some $\eta \in (0,1)$ and $\alpha > 0$.
  Then for all $p \in [1,\infty)$,
  \begin{equation*}
    \|f\|_p \leq \frac{1 + \eta^{1/p}}{1 - \eta^{1/p}} \alpha \mu(F)^{1/p}.
  \end{equation*}
\end{lem}

\begin{proof}
  {\color{red} WRITE PROOF}
\end{proof}

\todo{UP TO HERE. HAVE TO COMPLETE PROOF OF JOHN-NIRENBERG.}

{\color{blue}

\begin{equation*}
  \|\phi\|_{**,q} := \sup_{k \in \N} \sup_{\substack{F \in \mc{F}_k \\ 0 < \mu(F) < \infty}} \Big( \fint_{F} ({}^{k-1} \phi)^{*}(s)^q \, \dd\mu(s) \Big)^{1/q},
\end{equation*}

}



\subsection{Gundy's decomposition}

\subsection{The Radon--Nikodym property, martingale convergence, and Bochner space duality}\label{sec:RNP}

\subsection*{Exercises}

\begin{exercise}
  Let $(f_n)_{n \in \N}$ be a stochastic process on a probability space $(\Omega,\mc{A},\P)$.
  Suppose that $(f_n)$ is predictable with respect to the filtration generated by $(f_n)$ (see Example \ref{eg:filtration-generated-by-process}).
  Show that the process is deterministic, in the sense that each $f_n$ is constant.
\end{exercise}

\begin{exercise}\label{ex:winnings-unpredictability}
  In the setting of Example \ref{eg:gambling-filtrations}, show that the random variable $\mb{s}_{n+1}$ is $\mc{F}_n$-measurable if and only if $\mb{x}_{n+1} \equiv 0$.
\end{exercise}

\begin{exercise}\label{ex:gambling-in-linfty}
  This exercise takes place in the setting of Example \ref{eg:gambling-stoppingtimes}.
  \begin{itemize}
  \item
    Let $X = \ell^\infty(\N)$.
    Suppose that the wager vectors $\map{\mb{x}_n}{\Omega}{\ell^\infty(\N)}$ are such that for all $\omega \in \Omega$, the vectors $(\mb{x}_n(\omega))_{n \in \N}$ are pairwise distinct standard basis vectors (i.e. $\{0,1\}$-valued sequences, zero for all but one index).
    Fix $\lambda > 0$ and let $K = \{\mb{a} \in \ell^\infty(\N) : \|\mb{a}\|_\infty \geq \lambda\} = \ell^\infty(\N) \sm B_\lambda(0)$.
    Show that the stopping time
    \begin{equation*}
      T_K(\omega) := \inf\{n \in \N : \mb{s}_n(\omega) \in K\} 
    \end{equation*}
    is finite if and only if $\lambda \leq 1$.
  \item
    As above, but now let $X = \ell^2(\N)$, and show that the stopping time $T_K$ is finite for all $\lambda > 0$.
  \end{itemize}
\end{exercise}

\begin{exercise}\label{ex:CE-adjoint}
  Use the defining property \eqref{eq:conditional-expectation-property} of conditional expectations (i.e. do not use details of its construction) to prove Proposition \ref{prop:CE-adjoint}.
\end{exercise}

\begin{exercise}\label{ex:conditional-expectation-as-adjoint}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$.
  Using that $L^1(\mc{A}) \subsetneq L^\infty(\mc{A})^*$, show that the adjoint of the inclusion map $\map{\iota}{L^\infty(\mc{B})}{L^\infty(\mc{A})}$, which \emph{a priori} maps $L^1(\mc{A}) \to L^\infty(\mc{B})^* \supsetneq L^1(\mc{B})$, actually maps into $L^1(\mc{B})$ \emph{without invoking the existence of a conditional expectation operator on $L^1$.}
\end{exercise}

\begin{exercise}\label{ex:CE-measurable-op}
  Prove Proposition \ref{prop:CE-measurable-op}.
\end{exercise}

\begin{exercise}\label{ex:martingale-elementary-properties}
  Let $(\Omega,\mc{A},\P)$ be a probability space, $X$ a Banach space, and let $(M_n)_{n \in \N}$ be a martingale with respect to some filtration $(\mc{A}_n)_{n \in \N}$.
  \begin{itemize}
  \item
    Show that $M_n = \E^{\mc{A}_n} M_m$ for all $n,m \in \N$ with $m > n$,
  \item
    Let $(\mc{F}_n)_{n \in \N}$ be the filtration generated by $(M_n)_{n \in \N}$, i.e.
    \begin{equation*}
      \mc{F}_n := \sigma(M_0, M_1, \ldots, M_n).
    \end{equation*}
    Show that $(M_n)_{n \in \N}$ is a martingale with respect to $(\mc{F}_n)_{n \in \N}$.
  \item
    For all $p \in [1,\infty]$, show that the sequence $\|M_n\|_{L^p(\Omega,\P;X)}$ is monotonically increasing in $n$.
  \end{itemize}
  \todo{add: martingales in bijective correspondence with their difference sequences}
  \todo{add: martingales = independent increments}

\end{exercise}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
