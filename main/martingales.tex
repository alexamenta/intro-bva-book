
\todo{Write expository introduction}

\section{Gambling in Banach spaces}\label{sec:gambling}

To motivate the theory in this section, we're going to imagine a betting game.
At each turn, you bet on the outcome of a coin toss.
The quantities that you can bet are taken from a Banach space $X$.
The initial state of your wallet, $\mb{s}_{-1}$, is the zero vector
\begin{equation*}
  \mb{s}_{-1} = 0 \in X.
\end{equation*}
At each time $n \in \N = \{0,1,\ldots\}$, you choose a vector $\mb{x}_n \in X$ to wager.
I then flip a fair coin, which shows either Heads or Tails, and the state of your wallet becomes
\begin{equation*}
  \mb{s}_n =
  \begin{cases}
    \mb{s}_{n-1} + \mb{x}_n & \text{if the coin shows Heads} \\
    \mb{s}_{n-1} - \mb{x}_n & \text{if the coin shows Tails.}
  \end{cases}
\end{equation*}
The Banach space $X$ is not ordered, so there is no canonical notion of $\mb{s}_n$ being `more' or `less' than $\mb{s}_{n-1}$. Thus the game is not about winning or losing (the true winner of the game is Functional Analysis).
In this chapter we will discuss various probabilistic concepts that can be well-understood in the context of this game.

\section{Random variables, filtrations, and stochastic processes}

First we will set up some basic probabilistic language in Banach spaces.
See Section \ref{sec:probability} in the Appendix for a review of basic measure-theoretic probability theory.

\begin{defn}
  Let $X$ be a Banach space and $(\Omega,\mc{A},\P)$ a probability space.
  An \emph{$X$-valued random variable} is a strongly measurable function $\map{f}{\Omega}{X}$.
  If $f \in L^1(\Omega;X)$ is an integrable random variable, we define the \emph{expectation} to be the Bochner integral
  \begin{equation*}
    \E f = \int_{\Omega} f(\omega) \, \dd\P(\omega).
  \end{equation*}
  When $X$ is the scalar field, this coincides with the usual definition of the expectation of a scalar-valued random variable.
\end{defn}

\begin{defn}
  A \emph{filtration} on a probability space $(\Omega,\mc{A},\P)$ is a monotone increasing (i.e. nondecreasing) sequence of $\sigma$-subalgebras
  \begin{equation*}
    \mc{A}_0 \subseteq \mc{A}_1 \subseteq \mc{A}_2 \subseteq \cdots \subseteq \mc{A}.
  \end{equation*}
\end{defn}

\begin{example}\label{eg:dyadic-filtration}
  Consider the unit interval $[0,1) \subset \R$ with Borel $\sigma$-algebra and Lebesgue measure.
  For each $n \in \N$, let $\mc{F}_n$ be the $\sigma$-algebra generated by the \emph{dyadic intervals of length $2^{-n}$}, i.e. intervals of the form
  \begin{equation*}
    [2^{-n}k, 2^{-n}(k+1)) \qquad k = 0, 1, 2, \ldots, 2^{n}- 1.
  \end{equation*}
  Then $(\mc{F}_n)_{n \in \N}$ is a filtration, which we call the \emph{(standard) dyadic filtration}.
  A Banach-valued function $\map{f}{[0,1)}{X}$ is $\mc{F}_n$ measurable if and only if it is constant on each dyadic interval of length $2^{-n}$.  
\end{example}

\begin{example}\label{eg:coordinate-filtration}
  Let $\{-1,1\}$ be a two-point space with uniform probability measure, and consider the infinite product
  \begin{equation*}
    \Omega := \prod_{n \in \N} \{-1,1\} = \{-1,1\}^\N,
  \end{equation*}
  which (equipped with the product $\sigma$-algebra and measure) is a probability space.
  Elements of $\Omega$ are sequences $\omega = (\omega_n)_{n \in \N}$ where each $\omega_n = \pm 1$.
  Suppose $n \in \N$, fix a vector $\eta = (\eta_0, \eta_1, \ldots \eta_n) \in \{-1,1\}$ of length $n+1$, and define the set
  \begin{equation*}
    A_{\nu} := \{\omega \in \Omega : \omega_k = \eta_k \; \forall k = 0,1,\ldots,n\};
  \end{equation*}
  that is, a point $\omega \in \Omega$ belongs to $A_{\nu}$ if its first $n+1$ components are given by $\nu$.
  For each $n \in \N$, let $\mc{F}_n$ be the $\sigma$-algebra generated by all sets $A_{\nu}$ with $\nu \in \{-1,1\}^n$.
  Then $(\mc{F}_n)_{n \in \N}$ is a filtration, which we call the \emph{coordinate filtration}.
  A Banach-valued function $\map{f}{\Omega}{X}$ is $\mc{F}_n$-measurable if and only if $f(\omega) = f(\omega_0,\ldots,\omega_n)$ only depends on the first $n+1$ coordinates of its input variable.
\end{example}

\begin{rmk}
  Example \ref{eg:coordinate-filtration} encodes the same information as Example \ref{eg:dyadic-filtration}.
  Each dyadic interval $I \subset [0,1]$ has exactly two dyadic subintervals, and every vector $\nu \in \{-1,1\}^n$ can be extended in exactly two ways to a vector in $\{-1,1\}^{n+1}$.
  Equivalently, each infinite sequence $\omega \in \{0,1\}^\N$ corresponds to the binary expansion of a number $t \in [0,1)$, and this correspondence is bijective up to a measure zero subset (corresponding to the negligible non-uniqueness of binary expansions).
  The set of sequences $\omega' \in \{0,1\}^\N$ whose first $n+1$ entries coincide with those of $\omega$ then corresponds to the set $A_{(\omega_0,\ldots,\omega_n)}$, which corresponds to the unique dyadic interval of length $2^{-(n+1)}$ containing $t$. 
\end{rmk}

Filtrations are closely linked with stochastic processes.
While we don't plan on saying anything really serious about these in this course, it will be useful to keep the core concept in mind, as it guides a lot of probabilistic intuition.

\begin{defn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  A \emph{(discrete-time) $X$-valued stochastic process} on $(\Omega,\mc{A},\P)$ is a sequence of $\mc{A}$-measurable random variables $\map{f_n}{\Omega}{X}$, $n \in \N$.
  Given a filtration $(\mc{A}_n)_{n \in \N}$, a stochastic process $(f_n)_{n \in \N}$ is called \emph{predictable} (with respect to the filtration) if each $f_n$ is $\mc{A}_{n-1}$-measurable (with the convention that $\mc{A}_{-1} = \{\varnothing, \Omega\}$), and \emph{adapted} if each $f_n$ is $\mc{A}_{n}$ measurable.
\end{defn}

\begin{rmk}
  With obvious modifications one can talk about filtrations and stochastic processes starting at an arbitrary index, finite filtrations/processes, or filtrations/processes with respect to arbitrary (total or partial) orders, for example with a continuous time index.
  In this course we will only consider discrete indexing sets contained in $\N$.
\end{rmk}

One should think of a filtration $(\mc{A}_n)_{n=0}^\infty$ as representing the progression of available information over time, usually in relation to a stochastic process.
Each $\sigma$-subalgebra $\mc{A}_n \subset \mc{A}$ represents the information available at time $n$.
There are two equivalent ways of thinking about the availability of information: one is that at time $n$ one has access to all $\mc{A}_n$-measurable subsets; the other is that at time $n$ one has access to all $\mc{A}_n$-measurable functions.
The monotonicity assumption says that no information is lost as time progresses.
Predictability of a stochastic process $(f_n)_{n \in \N}$ with respect to the filtration $(\mc{A}_n)_{n \in \N}$ thus says the following: if the available information is represented by $(\mc{A}_n)_{n \in \N}$, then at each time $n$, one already has access to the $\mc{A}_n$-measurable function $f_{n+1}$.

\begin{example}\label{eg:filtration-generated-by-process}
  Let $(\Omega,\mc{A},\P)$ be a probability space, $X$ a Banach space, and let $(f_n)_{n \in \N}$ be an $X$-valued stochastic process on $(\Omega,\mc{A},\P)$.
  The \emph{filtration generated by the process $(f_n)_{n \in \N}$} is given by
  \begin{equation*}
    \mc{F}_n := \sigma(f_0,f_1,\ldots,f_n) \qquad \forall n \in \N.
  \end{equation*}
  The information-theoretic intuition says that at time $n \in \N$, one `knows' the functions $f_0, f_1, \ldots f_n$, as these are in $\mc{F}_n$, and one also knows all functions of the form
  \begin{equation*}
    g \circ (f_0, f_1, \ldots, f_n) \colon \omega \mapsto g(f_0(\omega),f_1(\omega),\ldots,f_n(\omega))
  \end{equation*}
  where $\map{g}{X^{n+1}}{\C}$ is measurable (as such compositions are automatically measurable).
  In fact, all $\mc{F}_n$-measurable functions $\Omega \to \C$ are of this form.\todo{cite a reference}
\end{example}

\begin{example}\label{eg:gambling-filtrations}
  Consider the game we introduced in Section \ref{sec:gambling}.
  At each time $n \in \N$ I flip a fair coin, which comes up Heads ($H$) or Tails $(T)$ with equal probability.
  The natural probability space on which to base this game is the infinite product $\Omega = \{-1,+1\}^{\N}$ (see Example \ref{eg:coordinate-filtration}).
  The value $-1$ represents Tails, while $+1$ represents Heads.
  For each $n \in \N$ let $\map{\pi_n}{\Omega}{\{-1,+1\}}$ be the $n$-th coordinate function, which represents the outcome of the $n$-th coin toss.
  The sequence $(\pi_n)_{n \in \N}$ is a scalar-valued stochastic process, and the filtration it generated is precisely the coordinate filtration discussed in Example \ref{eg:coordinate-filtration}.

  
  Your bet at time $n$, the vector $\mb{x}_n \in X$, is allowed to depend on the outcomes $\pi_0, \pi_1, \ldots, \pi_{n-1}$: you do not need to register all your bets in advance.
  In probabilistic language, $\map{\mb{x}_n}{\Omega}{X}$ is $\mc{F}_{n-1}$-measurable, i.e. the sequence $(\mb{x}_n)_{n \in \N}$ is a stochastic process which is predictable with respect to the filtration $(\mc{F}_n)_{n \in \N}$.

  Now consider the stochastic process $(\mb{s}_n)_{n \in \N}$, representing the evolution of the state of your wallet.
  By definition we have
  \begin{equation*}
    \mb{s}_{n+1} = \mb{s}_n + \pi_{n+1} \mb{x}_{n+1} \qquad \forall n \in \N;
  \end{equation*}
  keep in mind that this is an equality of $X$-valued random variables, i.e. functions $\Omega \to X$.
  Since $\mb{s}_n$, $\pi_{n+1}$, and $\mb{x}_{n+1}$ are all $\mc{F}_{n+1}$-measurable, we find that $\mb{s}_{n+1}$ is $\mc{F}_{n+1}$-measurable (i.e. we know the state of our wallet $\mb{s}_{n+1}$ at time $n+1$).
  Heuristically, $\mb{s}_{n+1}$ should not be $\mc{F}_n$-measurable unless $\mb{x}_{n+1} \equiv 0$, as this would amount to predicting the future (which can only be done by wagering nothing).
  You should prove this rigourously (Exercise \ref{ex:winnings-unpredictability}).
\end{example}


\begin{defn}
  Given a filtration $(\mc{A}_n)_{n=0}^\infty$ on a probability space $(\Omega, \mc{A}, \P)$, a random variable $\map{T}{\Omega}{\N \cup \{\infty\}}$ is called a \emph{stopping time} (with respect to $(\mc{A}_n)$) if 
  \begin{equation*}
    \{\omega \in \Omega: T(\omega) \leq n\} \in \mc{A}_n \qquad \forall n \geq 0.
  \end{equation*}
  The stopping time $T$ is \emph{finite} if $T$ is almost surely finite.
\end{defn}

Generally stopping times $T$ are defined in terms of some kind of stochastic \emph{stopping condition}.
Interpreting the filtration $(\mc{A}_n)_{n \in \N}$ as modelling the available information at time $n$, $T$ being a stopping time says precisely that at time $n$, one `knows' the set of points $\omega \in \Omega$ for which $T(\omega) \leq n$.
Said less precisely, if $T$ is a stopping time, then at time $n$, one can determine whether or not $T \leq n$.

\begin{example}\label{eg:gambling-stoppingtimes}
  We return to the betting game of Section \ref{sec:gambling}, elaborated upon in Example \ref{eg:gambling-filtrations}.
  Let's suppose that our goal is to get the state of our wallet $\mb{s} \in X$ into a fixed Borel measurable set $K \subset X$, and that we intend to stop betting once this condition holds (i.e. from that point on we only wager the zero vector).
  
  Let
  \begin{equation*}
    T_K(\omega) := \inf\{n \in \N : \mb{s}_n(\omega) \in K \}
  \end{equation*}
  with the usual convention that $T_K(\omega) = \infty$ if $\mb{s}_n(\omega) \notin K$ for all $n \in \N$.
  That is, $T_K$ is the first time $n$ at which $\mb{s}_n \in K$.
  At time $n$ we heuristically know whether or not our wallet satisfied $\mb{s}_m \in K$ for some $m \leq n$, which indicates that $T_K$ should be a stopping time with respect to the filtration $(\mc{F}_n)_{n \in \N}$ associated with the stochastic process $(\pi_n)_{n \in \N}$.
  Rigourously, one shows this by writing for all $n \in \N$
  \begin{equation*}
    \begin{aligned}
      \{\omega \in \Omega : T_K(\omega) \leq n\}
      &= \big\{\omega : \inf\{m : \mb{s}_m(\omega) \in K\} \leq n\big\} \\
      &= \{\omega  : \text{$\mb{s}_m(\omega) \in K$ for some $m \leq n$}\} \\
      &= \bigcup_{m = 0}^n \mb{s}_m^{-1}(K),
    \end{aligned}
  \end{equation*}
  and noting that since each $\mb{s}_m$ is $\mc{F}_m$-measurable, the set above is $\mc{F}_n$-measurable.
  Thus $T_K$ is a stopping time.
  Of course, whether $T_K$ is a finite stopping time depends on the set $K \subset X$, the wager vectors $(\mb{x}_n)_{n \in \N}$, and potentially even the geometry of $X$ (see Exercise \ref{ex:gambling-in-linfty}).
\end{example}

The proof of the following proposition was already done in the previous exercise for the a particular stochastic process, but the proof is identical for a general stochastic process.

\begin{prop}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space.
  Let $(f_n)_{n \in \N}$ be an $X$-valued stochastic process and $K \subset X$ a Borel measurable set.
  Then the function $\map{T_K}{\Omega}{\N \cup \{\infty\}}$ defined by
  \begin{equation*}
    T_K(\omega) := \inf\{n \in \N : f_n(\omega) \in K \}
  \end{equation*}
  is a stopping time.
\end{prop}

The stopping time $T_K$ defined above is called the \emph{first hitting time of $K$}.

\section{Conditional expectations}

\todo{a bit of exposition; include defn of $\E$ and talk about `best approximation with given information'}

\begin{defn}\label{defn:conditional-expectation}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space.
  Let $f \in L^1(\Omega, \mc{A}; X)$ be a integrable $X$-valued random variable.
  Given a $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a \emph{conditional expectation of $f$ given $\mc{B}$} is a $\mc{B}$-measurable random variable $\E^{\mc{B}}f \in L^1(\Omega, \mc{B}; X) \subset L^1(\Omega,\mc{A};X)$ such that
  \begin{equation}\label{eq:conditional-expectation-property}
    \qquad \int_B \E^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P \qquad \text{for all $B \in \mc{B}$.}
  \end{equation}
\end{defn}

\begin{example}\label{eg:atomic-CE}
  Let $(\Omega,\mc{A},\P)$ be a probability space and let $\mc{B} \subset \mc{A}$ be a sub-$\sigma$-algebra which is \emph{atomic}, in the sense that there is a collection of pairwise disjoint subsets $(B_\lambda)_{\lambda \in \Lambda}$ of $\mc{B}$ which generate $\mc{B}$, such that $\P(B_{\lambda}) > 0$ for all $\lambda$, and such that if $B_\lambda$ can be written as a disjoint union $B_\lambda = C \cup D$ for some sets $C,D \in \mc{B}$, then $\P(C) = 0$ or $\P(D) = 0$ (i.e. the sets $B_\lambda$ are \emph{atoms}).
  Let's compute \emph{the} conditional expectation $\E^{\mc{B}}f$ of an integrable random variable $f \in L^1(\mc{A};X)$ (it turns out there is only one).
  Since the atoms $(B_\lambda)_{\lambda}$ generate $\mc{B}$ and are pairwise disjoint, and since $\E^{\mc{B}}f$ is $\mc{B}$-measurable, $\E^{\mc{B}}f$ must be constant on each $B_\lambda$, so that
  \begin{equation*}
    \E^{\mc{B}} f = \sum_{\lambda \in \Lambda} \1_{B_\lambda} \otimes \mb{x}_\lambda
  \end{equation*}
  for some vectors $\mb{x}_{\lambda} \in X$.
  Averaging over one of the atoms $B_{\lambda}$ and using \eqref{eq:conditional-expectation-property} tells us that
  \begin{equation*}
    \begin{aligned}
      \mb{x}_\lambda = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} \E^{\mc{B}} f \, \dd\P = \frac{1}{\P(B_\lambda)} \int_{B_\lambda} f \, \dd\P.
    \end{aligned}
  \end{equation*}
  In probabilistic terms, the quantity on the right hand side is the conditional expectation of $f$ given $B_\lambda$, which exists since $\P(B_\lambda) > 0$.
\end{example}

The previous example shows that conditional expectations with respect to atomic $\sigma$-algebras exist and are unique.
The same is true for general $\sigma$-algebras, but proving this will take a few steps.
First we establish the uniqueness of conditional expectations.

\begin{prop}\label{prop:CE-uniqueness}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  For any $f \in L^1(\mc{A};X)$ and any $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, if $\E^{\mc{B}}f$ and $\td{\E}^{\mc{B}}f$ are two conditional expectations of $f$ given $\mc{B}$, then $\E^{\mc{B}}f = \td{\E}^{\mc{B}}f$ almost surely.
\end{prop}

\begin{proof}
  First we consider the real one-dimensional case.
  Fix $f \in L^1(\mc{A};\R)$.
  For all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P = \int_B f \, \dd\P - \int_B f \, \dd\P = 0.
  \end{equation*}
  Since $\E^{\mc{B}}f - \td{\E}^{\mc{B}}f$ is $\mc{B}$-measurable, the subsets
  \begin{equation*}
    B_+ := \{ \E^{\mc{B}}f - \td{\E}^{\mc{B}}f > 0\} \quad \text{and} \quad B_- := \{\E^{\mc{B}}f - \td{\E}^{\mc{B}}f < 0\}
  \end{equation*}
  are both in $\mc{B}$, so we get
  \begin{equation*}
    \int_\Omega |\E^{\mc{B}}f - \td{\E}^{\mc{B}}f | \, \dd\P
    = \int_{B_+}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P
    -  \int_{B_-}  \E^{\mc{B}}f - \td{\E}^{\mc{B}}f \, \dd\P
    = 0,
  \end{equation*}
  establishing that $\E^{\mc{B}}f = \td{\E}^{\mc{B}}f$ almost surely.

  Now let $X$ be any real Banach space and suppose $f \in L^1(\mc{A};X)$.
  We aim to show that $\E^{\mc{B}} f = \td{\E}^{\mc{B}} f$ almost surely.
  By Lemma \ref{lem:coordinatewise-equality-test} it suffices to show that
  \begin{equation}\label{eq:CE-testing-eq}
    \langle \E^{\mc{B}} f, \mb{x}^* \rangle \aseq \langle \td{\E}^{\mc{B}} f, \mb{x}^* \rangle \qquad \text{for all $\mb{x}^* \in X^*$.}
  \end{equation}
  This will follow from showing that $\langle \E^{\mc{B}} f, \mb{x}^* \rangle$ and $\langle \td{\E}^{\mc{B}} f, \mb{x}^* \rangle$ are both conditional expectations of the $\R$-valued function $\langle f, \mb{x}^* \rangle$ given $\mc{B}$, thanks to the uniqueness result already established in the one-dimensional case.
  For all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \langle \E^{\mc{B}} f, \mb{x}^* \rangle \, \dd\P
    = \Big\langle \int_B \E^{\mc{B}} f \, \dd\P, \mb{x}^* \Big\rangle
    = \Big\langle \int_B f \, \dd\P, \mb{x}^* \Big\rangle
    = \int_B \langle f, \mb{x}^* \rangle \, \dd\P
  \end{equation*}
  using that $\E^{\mc{B}}f$ is a conditional expectation of $f$, and the same argument shows that
  \begin{equation*}
    \int_B \langle \td{\E}^{\mc{B}} f, \mb{x}^* \rangle \, \dd\P = \int_B \langle f, \mb{x}^* \rangle \, \dd\P.
  \end{equation*}
  Thus $\langle \E^{\mc{B}} f, \mb{x}^* \rangle$ and $\langle \td{\E}^{\mc{B}} f, \mb{x}^* \rangle$ are conditional expectations of of $\langle f, \mb{x}^* \rangle$ given $\mc{B}$, establishing \eqref{eq:CE-testing-eq}, and thus proving that $\E^{\mc{B}} f \aseq \td{\E}^{\mc{B}} f$.

  Finally, if $X$ is a complex Banach space, the result follows by considering real and imaginary parts separately.
\end{proof}

Next we will establish existence, positivity, and $L^p$-contractivity of conditional expectations in the scalar-valued case.

\begin{thm}\label{thm:conditional-expectation-existence-scalar}
  Let $(\Omega,\mc{A},\P)$ be a probability space.
  For any $f \in L^1(\mc{A})$ and any $\sigma$-subalgebra $\mc{B} \subset \mc{A}$, a conditional expectation $\E^{\mc{B}}f$ exists.
  The operator $f \mapsto \E^{\mc{B}}f$ is linear, and for all $p \in [1,\infty]$, $\E^{\mc{B}}$ is a positive contraction on $L^p(\mc{A})$: that is, if $f \in L^p(\mc{A})$, then
  \begin{equation*}
    \|\E^{\mc{B}}f\|_{p} \leq \|f\|_{p},
  \end{equation*}
  and if $f$ is a.s. nonnegative then so is $\E^{\mc{B}}f$.
\end{thm}

\begin{proof}

  We can prove positivity from the defining property \eqref{eq:conditional-expectation-property}, before we establish existence.
  Suppose $f \in L^1(\mc{A})$ is a.s. nonnegative.
  Then for all $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P = \int_B f \, \dd\P \geq 0,
  \end{equation*}
  which implies that the $\mc{B}$-measurable function $\E^{\mc{B}}f$ is a.s. nonnegative.\footnote{This uses an exercise from measure theory: if $g$ is $\mc{B}$-measurable and $\int_B g \geq 0$ for all $\mc{B}$-measurable sets, then $g \geq 0$ a.e.. Proof: the set $N := \{g(\omega) < 0\}$ is $\mc{B}$-measurable, and assuming it has positive measure leads to the contradiction $0 \leq \int_B g < 0$.}
  
  Now fix $p \in [1,\infty]$ and let $f \in L^p(\mc{A})$; we will construct a linear contractive conditional expectation operator $\E^{\mc{B}}$ on $L^p(\mc{B};\K)$ directly.

  \textbf{Mild case: $p > 1$}, so most importantly $p' < \infty$.
  The inclusion map $\map{\iota}{L^{p'}(\mc{B})}{L^{p'}(\mc{A})}$ is  contractive, so (using that $L^p$ is the dual of $(L^{p'})^*$, which requires $p' < \infty$) its adjoint $\map{\E^{\mc{B}} := \iota^*}{L^p(\mc{A})}{L^p(\mc{B})}$ is also contractive.
  For all $f \in L^p(\mc{A})$ and $B \in \mc{B}$ we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P = \langle \iota^* f, \1_{B} \rangle = \langle f, \iota \1_{B} \rangle = \langle f, \1_{B} \rangle = \int_B f \, \dd\P,
  \end{equation*}
  so $\E^{\mc{B}} f \in L^p(\mc{B}) \subset L^1(\mc{B})$ is a conditional expectation of $f$ given $\mc{B}$.


  \textbf{(German) spicy case: $p = 1$.} The difficulty here is that $L^1$ is \emph{strictly} contained in the dual of $L^\infty$, so taking an adjoint of the inclusion $L^\infty(\mc{B}) \to L^\infty(\mc{A})$ is not so straightforward.\footnote{See Exercise \ref{ex:conditional-expectation-as-adjoint}.} 
  Instead we argue by density.
  We have that $L^2(\mc{A})$ is dense in $L^1(\mc{A})$, so we aim to extend the conditional expectation defined above (in the case $p=2$) by continuity.
  For $f \in L^2(\mc{A})$ and $g \in L^\infty(\mc{B})$ we have
  \begin{equation*}
    |\langle \E^{\mc{B}} f, g \rangle| = |\langle f, \iota g \rangle| = |\langle f, g \rangle| \leq \|f\|_1 \|g\|_\infty
  \end{equation*}
  using that $L^\infty(\mc{B}) \subset L^2(\mc{B})$.
  Taking the supremum over all nonzero $g \in L^\infty(\mc{B})$ proves that
  \begin{equation*}
    \|\E^{\mc{B}} f\|_1 \leq \|f\|_1,
  \end{equation*}
  so $\E^{\mc{B}}$ extends to a contraction $L^1(\mc{A}) \to L^1(\mc{B})$.
  For $f \in L^1(\mc{A})$ and $B \in \mc{B}$, using that integration on $B$ is a continuous linear functional on $L^1$, we have
  \begin{equation*}
    \int_B \E^{\mc{B}} f \, \dd\P
    = \lim_{n \to \infty} \int_B \E^{\mc{B}} f_n \, \dd\P
    = \lim_{n \to \infty} \int_B  f_n \, \dd\P
    = \int_B f \, \dd\P
  \end{equation*}
  where $f_n$ is a sequence in $L^2(\mc{A})$ converging to $f$ in $L^1(\mc{A})$.
  Thus $\E^{\mc{B}}f$ is a conditional expectation of $f$ given $\mc{B}$, and we are done.
\end{proof}

Note that the proof of the previous result also establishes the following adjoint relation, which can also be proven directly from the defining property \eqref{eq:conditional-expectation-property} (Exercise \ref{ex:CE-adjoint}).

\begin{prop}\label{prop:CE-adjoint}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$.
  For all $p \in (1,\infty]$, the conditional expectation $\E^{\mc{B}}$ on $L^p(\mc{A})$ is the adjoint of the corresponding conditional expectation on $L^{p'}(\mc{A})$.
\end{prop}

Now we can use the extension theorem for positive operators to show the existence of conditional expectations of Banach-valued random variables.

\begin{prop}
  Let $(\Omega,\mc{A},\P)$ be a probability space, let $\mc{B}$ be a $\sigma$-subalgebra of $\mc{A}$, and let $X$ be a Banach space.
  Then for any $f \in L^1(\mc{A};X)$, a conditional expectation $\E_X^{\mc{B}}f$ of $f$ given $\mc{B}$ exists.
  Furthermore, for all $p \in [1,\infty]$, $\E^{\mc{B}}$ is a contraction on $L^p(\mc{A};X)$.
\end{prop}


\begin{proof}  
  First fix $p \in [1,\infty)$.
  Since the conditional expectation $\E^{\mc{B}}$ is a positive operator on $L^p(\mc{A})$, by Theorem \ref{thm:positive-extensions} it admits a bounded $X$-valued extension, which we denote by $\E_X^{\mc{B}}$.
  Since $\E^{\mc{B}}$ is contractive, so is $\E_X^{\mc{B}}$.
  We just need to show that for all $f \in L^p(\mc{A};X)$, $\E_X^{\mc{B}}f$ is a conditional expectation of $f$ given $\mc{B}$; we will do this by scalarisation.
  For all $B \in \mc{B}$ and all functionals $\mb{x}^* \in X^*$, since the function $\langle f, \mb{x}^* \rangle$ is in $L^1(\mc{A})$, we have
  \begin{equation*}
    \begin{aligned}
      \Big\langle \int_B \E_X^{\mc{B}} f \, \dd\P, \mb{x}^* \Big\rangle
      &= \int_B \langle \E_X^{\mc{B}} f, \mb{x}^* \rangle \, \dd\P \\
      &\stackrel{(*)}{=} \int_B \E^{\mc{B}} (\langle f, \mb{x}^* \rangle) \, \dd\P
      = \int_B \langle f, \mb{x}^* \rangle \, \dd\P
      = \Big\langle \int_B f \, \dd\P, \mb{x}^* \Big\rangle.
    \end{aligned}
  \end{equation*}
  (see Exercise \ref{ex:tensor-extension-basic-props} for the starred equality).
  Since this holds for all $\mb{x}^* \in X^*$, we have
  \begin{equation*}
     \int_B \E_X^{\mc{B}} f \, \dd\P = \int_B f \, \dd\P,
   \end{equation*}
   which shows that $\E_X^{\mc{B}}$ is a conditional expectation of $f$ given $\mc{B}$.

   Now we establish the result for $p = \infty$: let $f \in L^\infty(\mc{A};X) \subset L^2(\mc{A};X)$.
   Then a conditional expectation $\E_X^{\mc{B}} f$ of $f$ given $\mc{B}$ is defined as an element of $L^2(\mc{B};X)$: we just need to show that $\|\E_X^{\mc{B}} f\|_\infty \leq \|f\|_\infty$.
   We can test this by duality using Proposition \ref{prop:bochner-preduality} and that $L^2(\mc{B}; X^*)$ is dense in $L^1( \mc{B}; X^*)$.
   
   For all $g \in L^2(\mc{B}; X^*)$, since the operator $\E^{\mc{B}} \in \Lin(L^2(\mc{A}))$ is self-adjoint, we have
\begin{equation*}
  \begin{aligned}
    | \langle \E_X^{\mc{B}} f, g \rangle |
    =  | \langle \td{\E^{\mc{B}}} f, g \rangle | 
    &\stackrel{(*)}{=} | \langle f, \td{\E^{\mc{B}}} g \rangle | \\
    &\leq  \|f\|_{L^\infty(\mc{A};X)} \|\E_X^{\mc{B}} g\|_{L^1(\mc{A};X^*)} \\
    &\leq  \|f\|_{L^\infty(\mc{A};X)} \| g\|_{L^1(\mc{A};X^*)}.
  \end{aligned}
\end{equation*}
For the starred equality see Exercise \ref{ex:tensor-adjoint} in the previous chapter, particularly the identity \eqref{eq:tensor-adjoint-identity}.
Taking the supremum over all nonzero $g \in L^2(\mc{B};X^*)$ completes the proof.
\end{proof}


\todo{further properties of vector-valued conditional expectations, as needed}

\begin{prop}\label{prop:CE-measurable-op}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$, and let $X$ and $Y$ be Banach spaces.
  Consider an operator-valued function $T \in L^\infty(\Omega, \mc{B}; \Lin(X,Y))$.
  For $f \in L^1(\Omega, \mc{A}; X)$ define the function $Tf \in L^1(\Omega, \mc{A}; Y)$ by
  \begin{equation*}
    Tf(\omega) := T(\omega) f(\omega).
  \end{equation*}
  Then the identity
  \begin{equation*}
    \E^{\mc{B}}(Tf) = T \E^{\mc{B}}f
  \end{equation*}
  holds.
\end{prop}

\begin{proof}
  Exercise \ref{ex:CE-measurable-op}. %mk
\end{proof}

\begin{rmk}
  We have only considered conditional expectations $\E^{\mc{B}}$ on probability spaces, but the concept can be extended to general measure spaces $(S,\mc{A},\mu)$ provided that the measure $\mu$ is $\sigma$-finite on the $\sigma$-subalgebra $\mc{B} \subset \mc{A}$ (although the arguments require a fair bit of modification).
  This approach is taken in \cite{HNVW16}.
\end{rmk}


\section{Martingales and martingale transforms}

\begin{defn}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, let $(\mc{A}_n)_{n \in \N}$ be a filtration on this space,  and let $X$ be a Banach space.
  \begin{itemize}
  \item
    A stochastic process $(M_n)_{n \in \N}$ with each $M_n \in L^1(\Omega; X)$ is called a \emph{martingale} with respect to $(\mc{A}_n)_{n \in \N}$ if 
    \begin{equation}\label{eq:mgale-defining-property}
      M_n = \E^{\mc{A}_n} M_{n+1} \qquad \forall n \in \N.
    \end{equation}
    Note that in particular each $M_n$ is $\mc{A}_n$-measurable.

  \item
    Let $(M_n)_{n \in \N}$ be a martingale as above.
    The associated \emph{martingale difference sequence} is the process $(dM_n)_{n \in \N}$ in $L^1(\Omega; X)$ defined by
    \begin{equation*}
      dM_0 := M_0, \qquad dM_k := M_k - M_{k-1} \quad (k \geq 1)
    \end{equation*}
    (the difference equation can be made to hold for all $k \geq 0$ by defining $M_{-1} := 0$).
  \end{itemize}
\end{defn}

\begin{rmk}
  Now is a good time to complete Exercise \ref{ex:martingale-elementary-properties}, establishing a few elementary properties of martingales. 
\end{rmk}

Martingales are stochastic processes that are `balanced': at time $n$, the best estimate of the state of the process at time $n+1$ is precisely the current state of the process.
They are intricately linked with many topics in harmonic and functional analysis.

\begin{example}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $(\mc{A}_n)_{n \in \N}$ a filtration.
  Let $X$ be a Banach space and $f \in L^1(\Omega;X)$.
  For each $n \in \N$ define $f_n := \E^{\mc{A}_n}f \in L^1(\Omega;X)$.
  Then by the monotonicity property of conditional expectations,
  \begin{equation*}
    \E^{\mc{A}_n} f_{n+1} = \E^{\mc{A}_n} \E^{\mc{A}_{n+1}} f = \E^{\mc{A}_n} f = f_n,
  \end{equation*}
  so $(f_n)_{n \in \N}$ is a martingale.
  This is called the \emph{martingale associated with $f$}.

  When $\Omega = [0,1)$ is the unit interval with Borel $\sigma$-algebra and Lebesgue measure, and when we consider the dyadic filtration $(\mc{F}_n)_{n \in \N}$ as in Example \ref{eg:dyadic-filtration}, the martingale associated with a function $f \in L^1([0,1);X)$ is given by
  \begin{equation*}
    f_n = \sum_{I \in \mc{D}_n} \1_I \otimes \langle f \rangle_I
  \end{equation*}
  where $\mc{D}_n$ is the set of dyadic intervals $I \subset [0,1)$ of length $2^{-n}$ and $\langle f \rangle_I = \fint_I f(t) \, \dd t$ is the average of $f$ on $I$ (these conditional expectations were computed in Example \ref{eg:atomic-CE}).
  Each dyadic interval $I \in \mc{D}_{n-1}$ can be `halved', i.e. $I = I_{-} \cup I_{+}$, where $I_{\pm} \in \mc{D}_{n}$ and $I_-$ is to the left of $I_+$ (i.e. $\sup_{I_-} = \inf_{I_+}$).
  Let's compute the difference $df_n$ on an interval $I \in \mc{D}_{n-1}$:
  \begin{equation*}
    \begin{aligned}
      \1_{I}(df_{n}) &= \1_{I}(f_{n} - f_{n-1}) \\
      &= \1_{I_{-}} \otimes \langle f \rangle_{I_{-}} + \1_{I_{+}} \otimes \langle f \rangle_{I_{+}} - (\1_{-} + \1_{+}) \otimes \langle f \rangle_{I} \\
      &= \1_{I_{-}} \otimes \Big( \frac{2}{|I|}\int_{I_{-}} f - \frac{1}{|I|}\int_{I} f \Big) + \1_{I_{+}} \otimes \Big( \frac{2}{|I|}\int_{I_{+}} f - \frac{1}{|I|}\int_{I} f \Big) \\
      &= \1_{I_{-}} \otimes \Big( \frac{1}{|I|}\int_{I_{-}} f - \frac{1}{|I|}\int_{I_{+}} f \Big) - \1_{I_{+}} \otimes \Big( \frac{1}{|I|}\int_{I_{-}} f - \frac{1}{|I|}\int_{I_{+}} f \Big) \\
      &= h_I \otimes \langle f, h_I \rangle
    \end{aligned}
  \end{equation*}
  where
  \begin{equation*}
    h_I := \frac{1}{|I|^{1/2}} (\1_{I_-} - \1_{I_+})
  \end{equation*}
  is the ($L^2$-normalised) \emph{Haar function} associated with $I \in \mc{D}_{n}$.
  Thus the representation of $f$ in terms of martingale differences corresponds to its Haar expansion (ignoring the issue of whether the sums converge):
  \begin{equation*}
    f = f_0 + \sum_{n \geq 1} df_{n} = \langle f \rangle_{[0,1)} + \sum_{n \geq 1} \sum_{I \in \mc{D}_{n-1}} h_I \otimes \langle f, h_I \rangle. 
  \end{equation*}

\end{example}

The martingale associated with an $L^p$ function has the following fundamental convergence property.
This will be extended to almost sure convergence in Theorem \ref{thm:mgale-pw-conv}. 

\begin{thm}\label{thm:mgale-conv-Lp}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $(\mc{A}_{n})_{n \in \N}$ a filtration.
  Let $\mc{A}_{\infty} \subset \mc{A}$ be the sub-$\sigma$-algebra generated by $\cup_{n \in \N} \mc{A}_{n}$.
  Let $X$ be a Banach space and $p \in [1,\infty)$.
  Then for all $f \in L^p(\Omega,\P;X)$ we have $\E^{\mc{A}_{n}} f \to \E^{\mc{A}_\infty} f$ with convergence in $L^p$.
\end{thm}

\begin{proof}
  Since $\E^{\mc{A}_{n}} f = \E^{\mc{A}_{n}} \E^{\mc{A}_{\infty}} f$ for all $n \in \N$, it suffices to assume that $f = \E^{\mc{A}_{\infty}} f$, i.e. that $f$ is $\mc{A}_{\infty}$-measurable.
  We will reduce to showing that
  \begin{equation*}
    \bigcup_{n \in \N} L^p(\Omega,\mc{A}_{n},\P;X)
  \end{equation*}
  is dense in $L^p(\Omega,\mc{A}_{\infty},\P;X)$.
  Assuming this is true for the moment, given $\varepsilon > 0$, there exists $n \in \N$ and $g \in L^p(\mc{A}_{n};X)$ such that $\|f - g\|_p < \varepsilon$.
  We also have $\E^{\mc{A}_{m}} g = g$ for all $m > n$, so for all such $m$ we have
  \begin{equation*}
    \begin{aligned}
      \|\E^{\mc{A}_{m}} f - f\|_p
      &\leq \|\E^{\mc{A}_m}(f - g)\|_p + \|\E^{\mc{A}_{m}} g - f\|_p \\
      &\leq 2\|f - g\|_{p} < 2\varepsilon.
    \end{aligned}
  \end{equation*}
  Taking $m \to \infty$ and noting that $\varepsilon$ was arbitrary, we find that $\E^{\mc{A}_{m}} f \to f$ in $L^p$.

  It remains to prove the density statement, and by Exercise \ref{ex:general-density} it suffices to do this in the scalar case $X = \K$.
  Consider the collection of sets
  \begin{equation*}
    \mc{C} := \Big\{A \in \mc{A}_{\infty} : \1_{A} \in \overline{\bigcup_{n \in \N} L^p(\mc{A}_{n})} \Big\}
  \end{equation*}
  where the closure is in $L^p(\mc{A}_{\infty})$.
  Then $\mc{C}$ is a $\sigma$-algebra which contains $\mc{A}_{n}$ for each $n \in \N$, so that $\mc{A}_{\infty} = \mc{C}$, which implies that all $\mc{A}_{\infty}$-simple functions are contained in $\overline{\bigcup_{n \in \N} L^p(\mc{A}_{n})}$, and thus that this closure is $L^p(\mc{A}_{\infty})$.  
\end{proof}

\begin{example}\label{eg:sum-process}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, $X$ a Banach space, and let $(g_n)_{n \in \N}$ be a sequence of random variables in $L^1(\Omega;X)$ which are mutually independent.
  Let $(\mc{F}_n)_{n \in \N}$ be the filtration generated by the process $(g_n)_{n \in \N}$, and for each $n \in \N$ let $\sigma_n := \sum_{m=0}^n g_n$ be the sum of the first $n+1$ random variables.
  Then we have
  \begin{equation*}
    \begin{aligned}
      \E^{\mc{F}_n} \sigma_{n+1} &= \E^{\mc{F}_n}\Big( \sum_{m=0}^ng_m \Big) + \E^{\mc{F}_n} g_{n+1} \\
      &=\sum_{m=0}^n g_m = \sigma_n
    \end{aligned}
  \end{equation*}
  since the random variables $(g_m)_{m=0}^n$ are $\mc{F}_n$-measurable and $g_{n+1}$ is independent of $\mc{F}_n$.\todo{this needs to be an exercise or a note}
  Thus the sum process $(\sigma_n)_{n \in \N}$ is a martingale.
\end{example}

\begin{prop}\label{prop:mgale-transforms}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, $X$ a Banach space, and let $(M_n)_{n \in \N}$ be a martingale (valued in $X$) with respect to a filtration $(\mc{A}_n)_{n \in \N}$.
  Let $Y$ be another Banach space, and let $(T_n)_{n \in \N}$ be a sequence of operators in $L^\infty(\Omega; \Lin(X,Y))$ which is predictable with respect to $(\mc{A}_n)_{n \in \N}$.
  For each $n \in \N$ and $\omega \in \Omega$ define a function $T_n M_n \in L^1(\Omega; Y)$ by
  \begin{equation*}
    (T \cdot M)_n := \sum_{m=0}^n T_m dM_m \in Y.
  \end{equation*}
  Then $(T \cdot M)_{n \in \N}$ is a ($Y$-valued) martingale with respect to $(\mc{A}_n)_{n \in \N}$.
\end{prop}

\begin{proof}
  Integrability of each $(T \cdot M)_n$ follows from that of each $M_m$ and the a.s. uniform boundedness of each $T_m$.
  To see that $(T \cdot M)_{n \in \N}$ is a martingale with respect to $(\mc{A}_n)_{n \in \N}$, it suffices to show\todo{pur a prop. allowing for this} that
  \begin{equation}\label{eq:transform-independent-increments}
    \E^{\mc{A}_n} d(T \cdot M)_{n+1} = 0
  \end{equation}
  for all $n \in \N$.
  By Proposition \ref{prop:CE-measurable-op}, since $T_{n+1}$ is $\mc{A}_n$-measurable by the predictability assumption and $dM_{n+1}$ is independent of $\mc{A}_n$ (Exercise \ref{}), we have 
  \begin{equation*}
    \E^{\mc{A}_n} d(T \cdot M)_{n+1} = \E^{\mc{A}_n} (T_{n+1} dM_{n+1}) = T_{n+1} \E^{\mc{A}_n} dM_{n+1} = 0,
  \end{equation*}
  proving \eqref{eq:transform-independent-increments} as required.
\end{proof}

\begin{example}\label{eg:betting-game-martingale}
  We return once more to our betting game, with notation given in Example \ref{eg:gambling-filtrations}.
  Consider the stochastic process $(\mb{s}_n)_{n \in \N}$ representing the evolution of the state of your wallet: recall that
  \begin{equation*}
    d\mb{s}_{n+1} = \mb{s}_{n+1} - \mb{s}_{n} = \pi_{n+1} \mb{x}_{n+1} \qquad \forall n \in \N,
  \end{equation*}
  where $\pi_{n+1}$ is the outcome of the coin toss and $\mb{x}_{n+1}$ is the vector wagered at time $n+1$.
  The process $(\pi_n)_{n \in \N}$ generates the filtration $(\mc{F}_n)_{n \in \N}$.
  Since the random variables $(\pi_n)_{n \in \N}$ are mutually independent, the sum process $(\sigma_{n})_{n \in \N}$ given by
  \begin{equation*}
    \sigma_{n} := \sum_{m=0}^n \pi_{n}
  \end{equation*}
  is a martingale (see Example \ref{eg:sum-process}).
  Now suppose that the wager vectors $\mb{x}_n \in L^1(\Omega,X)$ are integrable.
  We assumed that this sequence is predictable with respect to the filtration $(\mc{F}_{n})_{n \in \N}$ (meaning that we place the $n$-th bet before seeing the outcome of the $n$-th coin toss), so we can construct the martingale transform $\mb{x} \cdot \sigma$.\footnote{Technically we are identifying $X$ with $\Lin(\C;X)$ here. Given a vector $\mb{y} \in X$, the associated linear operator $\C \to X$ maps $\lambda \in \C$ to $\lambda \mb{y} \in X$.}
  By definition, the difference sequence of $\mb{x} \cdot \sigma$ is
  \begin{equation*}
    d(\mb{x} \cdot \sigma)_{n+1} = \mb{x}_{n+1} d\sigma_{n+1} = \pi_{n+1} \mb{x}_{n+1} = d\mb{s}_{n+1}, 
  \end{equation*}
  and the initial term is
  \begin{equation*}
    (\mb{x} \cdot \sigma)_0 = \mb{x}_{0} \pi_{0} = \mb{s}_{0},
  \end{equation*}
  so we have the equality of martingales $(\mb{s}_{n})_{n \in \N} = ((\mb{x} \cdot \sigma)_{n})_{n \in \N}$.
  That is, the state of your wallet $(\mb{s}_{n})_{n \in \N}$ is a martingale, and it is given by the martingale transform of the sum of coin flips $(\sigma_{n})_{n \in \N}$ by the wager vectors $(\mb{x}_{n})_{n \in \N}$.
\end{example}





\section{Maximal inequalities and pointwise convergence}
% proofs from Pisier with little modifications to allow for infinite martingales


\begin{defn}
  Let $(\Omega, \mc{A}, \P)$ be a probability space, let $(\mc{A}_n)_{n \in \N}$ be a filtration on this space.
  A real-valued stochastic process $(M_n)_{n \in \N}$ with each $M_n \in L^1(\Omega; \R)$ is called a \emph{submartingale} with respect to $(\mc{A}_n)_{n \in \N}$ if 
  \begin{equation}\label{eq:mgale-defining-property}
    M_n \stackrel{\as}{\leq} \E^{\mc{A}_n} M_{n+1} \qquad \forall n \in \N.
  \end{equation}
\end{defn}

As an example, consider a martingale $(f_n)_{n \in \N}$ taking values in a Banach space $X$, with respect to a filtration $(\mc{A}_{n})_{n \in \N}$.
Then by the pointwise estimate \eqref{eq:positive-pw-est} established in the proof of Theorem \ref{thm:positive-extensions} we have for all $n \in \N$ 
\begin{equation*}
  \|f_n\|_X = \|\E^{\mc{A}_n} f_{n+1}\|_X \stackrel{\as}{\leq} \E^{\mc{A}_n} \|f_{n+1}\|_X,
\end{equation*}
so that the sequence $(\|f_n\|_X)_{n \in \N}$ is a submartingale.

\begin{defn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $X$ a Banach space.
  Given an $X$-valued stochastic process $(f_{n})_{n \in \N}$ we define the \emph{maximal function}
  \begin{equation*}
    f^*(\omega) := \sup_{n \in \N} \|f_{n}(\omega)\|_{X} \qquad \forall \omega \in \Omega
  \end{equation*}
\end{defn}

\begin{thm}[Doob's maximal inequalities]\label{thm:doob}
  Let $(\Omega, \mc{A}, \P)$ be a probability space  and let $(f_{n})_{n \in \N}$ be a real-valued submartingale with respect to a filtration $(\mc{A}_{n})_{n \in \N}$.
  Then for all $t > 0$ we have
  \begin{equation*}
    t\P(\{f^* > t\}) \leq   \sup_{n \in \N} \int_{\{f^* > t\}} |f_{n}| \, \dd\P,
  \end{equation*}
  and if $f^* \stackrel{\as}{\geq} 0$ then for all $p \in (1,\infty)$ we have
  \begin{equation*}
    \|f^*\|_{L^p(\Omega)} \leq p' \sup_{n \in \N} \|f_{n}\|_{L^p(\Omega)}.
  \end{equation*}
\end{thm}

\begin{cor}
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $X$ a Banach space,  and let $(f_{n})_{n \in \N}$ be an $X$-valued martingale.
  Then for all $t > 0$ we have
  \begin{equation*}
    t\P(\{f^* > t\}) \leq  \sup_{n \in \N} \|f_{n}\|_{L^1(\Omega;X)},
  \end{equation*}
  and for all $p \in (1,\infty)$ we have
  \begin{equation*}
    \|f^*\|_{L^p(\Omega;X)} \leq p' \sup_{n \in \N} \|f_{n}\|_{L^p(\Omega;X)}.
  \end{equation*}
\end{cor}

\begin{proof}[Proof of Theorem \ref{thm:doob}]
  Fix $t > 0$ and define the stopping time (relative to the filtration $(\mc{A}_{n})_{n \in \N}$)
  \begin{equation*}
    T :=  \inf\{k \in \N : f_k > t\}.
  \end{equation*}
  Then we have
  \begin{equation*}
    \begin{aligned}
      t\P(\{f^* > t\})
      = t\P(\{T < \infty\})
      &= \lim_{N \to \infty} \sum_{k = 0}^{N} t\P(\{T = k\}) \\
      &\leq \lim_{N \to \infty} \Big| \sum_{k = 0}^{N} \int_{\{T = k\}} f_k \, \dd\P \Big| 
      \leq \lim_{N \to \infty} \Big| \sum_{k = 0}^{N} \int_{\{T = k\}} \E^{\mc{A}_{k}} f_N \, \dd\P \Big|
    \end{aligned}
  \end{equation*}
  using that $f_k > t$ on the set $\{T = k\}$ and that $(f_{n})_{n \in \N}$ is a submartingale.
  Since $T$ is a stopping time we have $\{T = k\} \in \mc{A}_{k}$, so for all $N \in \N$ we have
  \begin{equation*}
    \Big| \sum_{k = 0}^{N} \int_{\{T = k\}} \E^{\mc{A}_{k}} f_N \, \dd\P \Big|
    = \Big| \sum_{k = 0}^{N} \int_{\{T = k\}} f_{N} \, \dd\P \Big|
    = \Big| \int_{\{T \leq N\}} f_{N} \, \dd\P \Big|
    \leq \sup_{N \in \N} \int_{\{f^* > t\}} |f_{N}| \, \dd\P 
  \end{equation*}
  proving the first inequality.

  For the second inequality, using that $f^* \stackrel{\as}{\geq} 0$ and supposing $\varepsilon > 0$, for $N$ sufficiently large we have
  \begin{equation*}
    \begin{aligned}
      \|f^*\|_{p}^{p}
      &= \int_0^\infty pt^{p-1} \P(\{f^* > t\}) \, \dd t \\
      &\leq (1 + \varepsilon) \int_0^\infty pt^{p-2} \int_{\{f^* > t\}} f_{N}(\omega) \, \dd\P(\omega) \, \dd t \\
      &= (1 + \varepsilon) \int_{\Omega} f_{N}(\omega) \Big( \int_0^{f^*(\omega)} pt^{p-2} \, \dd t \Big) \, \dd\P(\omega) \\
      &= (1 + \varepsilon) \int_{\Omega} \frac{p}{p-1} f_{N}(\omega) f^*(\omega)^{p-1} \, \dd\P(\omega) \\
      &\leq (1 + \varepsilon) p' \|f_{N}\|_{p} \|f^*\|_{p}^{p-1}
    \end{aligned}
  \end{equation*}
  using H\"older's inequality in the last step.
  Dividing through by $\|f^*\|_{p}^{p-1}$ yields
  \begin{equation*}
    \|f^*\|_{p} \leq (1 + \varepsilon) p' \sup_{n \in \N}\|f_{n}\|_{p}
  \end{equation*}
  for all $\varepsilon > 0$, which completes the proof.
\end{proof}

% i think we can do without this
% but include it if necessary
% \begin{thm}[Burkholder--Davis--Gundy inequality]
% \end{thm}

A general principle says that convergence in $L^p$ plus $L^p$-boundedness of the appropriate maximal function implies almost everywhere convergence.
Here is the consequence for martingales.

\begin{thm}\label{thm:mgale-pw-conv} 
  Let $(\Omega, \mc{A}, \P)$ be a probability space and $(\mc{A}_{n})_{n \in \N}$ a filtration.
  Let $\mc{A}_{\infty} \subset \mc{A}$ be the sub-$\sigma$-algebra generated by $\cup_{n \in \N} \mc{A}_{n}$.
  Let $X$ be a Banach space and $p \in [1,\infty)$.
  Then for all $f \in L^p(\Omega,\P;X)$ we have $\E^{\mc{A}_{n}} f \to \E^{\mc{A}_\infty} f$ almost surely.
\end{thm}

\begin{proof}
  See Exercise \ref{ex:mgale-conv}.
\end{proof}

Note that this theorem says that the martingale $(f_n)_{n \in \N} = (\E^{\mc{A}_n} f)_{n \in \N}$ associated with an $\mc{A}_{\infty}$-measurable function $f \in L^p(\Omega;X)$ converges almost surely to $f$.

Of course, this martingale is $L^p$-bounded in the following sense:

\begin{defn}
  An $X$-valued stochastic process $(f_n)_{n \in \N}$ on a probability space $(\Omega,\mc{A},\P)$ is called \emph{$L^p$-bounded} if
  \begin{equation*}
    \sup_{n \in \N} \|f_n\|_{L^p(\Omega;X)} < \infty.
  \end{equation*}
\end{defn}

This raises an important question: given a general $L^p$-bounded $X$-valued martingale $(f_n)_{n \in \N}$, does it automatically hold that $f_n = \E^{\mc{A}_n} f$ for some $f \in L^p(\Omega;X)$?
The answer turns out to depend on the geometry of $X$, and we will discuss this in the next section.
For now we will quickly settle the scalar case.
When $p=1$ we need an additional condition to guarantee relative weak compactness.

\begin{defn}\label{defn:UI}
  Let $(\Omega,\mc{A},\P)$ be a probability space.
  A bounded subset $\mc{F} \subset L^1(\Omega)$ is \emph{uniformly integrable} (or \emph{equi-integrable}) if for all $\varepsilon > 0$, there exists $\delta > 0$ such that
  \begin{equation*}
    \P(A) < \delta \Rightarrow \sup_{f \in \mc{F}} \int_{A} \|f(\omega)\|_X \, \dd\P(\omega) < \varepsilon \qquad \forall A \in \mc{A}.
  \end{equation*}
  For a Banach space $X$, we say that an $X$-valued stochastic process $(f_n)_{n \in \N}$ is uniformly integrable if the set $\{\|f_n\|_{X} : n \in \N\} \subset L^1(\Omega)$ is uniformly integrable. 
\end{defn}

A bounded subset $\mc{F} \subset L^1(\Omega)$ is uniformly integrable if and only if it is weakly relatively compact (see \cite[Theorem 5.2.9]{AK06}).
For $p \in (1,\infty)$, since $L^p(\Omega)$ is reflexive, every bounded subset $\mc{F} \subset L^p(\Omega)$ is weakly relatively compact (see Corollary \ref{cor:reflexive-iff-weakcpt} of the Banach--Alaoglu theorem).
In both cases, the Eberlein--Smulian theorem (Theorem \ref{thm:eberlein-smulian}) says that every bounded sequence in $L^p(\Omega)$ (with the additional assumption of uniform integrability if $p=1$) has a convergent subsequence.
We use this to prove the following theorem.

\begin{thm}\label{thm:mgale-cv-repn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and fix $p \in [1,\infty)$.
  Let $(f_n)_{n \in \N}$ be an $L^p$-bounded scalar-valued martingale with respect to a filtration $(\mc{A}_n)_{n \in \N}$.
  If $p = 1$, suppose furthermore that $(f_n)_{n \in \N}$ is uniformly integrable.
  Then there exists a function $f_\infty \in L^p(\Omega,\mc{A},\P)$ such that $f_n = \E^{\mc{A}_n} f_\infty$ for all $n \in \N$.
\end{thm}

Note that by Theorems \ref{thm:mgale-conv-Lp} and \ref{thm:mgale-pw-conv} we have $f_n \to f_\infty$ almost surely and in $L^p$, so a corollary is that the martingale $(f_n)$ is a.s. convergent.

\begin{proof}
  By the discussion above, there is a subsequence $(f_{n_k})_{k \in \N}$ which converges weakly to a limit $f_\infty \in L^p(\Omega)$.
  For all $n \in \N$ and $A \in \mc{A}_n$,
  \begin{equation*}
    \int_A f_\infty \, \dd\P = \lim_{k \to \infty} \int_A f_{n_k} \, \dd\P,
  \end{equation*}
  and whenever $k$ is so large that $n_k \geq n$ we have
  \begin{equation*}
    \int_A f_{n_k} \, \dd\P = \int_A \E^{\mc{A}_n} f_{n_k} \, \dd \P = \int_A f_{n} \, \dd\P
  \end{equation*}
  by the martingale property.
  Thus for all $A \in \mc{A}_n$ we have
  \begin{equation*}
    \int_A f_\infty \, \dd\P = \int_A f_n,
  \end{equation*}
  which implies that $f_n = \E^{\mc{F}_n} f_\infty$.
\end{proof}

\begin{rmk}\label{rmk:mgale-cv-p1}
  In fact, when $p = 1$, the assumption of uniform integrability can be removed, and the following is true: every scalar-valued martingale with $\sup_{n \in \N} \|f_n\|_1 < \infty$ converges almost everywhere (but not necessarily in $L^1$).
  We'll skip the proof of this result so as to spend more time with Banach spaces, but if you're interested see \cite[Theorem 1.34]{gP16}.
  The proof isn't particularly difficult.
\end{rmk}

Later on we are going to need a corresponding result for submartingales.

\begin{thm}\label{thm:submartingale-convergence}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $(f_n)_{n \in \N}$ a (real-valued) submartingale on $\Omega$ which is $L^1$-bounded and uniformly integrable.
  Then $f_n$ converges almost surely and in $L^1$.
\end{thm}

\begin{proof}
  \todo{come back to this proof. Pisier Theorem 1.36}
\end{proof}

\section{Martingale convergence as a Banach space property}

With Theorem \ref{thm:mgale-cv-repn} as inspiration, we make the following definition.

\begin{defn}
  For $p \in [1,\infty]$, we say that a Banach space $X$ has the \emph{$p$-martingale convergence property} (or \emph{$p$-MCP}) if every $X$-valued martingale which is uniformly bounded in $L^p$ (and uniformly integrable, when $p=1$) converges almost surely.
  We say that $X$ has this property with respect to a given probability space $(\Omega,\mc{A},\P)$ if it holds for every $X$-valued martingale on this space.
\end{defn}

\begin{rmk}
  This is not a standard definition, because it turns out to be equivalent to the more familiar \emph{Radon--Nikodym property}, which we will discuss in the next chapter. But for now, it will help to give the property a more martingaley name.
\end{rmk}

Formally, the $1$-MCP is the strongest of these properties, and the $\infty$-MCP is the weakest.
As stated in the remark above, it will turn out that these properties are equivalent, but we don't know that yet.
For the moment we will investigate the $p$-MCP na\"ively, without invoking this equivalence.

\begin{prop}
  Let $1 \leq p < q \leq \infty$.
  If a Banach space $X$ has the $p$-MCP, then it also has the $q$-MCP.
\end{prop}

\begin{proof}
  For $p > 1$ this follows from the continuous inclusion $L^q(\Omega) \subset L^p(\Omega)$ for probability spaces $\Omega$: an $L^q$-bounded martingale is also $L^p$-bounded, and one can then invoke the $p$-MCP to derive the $q$-MCP.
  For $p = 1$ the same argument applies once we show that a bounded subset $\mc{F} \subset L^q(\Omega)$ is uniformly integrable.
  To see this, for all $f \in \mc{F}$ and measurable $A \subset \Omega$ use H\"older's inequality to estimate
  \begin{equation*}
    \int_{A} |f(\omega)| \, \dd\P(\omega) \leq \|f\|_{p} \P(A)^{1/p'}.
  \end{equation*}
  Thus for all $\varepsilon > 0$, if $\P(A) < (\varepsilon/\sup\{\|f\|_p : f \in \mc{F}\})^{p'}$ then
  \begin{equation*}
    \int_{A} |f(\omega)| \, \dd\P(\omega) < \varepsilon,
  \end{equation*}
  so $\mc{F}$ is uniformly integrable.
  
\end{proof}


Remark \ref{rmk:mgale-cv-p1} says that the scalar field $\K$ has the $1$-MCP, and arguing coordinatewise shows that every finite-dimensioal Banach space also has this property.
In the following two examples we will show that the Banach spaces $c_0$ and $L^1(\Omega)$ do not have the $\infty$-MCP (and hence do not have the $p$-MCP for any $p \in [1,\infty]$). 

\begin{example}\label{eg:c0-noMCP}
  Let $\{-1,1\}$ be equipped with the uniform probability measure and let $\Omega = \{-1,1\}^{\N}$ have the product measure.
  Let $\map{\pi_n}{\Omega}{\{-1,1\}}$ be the $n$-th coordinate function.
  Consider the Banach space $c_0$ of scalar-valued sequences $(a_n)_{n \in \N}$ satisfying $\lim_{n \to \infty} a_n = 0$, equipped with the $\ell^\infty$-norm.
  Let $(\mb{e}_n)_{n \in \N}$ be the canonical basis of $c_0$, i.e.
  \begin{equation*}
    \mb{e}_n(m) = \begin{cases} 1 & m=n \\ 0 & m \neq n. \end{cases}
  \end{equation*}
  Define a $c_0$-valued martingale $(f_n)_{n \in \N}$ with respect to the filtration generated by $(\pi_n)_{n \in \N}$ by
  \begin{equation*}
    f_n := \sum_{k = 0}^{n} \pi_k \otimes \mb{e}_k.
  \end{equation*} 
  (As shown in Example \ref{eg:betting-game-martingale}, this is a martingale; in fact, it is just a special case of our `betting game', in which one wagers the vector $\mb{e}_n$ at time $n$, regardless of the previous outcomes.)
  Then we have
  \begin{equation*}
    \|f_n\|_{L^\infty(\Omega;c_0)} = \esssup_{\omega \in \Omega} \|(\pi_k(\omega))_{k = 0}^n\|_{c_0} = 1,
  \end{equation*}
  so the martingale $(f_n)_{n \in \N}$ is uniformly bounded in $L^\infty$.
  But for all $\omega$ in $\Omega$ and all $n < m$, we have
  \begin{equation*}
    \|f_n(\omega) - f_m(\omega)\|_{c_0} = \max_{n < k \leq m} |\pi_k(\omega)| = 1,
  \end{equation*}
  so the sequence $(f_n(\omega))_{n \in \N}$ is not Cauchy in $c_0$ and hence not convergent.
\end{example}

\begin{example}\label{eg:L1-noMCP}
  With the notation of the previous example, let $X = L^1(\Omega)$ and consider the $X$-valued martingale
  \begin{equation}\label{eq:L1-example-mgale}
    f_n(\omega) := \prod_{k \leq n} (1 + \pi_{k}(\omega)\pi_k)
  \end{equation}
  (Exercise \ref{ex:mgale-check} asks you to show that this is indeed a martingale).
  For all $\omega \in \Omega$ and $n \in \N$ we have
  \begin{equation*}
    \begin{aligned}
      \|f_n(\omega)\|_{X} &= \int_{\Omega} \prod_{k \leq n} (1 + \pi_k(\omega)\pi_k(\eta))  \, \dd\P(\eta) \\
      &= \prod_{k \leq n} \int_{\Omega}  (1 + \pi_k(\omega)\pi_k(\eta)) \, \dd\P(\eta)
      = 1
    \end{aligned}
  \end{equation*}
  by mutual independence of the variables $\pi_k$, so the martingale $(f_n)$ is uniformly bounded in $L^\infty$.
  However, we also have
  \begin{equation*}
    \begin{aligned}
      &\|f_n(\omega) - f_{n+1}(\omega)\|_{X} \\
      &= \int_{\Omega} \Big| \Big(1 - (1 + \pi_{n+1}(\omega)\pi_{n+1}(\eta))\Big)\prod_{k \leq n} (1 + \pi_k(\omega)\pi_k(\eta)) \Big|\, \dd\P(\eta) \\
      &= \int_{\Omega} |\pi_{n+1}(\omega) \pi_{n+1}(\eta)| \, \dd\P(\eta) \prod_{k \leq n} \int_{\Omega} 1 + \pi_k(\omega)\pi_k(\eta) \, \dd\P(\eta) = 1,
    \end{aligned}
  \end{equation*}
  which shows that the sequence $(f_n(\omega))_{n \in \N}$ cannot be Cauchy in $X$, and hence is not convergent.
\end{example}

It is not surprising that the Banach spaces $c_0$ and $L^1(\Omega)$ fail the $\infty$-MCP, as these are classically `bad' spaces.
Most spaces that arise in practise are better behaved.

\begin{thm}\label{thm:MCP-sepdual}
  If $X$ is a separable dual space (i.e. $X$ is separable and $X = Y^*$ for some Banach space $Y$), then $X$ has the $\infty$-MCP.
\end{thm}

\begin{proof}
  Let $(f_n)_{n \in \N}$ be an $X$-valued martingale (on a probability space $\Omega$) which is uniformly bounded in $L^\infty$.
  By homogeneity we may assume without loss of generality that each $f_n$ is valued in the closed unit ball of $X$, which by Banach--Alaoglu is weak-* compact.
  For each $\omega \in \Omega$, let $f(\omega)$ be a weak-* limit point of the sequence $(f_n(\omega))_{n \in \N}$.
  
  Since $X = Y^*$ is separable, so is $Y$, and we can choose a countable dense subset $D \subset \overline{B_{Y}}$ of the unit ball of $Y$.
  For each $\mb{y} \in D$, the scalar-valued martingale $(\langle \mb{y}, f_n \rangle)_{n \in \N}$ is uniformly bounded in $L^\infty$ and thus converges a.s. to a limit (Theorem \ref{thm:mgale-cv-repn}), which must be $\langle \mb{y}, f \rangle$.
  For each $n \in \N$ and $\mb{y} \in D$ let $N_{\mb{y}} \subset \Omega$ denote the null set on which this convergence fails, let $N = \bigcup_{\mb{y} \in D} N_{\mb{y}}$ so that $\P(N) = 0$ (since $D$ is countable), and observe that
  \begin{equation*}
    \langle \mb{y}, f_n(\omega) \rangle \to \langle \mb{y}, f(\omega) \rangle \qquad \forall \omega \in \Omega \sm N \quad \forall \mb{y} \in D.
  \end{equation*}
  Since $D$ is dense in the unit ball of $Y$ we have this convergence (away from $N$) for all $\mb{y} \in Y$, and since $Y$ is weak-* dense in $Y^{**}$\todo{add Goldstine's theorem to the appendix} we have
    \begin{equation*}
      \langle f_n(\omega), \mb{y}^{**} \rangle \to \langle f(\omega), \mb{y}^{**} \rangle \qquad \forall \omega \in \Omega \sm N \quad \forall \mb{y}^{**} \in Y^{**} = X^{*}.
  \end{equation*}
  Since the functions $f_n$ are all measurable, this shows that $\map{f}{\Omega \sm N}{X}$ is weakly measurable, and since $X$ is separable, Pettis tells us that $f$ is strongly measurable.

  It remains to show that $f_n \to f$ on $\Omega \sm N$ in the norm topology on $X$.
  For all $\omega \in \Omega \sm N$ and $\mb{x} \in X$ we have, using that for each $\mb{y} \in D$ the scalar-valued martingale $\langle \mb{y}, \mb{x} - f_n \rangle$ converges to $\langle \mb{y}, \mb{x} - f \rangle$ on $\Omega \sm N$, 
  \begin{equation*}
    \|\mb{x} - f_n(\omega)\|_{X}
    = \sup_{\mb{y} \in D} |\langle \mb{y}, \mb{x} - f_n(\omega) \rangle| 
    = \sup_{\mb{y} \in D} |\E^{\mc{A}_n} (\langle \mb{y}, \mb{x} - f \rangle)(\omega) | 
    \leq \E^{\mc{A}_n}( \|\mb{x} - f\|_{X})(\omega)
  \end{equation*}
  using \eqref{eq:positive-pw-est}.
  Since the function $\|\mb{x} - f\|_{X}$ is bounded and measurable, we have
  \begin{equation*}
    \limsup_{n \to \infty} \|\mb{x} - f_n(\omega)\|_{X} \leq \limsup_{n \to \infty} \E^{\mc{A}_n}( \|\mb{x} - f\|_{X})(\omega) = \|\mb{x} - f(\omega)\|_{X}.
  \end{equation*}
  Now taking $\mb{x} = f(\omega)$ shows that $f_n(\omega) \to f(\omega)$ in $X$.
  The proof is complete.  
\end{proof}

So separable dual spaces have the $\infty$-MCP: this includes the spaces $L^p([0,1])$ for $p \in (1,\infty)$ as well as the sequence space $\ell^1$ (which is the dual of $c_0$).
The sequence space $\ell^\infty$ contains $c_0$, which does not have $p$-MCP for any $p$ as shown above, so $\ell^\infty$ doesn't have any of these properties either.
Of course, $\ell^\infty$ is a dual space, but it isn't separable.

\begin{lem}\label{lem:MCP-sepdet}
  For all $p \in [1,\infty]$, the $p$-MCP is separably determined: that is, a Banach space $X$ has the $p$-MCP if and only if every separable closed subspace $Y \subset X$ has the $p$-MCP.
\end{lem}

\begin{proof}
  The `only if' direction is immediate, as a $Y$-valued martingale can be seen as an $X$-valued martingale, and if a martingale converges a.s. in $X$, then since $Y$ is closed it must also converge a.s. in $Y$.

  On the other hand, suppose that every separable closed subspace $Y \subset X$ has the $p$-MCP, and let $(f_n)$ be an $X$-valued martingale which is uniformly bounded in $L^p$.
  Each $f_n$ is strongly measurable and hence separably-valued by the Pettis theorem (Theorem \ref{thm:Pettis-measurability}), so there is a sequence of separable closed subspaces $Y_n \subset X$ such that $f_n$ takes values in $Y_n$.
  The union of these spaces generates a separable closed subspace $Y$.
  The martingale $(f_n)_{n \in \N}$ then takes values in $Y$, and since $Y$ has the $p$-MCP by assumption, $(f_n)_{n \in \N}$ is a.s. convergent in $Y$, hence also in $X$.
\end{proof}

\begin{cor}\label{cor:MCP-reflexive}
  If $X$ is reflexive, then $X$ has the $\infty$-MCP.
\end{cor}

\begin{proof}
  By the previous lemma, it suffices to show that every separable closed subspace $Y \subset X$ has the $\infty$-MCP, and by Theorem \ref{thm:MCP-sepdual} we just need to show that every such $Y$ is a dual space.
  
  Consider the annihilator
  \begin{equation*}
    Y^{\perp} := \{\mb{x}^* \in X^* :  \text{$\langle \mb{y}, \mb{x}^* \rangle = 0$ for all $y \in Y$}\},
  \end{equation*}
  and the double annihilator
  \begin{equation*}
    Y^{\perp\perp} = (Y^\perp)^\perp = \{\mb{x}^{**} \in X^{**} : \text{$\langle \mb{z}, \mb{x}^{**} \rangle = 0$ for all $z \in Y^\perp$}\}.
  \end{equation*}
  By Proposition \ref{prop:duality-subspace-quotient} we know that $Y^{\perp\perp}$ is isometrically isomorphic to the dual space $X^*/Y^\perp$, so it suffices to show that $j(Y) = Y^{\perp\perp}$, where $\map{j}{X}{X^{**}}$ is the canonical inclusion.
  The containment $j(Y) \subset Y^{\perp\perp}$ is a direct consequence of the definition.
  To show the reverse inclusion, suppose that $\mb{x}^{**} \notin j(Y)$; we will conclude that $\mb{x}^{**} \notin Y^{\perp\perp}$.
  To do this we need to find a functional $\mb{x}^* \in Y^{\perp}$ such that $\langle\mb{x}^*, \mb{x}^{**}\rangle \neq 0$.
  Since $X$ is reflexive, $\mb{x}^{**} = j(\mb{x})$ for some $\mb{x} \in X \sm Y$.
  By Hahn--Banach there exists a functional $\mb{x}^* \in X^*$ such that $\langle \mb{x}, \mb{x}^* \rangle = 1$ and $\mb{x}^* \in Y^\perp$.
  Since $\langle \mb{x}, \mb{x}^* \rangle = \langle \mb{x}^*, j(\mb{x}) \rangle = \langle \mb{x}^*, \mb{x}^{**} \rangle$, the functional $\mb{x}^*$ does exactly what we want.
\end{proof}

Thus reflexive spaces have the $\infty$-MCP, even if they are not separable.


\section*{Exercises}

\begin{exercise}
  Let $(f_n)_{n \in \N}$ be a stochastic process on a probability space $(\Omega,\mc{A},\P)$.
  Suppose that $(f_n)$ is predictable with respect to the filtration generated by $(f_n)$ (see Example \ref{eg:filtration-generated-by-process}).
  Show that the process is deterministic, in the sense that each $f_n$ is constant.
\end{exercise}

\begin{exercise}\label{ex:winnings-unpredictability}
  In the setting of Example \ref{eg:gambling-filtrations}, show that the random variable $\mb{s}_{n+1}$ is $\mc{F}_n$-measurable if and only if $\mb{x}_{n+1} \equiv 0$.
\end{exercise}

\begin{exercise}\label{ex:gambling-in-linfty}
  This exercise takes place in the setting of Example \ref{eg:gambling-stoppingtimes}.
  \begin{itemize}
  \item
    Let $X = \ell^\infty(\N)$.
    Suppose that the wager vectors $\map{\mb{x}_n}{\Omega}{\ell^\infty(\N)}$ are such that for all $\omega \in \Omega$, the vectors $(\mb{x}_n(\omega))_{n \in \N}$ are pairwise distinct standard basis vectors (i.e. $\{0,1\}$-valued sequences, zero for all but one index).
    Fix $\lambda > 0$ and let $K = \{\mb{a} \in \ell^\infty(\N) : \|\mb{a}\|_\infty \geq \lambda\} = \ell^\infty(\N) \sm B_\lambda(0)$.
    Show that the stopping time
    \begin{equation*}
      T_K(\omega) := \inf\{n \in \N : \mb{s}_n(\omega) \in K\} 
    \end{equation*}
    is finite if and only if $\lambda \leq 1$.
  \item
    As above, but now let $X = \ell^2(\N)$, and show that the stopping time $T_K$ is finite for all $\lambda > 0$.
  \end{itemize}
\end{exercise}

\begin{exercise}\label{ex:CE-adjoint}
  Use the defining property \eqref{eq:conditional-expectation-property} of conditional expectations (i.e. do not use details of its construction) to prove Proposition \ref{prop:CE-adjoint}.
\end{exercise}

\begin{exercise}\label{ex:conditional-expectation-as-adjoint}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $\mc{B}$ a $\sigma$-subalgebra of $\mc{A}$.
  Using that $L^1(\mc{A}) \subsetneq L^\infty(\mc{A})^*$, show that the adjoint of the inclusion map $\map{\iota}{L^\infty(\mc{B})}{L^\infty(\mc{A})}$, which \emph{a priori} maps $L^1(\mc{A}) \to L^\infty(\mc{B})^* \supsetneq L^1(\mc{B})$, actually maps into $L^1(\mc{B})$ \emph{without invoking the existence of a conditional expectation operator on $L^1$.}
\end{exercise}

\begin{exercise}\label{ex:CE-measurable-op}
  Prove Proposition \ref{prop:CE-measurable-op}.
\end{exercise}

\begin{exercise}\label{ex:martingale-elementary-properties}
  Let $(\Omega,\mc{A},\P)$ be a probability space, $X$ a Banach space, and let $(M_n)_{n \in \N}$ be a martingale with respect to some filtration $(\mc{A}_n)_{n \in \N}$.
  \begin{itemize}
  \item
    Show that $M_n = \E^{\mc{A}_n} M_m$ for all $n,m \in \N$ with $m > n$,
  \item
    Let $(\mc{F}_n)_{n \in \N}$ be the filtration generated by $(M_n)_{n \in \N}$, i.e.
    \begin{equation*}
      \mc{F}_n := \sigma(M_0, M_1, \ldots, M_n).
    \end{equation*}
    Show that $(M_n)_{n \in \N}$ is a martingale with respect to $(\mc{F}_n)_{n \in \N}$.
  \item
    For all $p \in [1,\infty]$, show that the sequence $\|M_n\|_{L^p(\Omega,\P;X)}$ is monotonically increasing in $n$.
  \end{itemize}
  \todo{add: martingales in bijective correspondence with their difference sequences}
  \todo{add: martingales = independent increments}
\end{exercise}

\begin{exercise}\label{ex:mgale-conv}
  Prove the pointwise convergence theorem for martingales, Theorem \ref{thm:mgale-pw-conv}, as a consequence of Doob's maximal inequality and the $L^p$-convergence theorem (Theorem \ref{thm:mgale-conv-Lp}).
\end{exercise}

\begin{exercise}\label{ex:UI-characterisation}
  Let $(\Omega,\mc{A},\P)$ be a probability space.
  Show that a bounded subset $\mc{F} \subset L^1(\Omega)$ is uniformly integrable if and only if
  \begin{equation*}
    \lim_{t > 0} \sup_{f \in \mc{F}} \int_{|f| > t} |f(\omega)| \, \dd\P(\omega) = 0.
  \end{equation*}
\end{exercise}


\begin{exercise}\label{ex:mgale-check}
  Show that the $L^1(\Omega;P)$-valued stochastic process defined in \eqref{eq:L1-example-mgale} is a martingale. %mk
\end{exercise}

\begin{exercise}\label{ex:L1-noMCP-var}
  Modify Example \ref{eg:L1-noMCP} to show that $L^1([0,1])$ does not have the $\infty$-martingale convergence property. (Do not simply use that $L^1([0,1])$ is isometrically isomorphic to $L^1(\Omega)$---construct a `bad' martingale directly.)
\end{exercise}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
