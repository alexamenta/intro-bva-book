In many applications of Banach-valued analysis, the most natural assumption to make on the target Banach space is the \emph{UMD property}.
This can be characterised in various ways, but the characterisation which gives it its name is the unconditionality of difference sequences of martingales valued in the Banach space.
In this chapter we will consider `basic' characterisations and implications of this property.
In the next chapter, which is probably the most important in terms of applications, we will discuss consequences for Fourier multiplier theorems and Littlewood--Paley theory.

\section{Definition and first examples}

Recall that the difference sequence of a stochastic process $\mb{f}_{\bullet}$ is the stochastic process $d\mb{f}_{\bullet}$ defined by $d\mb{f}_{n} := \mb{f}_{n} - \mb{f}_{n-1}$, with $\mb{f}_{-1} := \mb{0}$.

\begin{defn}
  For $p \in (1,\infty)$ a Banach space $X$ is said to have the \emph{$\UMD_{p}$ property} (Unconditionality of Martingale Differences in $L^p$) if there exists a constant $C < \infty$ such that for all probability spaces $(\Omega,\mc{A},\P)$, all $X$-valued $L^p$-bounded martingales $\mb{f}_{\bullet}$, and all sequences of signs $\xi_n = \pm 1,$
  \begin{equation}\label{eq:UMD-property}
    \sup_{N \in \N} \Big\| \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)} \leq C \sup_{N \in \N} \Big\| \sum_{n=0}^{N} d\mb{f}_{n}\Big\|_{L^p(\Omega;X)}.
  \end{equation}
  The best possible constant $C$ in this inequality is denoted $\beta_{p}(X)$.
  We say that $X$ has the \emph{UMD property}\index{UMD property} if it has the $\UMD_{p}$ property for all $p \in (1,\infty)$.
\end{defn}

\begin{rmk}
  We will see in Section \ref{sec:UMD-p-independence} that the $\UMD_{p}$ property for some $p \in (1,\infty)$ implies the $\UMD$ property, so (as with the $p$-martingale convergence properties) there is no need to isolate $\UMD_{p}$ as a separate property other than for pedagogical reasons.
\end{rmk}

In Banach space lingo, \eqref{eq:UMD-property} says that the sequence $d\mb{f}_\bullet$ in $L^p(\Omega;X)$ is \emph{unconditional}\index{unconditional sequence}, or equivalently, that the convergence (or divergence) in $L^p(\Omega;X)$ of the series $\sum_{n \in \N} d\mb{f}_n$ is independent of the order of summation.\footnote{For a detailed discussion of this point see for example \cite[Section 4.1.b]{HNVW16} or \cite[Section 1.c]{LT77}.}
Put very coarsely, the UMD property says that martingale differences behave somewhat like orthogonal sequences in $L^p(\Omega;X)$.
In fact, when $X$ is a Hilbert space, the $\UMD_{2}$ property follows directly from orthogonality considerations.

\begin{prop}\label{prop:Hilbert-UMD2}\index{UMD property!of Hilbert spaces}
  Every Hilbert space $H$ has the $\UMD_{2}$ property, with best constant $\beta_{2}(H) = 1$.
\end{prop}

\begin{proof}
  Let $\mb{f}_{\bullet}$ be an $H$-valued martingale on a probability space $(\Omega,\mc{A},\P)$.
  Then for all sign sequences $\xi_{\bullet}$ and all $N \in \N$, since the differences $d\mb{f}_{n}$ are independent and hence pairwise orthogonal, we have
  \begin{equation*}
    \Big\| \sum_{n=0}^N \xi_{n} d\mb{f}_{n} \Big\|_{L^2(\Omega;H)}^{2}
    = \sum_{n = 0}^{N} \xi_{n}^{2} \|d\mb{f}_{n}\|_{L^2(\Omega;H)}^{2}
    = \sum_{n = 0}^{N} \|d\mb{f}_{n}\|_{L^2(\Omega;H)}^{2}
    = \Big\| \sum_{n=0}^N d\mb{f}_{n} \Big\|_{L^2(\Omega;H)}^{2}.
  \end{equation*}
  Thus we have equality in \eqref{eq:UMD-property}, with constant $C = 1$.
\end{proof}

The UMD property can be rephrased succinctly in terms of \emph{martingale sign transforms}.\index{martingale transforms}
Given a finite martingale $(\mb{f}_{n})_{n=0}^{N}$ valued in a Banach space $X$, and given a finite sequence of signs $(\xi_{n})_{n=0}^{N}$, recall that we defined the transformed martingale $(\xi \cdot \mb{f})_{\bullet}$ in terms of its difference sequence,
\begin{equation*}
  d(\xi \cdot \mb{f})_{n} := \xi_{n} d\mb{f}_{n} \qquad \text{(or equivalently, $(\xi \cdot \mb{f})_{k} = \sum_{n=0}^{k} \xi_{n} d\mb{f}_{n}$.)}
\end{equation*}
The $\UMD_{p}$ property can then be restated as follows: for all finite martingales $(\mb{f}_{n})_{n=0}^{N}$ and all finite sign sequences $(\xi_{n})_{n=1}^{N}$, we have a bound
\begin{equation*}
  \|(\xi \cdot \mb{f})_{N}\|_{L^p(\Omega;X)} \lesssim \|\mb{f}_{N}\|_{L^p(\Omega;X)}.
\end{equation*}
This can be restated in terms of filtrations.
Given a finite filtration $(\mc{A}_{n})_{n=0}^{N}$ on a probability space $(\Omega,\mc{A},\P)$ and a finite sequence of signs $(\xi_{n})_{n=0}^{N}$, let $T_{\mc{A}_{\bullet}, \xi_{\bullet}}$ denote the bounded operator acting on $L^1(\Omega;X)$ by
\begin{equation*}
  T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f} := \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n} = (\xi \cdot \mb{f})_{N},
\end{equation*}
where $\mb{f}_{\bullet}$ is the finite martingale associated with $\mb{f}$ and $\mc{A}_{\bullet}$.
At this point, we have already given all the necessary details to prove the following convenient characterisation of the UMD property.

\begin{prop}\label{prop:UMD-signtransforms}
  Let $X$ be a Banach space.
  Then the $\UMD_{p}$ constant of $X$ is given by
  \begin{equation*}
    \beta_{p}(X) = \sup_{\Omega,\mc{A}_{\bullet}, \xi_{\bullet}}  \|T_{\mc{A}_{\bullet}, \xi_{\bullet}}\|_{\Lin(L^p(\Omega;X))},
  \end{equation*}
  where the supremum is taken over all probability spaces $(\Omega,\mc{A},\P)$, all finite filtrations $\mc{A}_{\bullet}$, and all finite sign sequences $\xi_{\bullet}$.
\end{prop}

This characterisation of the $\UMD_{p}$ constant, with a short duality argument, implies the following.

\begin{prop}\label{prop:UMD-duality}\index{UMD property!relation with duality}
  Let $p \in (1,\infty)$, and let $X$ be a Banach space.
  Then $X$ has the $\UMD_{p}$ property if and only if the dual space $X^{*}$ has the $\UMD_{p'}$ property, and $\beta_{p}(X) = \beta_{p'}(X^{*})$.
\end{prop}

\begin{proof}
  Fix a finite filtration $\mc{A}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$, and a finite sign sequence $\xi_{\bullet}$.
  Suppose $\mb{f} \in L^{p}(\Omega;X)$ and $\mb{g} \in L^{p'}(\Omega;X^{*})$.
  Since the (scalar) conditional expectation $\E^{\mc{A}_{n}}$ on $L^p(\Omega)$ is adjoint to the corresponding conditional expectation on $L^{p'}(\Omega)$ (Proposition \ref{prop:CE-adjoint}), and since
  \begin{equation*}
    \langle \E^{\mc{A}_{n}} \mb{f}, \mb{g} \rangle = \langle \mb{f}, \E^{\mc{A}_{n}} \mb{g} \rangle 
  \end{equation*}
  by Exercise \ref{ex:tensor-adjoint}, we get
  \begin{equation*}
    \langle T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}, \mb{g} \rangle =  \langle \mb{f}, T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{g} \rangle.
  \end{equation*}
  Thus by the norming property of $L^{p'}(\Omega;X^{*})$ as a subset of $L^{p}(\Omega;X)^{*}$ (Proposition \ref{prop:bochner-preduality}), and of $L^{p}(\Omega;X)$ as a subset of $L^{p'}(\Omega;X^{*})^{*}$,\footnote{Technically we haven't proven this. The proof is a small modification of that of Proposition \ref{prop:bochner-preduality}, using that $X$ is a norming subspace of $X^{**} = (X^{*})^{*}$. See \cite[Proposition 1.3.1]{HNVW16} for the needed duality result in full generality.}  we get
  \begin{equation*}
    \begin{aligned}
      \|T_{\mc{A}_{\bullet}, \xi_{\bullet}}\|_{\Lin(L^{p}(\Omega;X))}
      &= \sup_{\mb{f}} \| T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f} \|_{L^{p}(\Omega;X)} \\
      &= \sup_{\mb{f}, \mb{g}} \big| \langle T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}, \mb{g} \rangle \big| \\
      &= \sup_{\mb{f}, \mb{g}} \big| \langle  \mb{f}, T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{g} \rangle \big| \\
      &= \sup_{\mb{g}} \| T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{g} \|_{L^{p'}(\Omega;X^{*})}
      &= \|T_{\mc{A}_{\bullet}, \xi_{\bullet}}\|_{\Lin(L^{p'}(\Omega;X^{*}))},
  \end{aligned}
\end{equation*}
with suprema taken over all normalised $\mb{f} \in L^{p}(\Omega;X)$ and $\mb{g} \in L^{p'}(\Omega;X^{*})$.
Taking the supremum over all $\Omega$, $\mc{A}_{\bullet}$, and $\xi_{\bullet}$ then shows that
\begin{equation*}
  \beta_{p}(X) = \beta_{p'}(X^{*}), 
\end{equation*}
completing the proof.
\end{proof}

Before moving on to more subtle properties, we show that the $\UMD_{p}$ property extends to Bochner spaces.
This will be more useful once we know the $p$-independence of the UMD property.

\begin{prop}\label{prop:Bochner-UMDp}\index{UMD property!of Bochner spaces}
  Let $p \in (1,\infty)$, and suppose $X$ is a Banach space with the $\UMD_{p}$ property.
  Let $(S,\mc{A},\mu)$ be a measure space with $\mu(S) > 0$.
  Then the Bochner space $L^p(\mu;X)$ has $\UMD_{p}$, with $\beta_{p}(L^p(\mu;X)) = \beta_{p}(X)$.
\end{prop}

\begin{proof}
  To avoid confusion let $Y = L^p(\mu;X)$, and let $\mb{f}_{\bullet}$ be a $Y$-valued martingale on a probability space $(\Omega,\mc{A},\P)$.
  Then for all $N \in \N$, by wrapping the $\UMD_{p}$ property of $X$ in two applications of Fubini's theorem, we have
  \begin{equation*}
    \begin{aligned}
      \Big\| \sum_{n=0}^N \xi_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;Y)}^{p}
      &= \int_{\Omega} \int_{S} \Big\| \sum_{n=0}^N \xi_{n} d\mb{f}_{n}(\omega)(s) \Big\|_{X}^{p} \, \dd\mu(s) \, \dd\P(\omega) \\
      &=  \int_{S} \Big( \int_{\Omega} \Big\| \sum_{n=0}^N \xi_{n} d\mb{f}_{n}(\omega)(s) \Big\|_{X}^{p}  \, \dd\P(\omega) \Big) \, \dd\mu(s) \\
      &\leq \beta_{p}(X)^{p} \int_{S} \Big( \int_{\Omega} \Big\| \sum_{n=0}^N d\mb{f}_{n}(\omega)(s) \Big\|_{X}^{p}  \, \dd\P(\omega) \Big) \, \dd\mu(s) \\
      &= \beta_{p}(X)^{p} \int_{\Omega}  \int_{S} \Big\| \sum_{n=0}^N d\mb{f}_{n}(\omega)(s) \Big\|_{X}^{p}   \, \dd\mu(s) \, \dd\P(\omega) \\
      &= \beta_{p}(X)^{p} \Big\| \sum_{n=0}^N d\mb{f}_{n} \Big\|_{L^p(\Omega;Y)}^{p},
    \end{aligned}
  \end{equation*}
  which establishes $\beta_{p}(Y) \leq \beta_{p}(X)$.
  The reverse estimate is Exercise \ref{ex:UMD-Lp-reverse}.
\end{proof}

\section{$p$-independence of the UMD property}\label{sec:UMD-p-independence}\index{UMD property!independence of $p$}

Our goal now is to prove that the $\UMD_{p}$ property is independent of $p$.
For this, we will use the characterisation from Proposition \ref{prop:UMD-signtransforms} of the $\UMD_{p}$ property in terms of finite martingale sign transforms $T_{\mc{A}_{\bullet}, \xi_{\bullet}}$.
The basic strategy is as follows:
\begin{itemize}
\item Assuming that the finite sign transforms $T_{\mc{A}_{\bullet},\xi_{\bullet}}$ are uniformly bounded on $L^p(\Omega;X)$, prove that they have weak-type $(1,1)$ (i.e. that they are bounded from $L^1(\Omega;X)$ to the Lorentz space $L^{1,\infty}(\Omega;X)$) uniformly over all finite filtrations and sign sequences.
\item Use Marcinkiewicz's interpolation theorem to deduce that the finite sign transforms are uniformly bounded on $L^q(\Omega;X)$ for all $1 < q < p$, and thus that $X$ has the $\UMD_{q}$ property for all $1 < q < p$.
\item Argue by duality to show that $X$ has the $\UMD_{r}$ property for all $p < r < \infty$, and thus that $X$ has the full $\UMD$ property.
\end{itemize}

The first two steps are similar to how we proved Doob's maximal inequality (Theorem \ref{thm:doob}), except there we implicitly interpolated between a weak-type $(1,1)$ estimate and an $L^\infty$ bound that such maximal operators automatically satisfy.
The weak-type $(1,1)$ estimate for sign transforms will be proven relies on \emph{Gundy's decomposition} for martingales, which is analogous to the Calder\'on--Zygmund decomposition of a function.\footnote{
Students of harmonic analysis may recognise this procedure as essentially being the same as that used in Calder\'on--Zygmund theory: start with an assumed $L^p$ estimate, use properties of the operators under consideration (here we use the martingale property rather than kernel estimates) to deduce weak-type $(1,1)$, and finally use interpolation and duality to extend the $L^p$-boundedness to $L^q$ for all $q \in (1,\infty)$.}

\begin{thm}[Gundy's decomposition]\label{thm:gundy}\index{Gundy's decomposition}
  Let $X$ be a Banach space and $(\Omega,\mc{A},\P)$ a probability space.
  Suppose $\mb{f} \in L^1(\Omega;X)$ with $\|\mb{f}\|_{L^1(\Omega;X)} = 1$, and let $\mc{A}_{\bullet}$ be a filtration.
  Then given $\lambda > 0$, there is a decomposition
  \begin{equation*}
    \mb{f} = \mb{a} + \mb{b} + \mb{c}
  \end{equation*}
  with $\mb{a}, \mb{b}, \mb{c} \in L^1(\Omega;X)$ satisfying
  \begin{itemize}
  \item $\|\mb{a}\|_{L^{1}(\Omega;X)} \lesssim 1$, $\P\big(\{\sup_{n \in \N} \|d\mb{a}_{n}\|_{X} \neq 0\}\big) \lesssim \lambda^{-1}$,
  \item $\| \sum_{n \in \N} \|d\mb{b}_{n}\|_{X} \|_{L^1(\Omega)} \lesssim 1$,
  \item $\| \mb{c} \|_{L^\infty(\Omega;X)} \lesssim \lambda$ and $\|\mb{c}\|_{L^1(\Omega;X)} \lesssim 1$.
  \end{itemize}
  where $\mb{a}_{n} = \E^{\mc{A}_{n}} \mb{a}$ and $\mb{b}_{n} = \E^{\mc{A}_{n}} \mb{b}$ are the martingales associated with the filtration $\mc{A}_{\bullet}$.
\end{thm}

We will prove this in the next section.
For now we will assume it, and deduce the weak-type $(1,1)$ estimate for finite sign transforms in $\UMD_{p}$ spaces.

\begin{prop}\label{prop:martingale-w11}
  Suppose that $X$ is a Banach space with the $\UMD_{p}$ property.
  Then for all probability spaces $(\Omega,\mc{A},\P)$ and all finite filtrations $\mc{A}_{\bullet}$ and sign sequences $\xi_{\bullet}$,
  \begin{equation*}
    \|T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}\|_{L^{1,\infty}(\Omega;X)} := \sup_{t > 0} t\P(\|T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}\|_{X} > t) \lesssim_{p,X} \|\mb{f}\|_{L^1(\Omega;X)}
  \end{equation*}
  for all $\mb{f} \in L^1(\Omega;X)$.
\end{prop}

\begin{proof}[Proof, assuming Theorem \ref{thm:gundy}]
  Fix a finite filtration $(\mc{A}_{n})_{n=0}^{N}$ on a probability space $(\Omega,\mc{A},\P)$, and a sequence of signs $(\xi_{n})_{n=0}^{N}$.
  Let $\mb{f} \in L^1(\Omega;X)$; by rescaling we may assume $\|\mb{f}\|_{L^1(\Omega;X)} = 1$.\footnote{If $\mb{f} = 0$ there is nothing to show anyway.}
  Fix $t > 0$, and let
  \begin{equation*}
    \mb{f} = \mb{a} + \mb{b} + \mb{c}
  \end{equation*}
  be the Gundy decomposition of $\mb{f}$ at level $t$ (Theorem \ref{thm:gundy}).
  Writing
  \begin{equation*}
    \td{\mb{f}} := T_{\mb{A}_{\bullet}, \xi_{\bullet}}\mb{f},
    \qquad \td{\mb{a}} := T_{\mb{A}_{\bullet}, \xi_{\bullet}}\mb{a},
    \qquad \td{\mb{b}} := T_{\mb{A}_{\bullet}, \xi_{\bullet}}\mb{b},
    \qquad \td{\mb{c}} := T_{\mb{A}_{\bullet}, \xi_{\bullet}}\mb{c},
  \end{equation*}
  We have by linearity of the martingale transform
  \begin{equation*}
    \td{\mb{f}} = \td{\mb{a}} + \td{\mb{b}} + \td{\mb{c}}
  \end{equation*}
  and thus
  \begin{equation*}
    \P(\|\td{\mb{f}}\|_{X} > 3t)
    \leq \P(\|\td{\mb{a}}\|_{X} > t) + \P(\|\td{\mb{b}}\|_{X} > t) + \P(\|\td{\mb{c}}\|_{X} > t).
  \end{equation*}

  We estimate these three summands separately.
  First, we have
  \begin{equation*}
    \begin{aligned}
      \P(\|\td{ \mb{a} }\|_{X} > t)
      \leq \P(\sup_{n} \|d \mb{a}_{n}\|_{X} \neq 0) \lesssim t^{-1},
    \end{aligned}
  \end{equation*}
  as we cannot have $\| \td{ \mb{a} }  \|_{X} = \|T_{ \mc{A}_{\bullet}, \xi_{\bullet} } \mb{a}\|_{X}> t$ without one of the differences $d\mb{a}_{n}$ being nonzero.
  For the second term, since
  \begin{equation*}
    \|\td{\mb{b}}(\omega)\|_{X} = \Big\| \sum_{n=0}^{N} \xi_{n} d\mb{b}_{n}(\omega) \Big\|_{X} \leq \sum_{n = 0}^{N} \|d\mb{b}_{n}(\omega)\|_{X} \qquad \forall \omega \in \Omega,
  \end{equation*}
  we have
  \begin{equation*}
    \P(\|\td{\mb{b}}\|_{X} > t) \leq \P\Big( \sum_{n = 0}^{N} \|d\mb{b}_{n}\|_{X} > t \Big)
    \leq t^{-1} \Big\|  \sum_{n = 0}^{N} \|d\mb{b}_{n}\|_{X} \Big\|_{L^1(\Omega)} \lesssim t^{-1}
  \end{equation*}
  by Chebyshev's inequality.
  Finally, using Chebyshev again, the $\UMD_{p}$ property, and log-convexity of $L^p$-norms,
  \begin{equation*}
    \begin{aligned}
      \P(\|\td{\mb{c}}\|_{X} > t)
      &\leq t^{-p}\|T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{c} \|_{L^p(\Omega;X)}^{p} \\
      &\lesssim_{p,X} t^{-p} \|\mb{c} \|_{L^p(\Omega;X)}^{p} \\
      &\leq t^{-p}  \Big( \|\mb{c} \|_{L^1(\Omega;X)}^{\frac{1}{p}} \|\mb{c}\|_{L^\infty(\Omega;X)}^{1-\frac{1}{p}} \Big)^{p} \\
      &\lesssim_{p} t^{-p}  t^{p-1} = t^{-1} .
    \end{aligned}
  \end{equation*}
  Therefore we have
  \begin{equation*}
    3t\P(\|T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}\|_{X} > 3t)
    \lesssim_{p,X} 3t(3t^{-1}) \simeq 1 = \|\mb{f}\|_{L^{1}(\Omega;X)}.
  \end{equation*}
  Taking the supremum over $t > 0$ completes the proof.
\end{proof}

Now we can prove the $p$-independence of the UMD property, still implicitly assuming the validity of Gundy's decomposition.

\begin{thm}\label{thm:UMD-p-independent}\index{UMD property!independence of $p$}
  Let $X$ be a Banach space which has the $\UMD_{p}$ property for some $p \in (1,\infty)$.
  Then $X$ has $\UMD_{q}$ for all $q \in (1,\infty)$ (i.e. $X$ is UMD).
\end{thm}

\begin{proof}
  By Proposition \ref{prop:martingale-w11} we have that all finite martingale sign transforms $T_{\mc{A}_{\bullet}, \xi_{\bullet}}$ are uniformly bounded from $L^1(\Omega;X)$ to $L^{1,\infty}(\Omega;X)$.
  By the Marcinkiewicz interpolation theorem for Bochner spaces (Theorem \ref{thm:marcinkiewicz} in the appendix), since these operators are also uniformly bounded on $L^p(\Omega;X)$, we find that they are uniformly bounded on $L^q(\Omega;X)$ for all $q \in (1,p)$, and thus $X$ has the $\UMD_{q}$ property for all such $q$.

  On the other hand, by Proposition \ref{prop:UMD-duality}, we know that $X^{*}$ has the $\UMD_{p'}$ property, and thus by the argument above $X^{*}$ has the $\UMD_{r}$ property for all $r \in (1,p')$.
  Hence $X$ itself has the $\UMD_{r'}$ property for all $r \in (1,p')$, which (messing with H\"older conjugates) says that $X$ has the $\UMD_{s}$ property for all $s \in (p,\infty)$.
  Thus $X$ is UMD.
\end{proof}

\begin{cor}\index{UMD property!of Lebesgue spaces}\index{UMD property!of Bochner spaces}
  The following Banach spaces are UMD:
  \begin{itemize}
  \item every Hilbert space,
  \item every finite-dimensional space,
  \item every Lebesgue space $L^p(S)$ over a measure space $(S,\mc{A},\mu)$, with $p \in (1,\infty)$,
  \item every Bochner space $L^p(S;X)$ with $S$ and $p$ as above, provided that $X$ is UMD.
  \end{itemize}
\end{cor}

\begin{proof}
  We proved in Proposition \ref{prop:Hilbert-UMD2} that every Hilbert space has the $\UMD_{2}$ property, so by $p$-independence, every Hilbert space is $\UMD$.
  Every finite-dimensional space is isomorphic to a Hilbert space, hence also UMD (see Exercise \ref{ex:UMD-isomorphism}).
  If $X$ is a UMD space, then in particular $X$ is $\UMD_{p}$, and thus by Proposition \ref{prop:Bochner-UMDp} the Bochner space $L^p(S;X)$ is also $\UMD_{p}$ (hence $\UMD$).
  Taking $X$ to be the scalar field, which is a Hilbert space, proves that $L^p(S)$ is $\UMD$.
\end{proof}

\begin{rmk}
  Theorem \ref{thm:UMD-p-independent} says that if the constant $\beta_{p}(X)$ is finite for some $p \in (1,\infty)$, then $\beta_{q}(X) < \infty$ for all $q \in (1,\infty)$, but it does not give sharp control on how $\beta_{q}(X)$ changes.
  It is possible (but difficult) to prove that
  \begin{equation*}
    \beta_{p}(H) = \max(p,p') - 1
  \end{equation*}
  for every Hilbert space $H$ and all $p \in (1,\infty)$ \cite[Corollary 4.5.15]{HNVW16}.
  As far as I know, this is the only case where the UMD constant of a space is known exactly (excluding the cases $\beta_{p}(X) = \infty$).
\end{rmk}


\section{The proof of Gundy's decomposition}

The proof of Gundy's decomposition, like the proof of Doob's maximal inequality, involves stopping time arguments, but this time they are somewhat heavier.
We will use a few stopping time concepts that we didn't introduce earlier.
Stopping times were defined back in Definition \ref{defn:stopping-time}.

\begin{defn}\index{stopping time}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $\mc{A}_{\bullet}$ a filtration.
  Let $\map{T}{\Omega}{\N \cup \{\infty\}}$ be a stopping time with respect to the filtration $\mc{A}_{\bullet}$.

  \begin{itemize}
  \item We define the \emph{$\sigma$-algebra associated with the stopping time} $\mc{A}_{T}$ as follows: a subset $A \in \mc{A}$ belongs to $\mc{A}_{T}$ if and only if $A \cap \{T \leq n\} \in \mc{A}_{n}$ for all $n \in \N$.
    One can show that $\mc{A}_{T}$ is indeed a $\sigma$-algebra.
  \item Given a stochastic process $\mb{f}_{\bullet}$ valued in a Banach space $X$, we define the random variable $\mb{f}_{T} \colon \Omega \sm \{T = \infty\} \to X$ by
    \begin{equation*}
      \mb{f}_{T}(\omega) := \mb{f}_{T(\omega)}(\omega).
    \end{equation*}
  \end{itemize}
\end{defn}

Note that for each $n \in \N$, the constant random variable $n(\omega) \equiv n$ is a stopping time with respect to every filtration.
Note also that for two stopping times $S,T$ with respect to a filtration $\mc{A}_{\bullet}$, the minimum $\min(S,T)$ is also a stopping time with respect to $\mc{A}_{\bullet}$.
In particular, $\min(n,T)$ is a stopping time with respect to $\mc{A}_{\bullet}$, and we can define the sequence of $\sigma$-algebras $(\mc{A}_{\min(n,T)})_{n \in \N}$ and the random variables $(\mb{f}_{\min(n,T)})_{n \in \N}$.
The sequence $(\mc{A}_{\min(n,T)})_{n \in \N}$ is in fact a filtration (Exercise \ref{ex:stopped-filtration}), and the stochastic process $\mb{f}_{\min(n,T)}$ is called the \emph{stopped process}:\index{stopped process} the idea is that one takes the process $\mb{f}_{\bullet}$, but once the stopping time $T$ is reached (keeping in mind that this time depends on $\omega \in \Omega$), the process is `stopped' at the value $\mb{f}_{T}$.

\begin{prop}\label{prop:stopped-martingale}
  Let $X$ be a Banach space and $(\Omega,\mc{A},\P)$ a probability space, and let $\mc{A}_{\bullet}$ be a filtration.
  Let $\mb{f} \in L^1(\Omega;X)$, and let $\mb{f}_{\bullet} := \E^{\mc{A}_{\bullet}} \mb{f}$ be the associated martingale. 
  If $\map{T}{\Omega}{\N \cup \{\infty\}}$ is a finite stopping time, then
  \begin{equation}\label{eq:stopmgale}
    \E^{\mc{A}_{T}} \mb{f} \aeeq \mb{f}_{T},
  \end{equation}
  and so $\|\mb{f}_{T}\|_{L^1(\Omega;X)} \leq \|\mb{f}\|_{L^1(\Omega;X)}$.
  Furthermore, the stopped process $\mb{f}_{\min(\bullet,T)}$ is a martingale with respect to $\mc{A}_{\min(\bullet,T)}$.
\end{prop}

\begin{proof}
  The fact that $\mb{f}_{\min(\bullet,T)}$ is a martingale follows from \eqref{eq:stopmgale}, which implies in addition that $\mb{f}_{\min(\bullet,T)} = \E^{\mc{A}_{\min(\bullet,T)}} \mb{f}$ is the martingale associated with $\mb{f}$ and the filtration $\mc{A}_{\min(\bullet,T)}$.
  Thus we need only prove \eqref{eq:stopmgale}.
  
  Fix a set $A \in \mc{A}_{T}$, and let $A_{k} = A \cap \{T = k\}$ for $k \in \N \cup \{\infty\}$, so that $A_{k} \in \mc{A}_{k}$ and $(A_{k})_{k \in \N \cup \{\infty\}}$ is a partition of $A$.
  With the interpretation $\mb{f}_{\infty} := \mb{f}$, we have
  \begin{equation*}
      \int_{A} \mb{f} \, \dd \P 
      = \sum_{k \in \N \cup \{\infty\}} \int_{A_k} \mb{f} \, \dd \P 
      = \sum_{k \in \N \cup \{\infty\}} \int_{A_{k}} \mb{f}_{k} \, \dd \P 
      = \sum_{k \in \N \cup \{\infty\}} \int_{A_{k}} \mb{f}_{T} \, \dd \P 
      = \int_{A} \mb{f}_{T} \, \dd \P.
  \end{equation*}
  Since this holds for all $A \in \mc{A}_{T}$, the defining property of conditional expectations says that $\mb{f}_{T} = \E^{\mc{A}_{T}} \mb{f}$.
  The nonexpansiveness of conditional expectations on $L^1$ then implies $\|\mb{f}_{T}\|_{L^1(\Omega;X)} \leq \|\mb{f}\|_{L^1(\Omega;X)}$.
\end{proof}

Now, to Gundy's decomposition.

\begin{proof}[Proof of Theorem \ref{thm:gundy}]\index{Gundy's decomposition}
  Define a stopping time
  \begin{equation*}
    r(\omega) := \inf\big\{ n \in \N : \|\mb{f}_{n}(\omega)\|_{X} > \lambda \big\},
  \end{equation*}
  where $\mb{f}$ is the martingale associated with $\mb{f}$ and $\mc{A}_{\bullet}$.
  For $n \in \N$ define the scalar-valued function $v_{n} = \|d\mb{f}_{n}\|_{X} \1_{\{r = n\}}$, and define two more stopping times
  \begin{equation*}
    \begin{aligned}
      s(\omega) &:= \inf\Big\{ n \in \N : \sum_{k=0}^{n} \E^{\mc{A}_{k}} v_{k+1} > \lambda \Big\} \\
      T(\omega) &:= \min(r(\omega), s(\omega)).
    \end{aligned}
  \end{equation*}
  We define the first term in the decomposition,
  \begin{equation*}
    \mb{a} := \mb{f} - \mb{f}_{T}.
  \end{equation*}
  Then by Proposition \ref{prop:stopped-martingale} we can estimate
  \begin{equation}\label{eq:a-1stest}
    \|\mb{a}\|_{L^1(\Omega;X)} \leq \|\mb{f}\|_{L^1(\Omega;X)} + \|\E^{\mc{A}_{T}}\mb{f}\|_{L^1(\Omega;X)}
    \lesssim \|\mb{f}\|_{L^1(\Omega;X)} = 1,
  \end{equation}
  and at least for this term of the decomposition it remains to show that
  \begin{equation}\label{eq:a-2ndest}
    \P\Big(\Big\{\sup_{n \in \N} \|d\mb{a}_{n}\|_{X} \neq 0\Big\}\Big) = \P\Big(\bigcup_{n \in \N} \big\{\|d\mb{a}_{n}\|_{X} \neq 0\big\}\Big) \lesssim \lambda^{-1}.
  \end{equation}
  If $\|d\mb{a}_{n}(\omega)\|_{X} \neq 0$ for some $n \in \N$, then we must have $d\mb{f}_{n}(\omega) \neq d\mb{f}_{\min(n,T)}(\omega)$ for some $n \in \N$, which implies that $T(\omega) < \infty$ (in fact, $T(\omega) < n$ for this particular $n$).
  So we have
  \begin{equation*}
    \P\Big(\bigcup_{n \in \N} \big\{\|d\mb{a}_{n}\|_{X} \neq 0\big\}\Big) \leq \P(\{T < \infty\}) \leq \P(\{s < \infty\}) + \P(\{r < \infty\}).
  \end{equation*}
  The summand involving $r$ is estimated by
  \begin{equation}\label{eq:a-rest}
    \P\big(\{r < \infty\}\big) = \P\big( \{\sup_{n \in \N} \|\mb{f}_{n}\|_{X} > \lambda \} \big)
    = \P\big( \mc{M}(\mb{f}_{\bullet}) > \lambda \big) \lesssim \lambda^{-1},
  \end{equation}
  using Doob's maximal inequality (Theorem \ref{thm:doob}).
  As for the summand involving $s$, we can use Chebyshev and the properties of conditional expectations to estimate
  \begin{equation*}
    \begin{aligned}
      \P\big(\{s < \infty\}\big)
      &= \P\big( \big\{ \sum_{k=0}^{\infty} \E^{\mc{A}_{k}} v_{k+1} > \lambda \big\} \big) \\
      &\leq \lambda^{-1} \E\Big( \sum_{k=0}^{\infty} \E^{\mc{A}_{k}} v_{k+1} \Big)
      = \lambda^{-1} \sum_{k=0}^{\infty} \E v_{k+1}
      = \lambda^{-1} \sum_{k=1}^{\infty} \E v_{k}.
    \end{aligned}
  \end{equation*}
  By definition of $v_{k}$ we have
  \begin{equation*}
    \E v_{k} = \E ( \|d\mb{f}_{k}\|_{X} \1_{\{r = k\}} ).
  \end{equation*}
  When $r(\omega) = k$ we have that
  \begin{equation*}
    \|\mb{f}_{k}(\omega)\|_{X} > \lambda \geq \|\mb{f}_{k-1}(\omega)\|_{X},
  \end{equation*}
  and thus also that
  \begin{equation*}
    \|d\mb{f}_{k}(\omega)\|_{X} \leq \|\mb{f}_{k}(\omega)\|_{X} + \|\mb{f}_{k-1}(\omega)\|_{X} \lesssim \|\mb{f}_{k}(\omega)\|_{X}.
  \end{equation*}
  This lets us estimate
  \begin{equation*}
    \E ( \|d\mb{f}_{k}\|_{X} \1_{\{r = k\}} )
    \lesssim \E ( \|\mb{f}_{k}\|_{X} \1_{\{r = k\}} )
    = \E ( \|\E^{\mc{A}_{k}} \mb{f}\|_{X} \1_{\{r = k\}} ).
  \end{equation*}
  The pointwise estimate for tensor extensions of positive operators in Theorem \ref{thm:positive-extensions} then allows us to proceed:
  \begin{equation*}
    \begin{aligned}
    \E ( \|\E^{\mc{A}_{k}} \mb{f}\|_{X} \1_{\{r = k\}} )
    \leq \E ( \E^{\mc{A}_{k}}(\|\mb{f}\|_{X}) \1_{\{r = k\}} )
    &= \E ( \E^{\mc{A}_{k}}(\|\mb{f}\|_{X} \1_{\{r = k\}}) ) \\
    &= \E (\|\mb{f}\|_{X} \1_{\{r = k\}})
  \end{aligned}
  \end{equation*}
  using that the set $\{r = k\}$ is $\mc{A}_{k}$-measurable (i.e. that $r$ is a stopping time with respect to $\mc{A}_{\bullet}$).
  Putting all this together yields
  \begin{equation*}
    \begin{aligned}
      \P\big(\{s < \infty\}\big)
      &\leq \lambda^{-1} \sum_{k=1}^{\infty} \E v_{k} \\
      &\lesssim \lambda^{-1} \sum_{k=1}^{\infty} \E (\|\mb{f}\|_{X} \1_{\{r = k\}})
      = \lambda^{-1} \E(\|\mb{f}\|_{X} \1_{\{1 \leq r < \infty\}}) \leq \lambda^{-1},
    \end{aligned}
  \end{equation*}
  which along with \eqref{eq:a-rest} proves the remaining estimate \eqref{eq:a-2ndest} for $\mb{a}$.
  
  Now we need to define functions $\mb{b}$ and $\mb{c}$ such that $\mb{f} - \mb{a} = \mb{b} + \mb{c}$.
  We will define $\mb{b}$ and $\mb{c}$ via their martingale difference sequences $d\mb{b}_{\bullet}$ and $d\mb{c}_{\bullet}$, and show that these indeed give rise to functions $\mb{b}, \mb{c} \in L^1(\Omega;X)$ with the desired properties.
  Since $\mb{f} - \mb{a} = \mb{f}_{T}$ by definition, we will need to have
  \begin{equation}\label{eq:db-req}
    d\mb{b}_{n} + d\mb{c}_{n} = d\mb{f}_{\min(n,T)} \qquad \forall n \in \N.
  \end{equation}
  By writing
  \begin{equation*}
    \begin{aligned}
      d\mb{f}_{\min(n,T)}
      &= \mb{f}_{\min(n,T)} - \mb{f}_{\min(n-1,T)} \\
      &= \1_{\{n \leq T\}} d\mb{f}_{n} 
      = \1_{\{n \leq r\}} \1_{\{n \leq s\}} d\mb{f}_{n} 
      =  ( \1_{\{n = r\}} + \1_{\{n < r\}}) \1_{\{n \leq s\}} d\mb{f}_{n},
    \end{aligned}
  \end{equation*}
  we see that \eqref{eq:db-req} is satisfied by the definitions
  \begin{equation*}
    \begin{aligned}
      d\mb{b}_{n} &:= \1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n}  - \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n} ), \\
      d\mb{c}_{n} &:= \1_{\{n < r\}} \1_{\{n \leq s\}} d\mb{f}_{n}  + \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n} )
    \end{aligned}
  \end{equation*}
  (for $n = 0$ we omit the conditional expectations).
  These are indeed martingale difference sequences: for all $n \in \N$ we have $\E^{\mc{A}_{n}} d\mb{b}_{n+1} = 0$ almost automatically, and
  \begin{equation*}
    \begin{aligned}
      \E^{\mc{A}_{n}} d\mb{c}_{n+1}
      &= \E^{\mc{A}_{n}} (d\mb{f}_{\min(n+1,T)} - d\mb{b}_{n+1}) \\
      &= \E^{\mc{A}_{n}} d\mb{f}_{\min(n+1,T)} \\
      &= \E^{\mc{A}_{n}} (\1_{\{n+1 \leq T\}} d\mb{f}_{n+1}) \\
      &= \1_{\{n+1 \leq T\}}\E^{\mc{A}_{n}} (d\mb{f}_{n+1}) = 0
    \end{aligned}
  \end{equation*}
  using that the set $\{n+1 \leq T\} = \Omega \sm \{T \leq n\}$ is $\mc{A}_{n}$-measurable ($T$ being a stopping time with respect to $\mc{A}_{\bullet}$) and that $\mb{f}_{\bullet}$ is a martingale.

  To ensure that we can use the martingale difference sequences $d\mb{b}_{\bullet}$ and $d\mb{c}_{\bullet}$ to define functions $\mb{b}, \mb{c} \in L^1(\Omega;X)$, we need to ensure convergence of the series $\sum_{n \in \N} d\mb{b}_{n}$ and $\sum_{n \in \N} d\mb{c}_{n}$ in $L^1(\Omega;X)$.
  Luckily we have
  \begin{equation*}
    \begin{aligned}
    \sum_{n \in \N} \|d\mb{b}_{n}\|_{X} \lesssim \sum_{n \in \N} \1_{\{n = r\}} \1_{\{n \leq s\}} \|d\mb{f}_{n}\|_{X}
    &\leq \sum_{n \in \N} v_{n} = \sum_{n \in \N} \E(\|\mb{f}\|_{X} \1_{\{r = n\}}) \lesssim 1
  \end{aligned}
  \end{equation*}
  by the argument used in estimating $\mb{a}$.
  This establishes that $\mb{b} = \sum_{n \in \N} d\mb{b}_{n} \in L^1(\Omega;X)$ exists, and also proves the claimed estimate
  \begin{equation*}
    \Big\| \sum_{n \in \N} \|d\mb{b}_{n}\|_{X} \Big\|_{L^1(\Omega;X)} \lesssim 1.
  \end{equation*}
  Now since $\mb{f}_{\min(n,T)} = \mb{b}_{n} + \mb{c}_{n}$, and since the martingales $\mb{f}_{\min(\bullet,T)}$ and $\mb{b}_{\bullet}$ both converge in $L^1(\Omega;X)$ and pointwise almost everywhere (Theorem \ref{thm:mgale-pw-conv}), it follows that $\mb{c}_{\bullet}$ also converges to some $\mb{c} \in L^1(\Omega;X)$, both in $L^1(\Omega;X)$ and pointwise almost everywhere.
  We have
  \begin{equation*}
    \begin{aligned}
    \|\mb{c}\|_{L^1(\Omega;X)} &\leq \|\mb{b}\|_{L^1(\Omega;X)} + \|\mb{f}_{T}\|_{L^1(\Omega;X)} \\
    &\leq \Big\| \sum_{n \in \N} \|d\mb{b}_{n}\|_{X} \Big\|_{L^1(\Omega;X)} + \|\E^{\mc{A}_{T}} \mb{f}\|_{L^1(\Omega;X)}
    \lesssim 1
  \end{aligned}
  \end{equation*}
  as required; only the bound $\|\mb{c}\|_{L^\infty(\Omega;X)} \lesssim \lambda$ remains.
  By the almost everywhere pointwise convergence we have for almost all $\omega \in \Omega$
  \begin{equation*}
    \begin{aligned}
      \mb{c}(\omega)
      &= \sum_{n \in \N} d\mb{c}_{n}(\omega) \\
      &= \sum_{n \in \N} \1_{\{n < r\}}(\omega) \1_{\{n \leq s\}}(\omega) d\mb{f}_{n}(\omega)  + \sum_{n \geq 1} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n} )(\omega).
    \end{aligned}
  \end{equation*}
  The first sum can be estimated by
  \begin{equation*}
    \begin{aligned}
      \sum_{n=0}^{N} \1_{\{n < r\}}(\omega) \1_{\{n \leq s\}}(\omega) d\mb{f}_{n}(\omega)
      &= \sum_{n=0}^{s(\omega)} \1_{\{n < r\}}(\omega) d\mb{f}_{n}(\omega) \\
      &= \begin{cases}
        \mb{f}_{\min(r-1,s)}(\omega) & \text{if $r(\omega) > 0$} \\ \mb{0} & \text{if $r(\omega) = 0$,}
      \end{cases}
    \end{aligned}
  \end{equation*}
  so by definition of the stopping time $r$ we have
  \begin{equation*}
    \Big\| \sum_{n=0}^{N} \1_{\{n < r\}}(\omega) \1_{\{n \leq s\}}(\omega) d\mb{f}_{n}(\omega) \Big\|_{X} \leq \lambda.
  \end{equation*}
  As for the second sum, since the set $\{n \leq s\} = \Omega \sm \{s \leq n-1\}$ is $\mc{A}_{n-1}$-measurable, we can write
  \begin{equation*}
    \begin{aligned}
      \sum_{n \geq 1} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n} )(\omega)
      &= \sum_{n \geq 1} \1_{\{n \leq s\}}(\omega) \E^{\mc{A}_{n-1}} (\1_{\{n = r\}}  d\mb{f}_{n} )(\omega) \\
      &= \sum_{n = 1}^{s(\omega)} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}}  d\mb{f}_{n} )(\omega),
    \end{aligned}
  \end{equation*}
  and again using the pointwise estimate from Theorem \ref{thm:positive-extensions} we have
  \begin{equation*}
    \begin{aligned}
      \Big\| \sum_{n = 1}^{s(\omega)} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}}  d\mb{f}_{n} )(\omega) \Big\|_{X}
      &\leq \sum_{n=1}^{s(\omega)} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \|d\mb{f}_{n}\|_{X} )(\omega) \\
      &= \sum_{n=0}^{s(\omega) - 1} \E^{\mc{A}_{n}} (v_{n-1} )(\omega) \leq \lambda
    \end{aligned}
  \end{equation*}
  by the definition of the stopping time $s$.
  All up, this yields
  \begin{equation*}
    \|\mb{c}\|_{L^\infty(\Omega;X)} \lesssim \lambda
  \end{equation*}
  and completes the proof.
\end{proof}

\section{Burkholder's inequalities and martingale transforms}

The UMD property is equivalent to \emph{Burkholder's inequalities}, which say that the $L^p$-norm of a martingale (i.e. the supremum of $L^p$ norms of its terms) is equivalent to a Rademacher average of its difference sequence in $L^p$.
We will state it in terms of the Rademacher spaces $\varepsilon(X)$ defined in Section \ref{sec:rademacher-spaces}.

\begin{thm}[Burkholder's inequalities]\label{thm:burkholder}\index{Burkholder's inequalities}
  A Banach space $X$ is $\UMD$ if and only if for all $p \in (1,\infty)$ and every $L^p$-bounded martingale $\mb{f}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$,
  \begin{equation}\label{eq:burkholder}
    \sup_{n \in \N} \|\mb{f}_{n}\|_{L^p(\Omega;X)} \simeq_{p,X} \|d\mb{f}_{\bullet}\|_{\varepsilon(L^p(\Omega;X))} = \Big( \E \Big\| \sum_{n \in \N} \varepsilon_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)}^{2} \Big)^{1/2}
  \end{equation}
  where $\varepsilon_{\bullet}$ is a Rademacher sequence on another probability space $(\Omega', \mc{A}', \P')$.
\end{thm}

\begin{proof}
  First suppose that $X$ is UMD.
  For one side of the estimate, use Kahane--Khintchine to write
  \begin{equation*}
    \begin{aligned}
      \|d\mb{f}_{\bullet}\|_{\varepsilon(L^p(\Omega;X))}
      &= \sup_{N \in \N} \|d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))} \\
      &\simeq_{p} \sup_{N \in \N} \Big( \int_{\Omega'} \int_{\Omega} \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega') d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega) \, \dd\P'(\omega') \Big)^{1/p}.
    \end{aligned}
  \end{equation*}
  For each $\omega' \in \Omega'$ and $N \in \N$, the UMD property of $X$ yields
  \begin{equation*}
    \begin{aligned}
      \int_{\Omega} \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega') d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega)
      &\leq \beta_{p}(X)^{p} \int_{\Omega} \Big\| \sum_{n = 0}^{N} d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega) \\
      &= \beta_{p}(X)^{p} \int_{\Omega} \|\mb{f}_{N}(\omega)\|_{X}^{p} \, \dd\P(\omega) \\
      &\leq \beta_{p}(X)^{p} \|\mb{f}_{N}\|_{L^p(\Omega;X)}^{p}.
  \end{aligned}
\end{equation*}
Integrating over $\omega' \in \Omega'$ and taking the $p$-th root and supremum over $N \in \N$ yields
\begin{equation*}
  \|d\mb{f}_{\bullet}\|_{\varepsilon(L^p(\Omega;X))} \lesssim_{p,X} \sup_{n \in \N} \|\mb{f}_{n}\|_{L^p(\Omega;X)}.
\end{equation*}
On the other hand, for $\omega' \in \Omega'$ and $N \in \N$ we can also write
\begin{equation*}
  \begin{aligned}
  \int_{\Omega} \Big\| \sum_{n = 0}^{N} d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega)
  &= \int_{\Omega} \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega')^{2} d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega) \\
  &\leq \beta_{p}(X)^{p} \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega') d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega).
\end{aligned}
\end{equation*}
Integrating this over $\omega' \in \Omega'$, taking the $p$-th root, and using Kahane--Khintchine gives
\begin{equation*}
  \|\mb{f}_{N}\|_{L^p(\Omega;X)} \lesssim_{p,X} \|d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))},
\end{equation*}
so taking the supremum over $N \in \N$ establishes \eqref{eq:burkholder}.

Now let's assume \eqref{eq:burkholder} and establish that $X$ is $\UMD_{p}$, and hence $\UMD$.
Let $\xi_{\bullet}$ be a sequence of signs and $(\mb{f}_{\bullet})$ an $X$-valued $L^p$-bounded martingale, and fix $N \in \N$.
Then by assumption
\begin{equation*}
  \begin{aligned}
  \Big\| \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)}
  &\lesssim_{p,X} \| \xi_{\bullet} d\mb{f}_{\bullet} \|_{\varepsilon_{N}(L^p(\Omega;X))} \\
  &\leq \|d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))}
  \lesssim_{p,X} \|\mb{f}_{N}\|_{L^p(\Omega;X)} = \Big\| \sum_{n = 0}^{N} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)}
\end{aligned}
\end{equation*}
using the contraction principle in the second estimate.
Taking the supremum over all $N$says exactly that $X$ is $\UMD_{p}$.
\end{proof}

As a straightforward corollary, using the identification of Rademacher averages in Lebesgue spaces from Theorem \ref{thm:rademacher-lp}, we obtain Burkholder's classical square function estimate for scalar-valued martingales.

\begin{cor}
  Let $f_{\bullet}$ be an $L^p$-bounded scalar-valued martingale on a probability space $(\Omega,\mc{A},\P)$.
  Then
  \begin{equation*}
    \sup_{n \in \N} \|f_{n}\|_{L^p(\Omega)} \simeq_{p} \Big\| \Big( \sum_{n \in \N} |df_{n}| \Big)^{1/2} \Big\|_{L^p(\Omega)}. 
  \end{equation*}
\end{cor}

Burkholder's inequalities lead to an easy criterion for the boundedness of a martingale transform with scalar-valued coefficients.
Recall from Definition \ref{dfn:mgale-transform}: given a Banach space $X$ and an $X$-valued martingale $\mb{f}_{\bullet}$ with respect to a filtration $\mc{A}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$, and given a scalar-valued stochastic process $g_{\bullet}$ in $L^\infty(\Omega)$ which is predictable with respect to $\mc{A}_{\bullet}$,
the martingale transform $(g \cdot \mb{f})_{\bullet}$ is the martingale defined by
\begin{equation*}
  (g \cdot \mb{f})_n := \sum_{m=0}^n g_m d\mb{f}_m,
\end{equation*}
or equivalently by the difference sequence
\begin{equation*}
  d(g \cdot \mb{f})_{n} = g_{m} d\mb{f}_{m}.
\end{equation*}

\begin{thm}\label{thm:scalar-mgale-tf-bdd}\index{martingale transforms!boundedness in UMD spaces}
  Let $X$ be a Banach space.
  Let $p \in (1,\infty)$, and let $\mb{f}_{\bullet}$ an $X$-valued $L^p$-bounded martingale with respect to a filtration $\mc{A}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$.
  Suppose that $g_{\bullet}$ is a scalar-valued stochastic process which is $L^\infty$-bounded and $\mc{A}_{\bullet}$-predictable.
  Then for all $N \in \N$,
  \begin{equation*}
    \|(g \cdot \mb{f})_{N}\|_{L^p(\Omega;X)}
    \lesssim_{p,X} \Big(\sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \Big) \|\mb{f}_{N}\|_{L^p(\Omega;X)}.
  \end{equation*}
\end{thm}

\begin{proof}
  By Burkholder's inequalities (Theorem \ref{thm:burkholder}) and Exercise \ref{ex:rad-leb-comm} (a consequence of Kahane--Khintchine), 
  \begin{equation*}
    \begin{aligned}
      \|(g \cdot \mb{f})_{N}\|_{L^p(\Omega;X)}
      &\simeq_{p,X} \|d(g \cdot \mb{f})_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))} \\
      &= \|g_{\bullet} d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))} \\
      &\simeq_{p} \|g_{\bullet} d\mb{f}_{\bullet}\|_{L^p(\Omega;\varepsilon_{N}(X))} \\
      &\simeq_{p} \Big( \int_{\Omega} \|g_{\bullet}(\omega) d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p}.
    \end{aligned}
  \end{equation*}
  The contraction principle then yields
  \begin{equation*}
    \begin{aligned}
    \Big( \int_{\Omega} \|g_{\bullet}(\omega) d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p}
    &\lesssim \Big( \int_{\Omega} \big( \sup_{n \in \N} |g_{n}(\omega)| \big)^{p} \|d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p} \\
    &\lesssim \big( \sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \big)  \Big( \int_{\Omega} \|d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p},
  \end{aligned}
\end{equation*}
and one more application of Exercise \ref{ex:rad-leb-comm} and Burkholder's inequality gives
\begin{equation*}
  \begin{aligned}
    \big( \sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \big)  \Big( \int_{\Omega} \|d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p}
    &= \big( \sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \big) \|d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))} \\
    &\lesssim_{p,X} \big( \sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \big) \|\mb{f}_{N}\|_{L^p(\Omega;X)},
  \end{aligned}
\end{equation*}
as we wanted.
\end{proof}

See Exercise \ref{ex:R-bd-mgale-tf} for one possible extension of this result to operator-valued martingale transforms.

\section{Haar decompositions}

We return to the Haar decomposition of a function, established way back in Example \ref{eg:haar}.
Given a Banach space $X$ and an integrable function $\mb{f} \in L^1([0,1);X)$ on the unit interval, the \emph{Haar expansion} of $\mb{f}$ is the representation
\begin{equation}\label{eq:haar-expansion}
  \mb{f} = \langle \mb{f} \rangle_{[0,1)} + \sum_{n \geq 1} \sum_{I \in \mc{D}_{n-1}} h_{I} \otimes \langle \mb{f}, h_{I} \rangle,
\end{equation}
where $\mc{D}_{n}$ is the set of dyadic subintervals of $[0,1)$ of length $2^{-n}$, $h_{I}$ is the $L^2$-normalised Haar function
\begin{equation*}
  h_{I} := \frac{1}{|I|^{1/2}} (\1_{I_{-}} - \1_{I_{+}})
\end{equation*}
where $I_{\pm}$ are the two dyadic subintervals of $I$ (with $\sup I_{-} = \inf I_{+}$).
To be precise, we have
\begin{equation}\label{eq:haar-mgale-repn}
  \mb{f} = \lim_{N \to \infty} \mb{f}_{N} =  \mb{f}_{0} + \lim_{N \to \infty} \sum_{n = 1}^{N} d\mb{f}_{n}
  = \mb{f}_{0} + \sum_{n = 1}^{\infty} d\mb{f}_{n}
\end{equation}
where $\mb{f}_{\bullet}$ is the martingale associated with $\mb{f}$ and the dyadic filtration $\mc{A}_{\bullet}$ (defined by $\mc{A}_{n} = \sigma(\mc{D}_{n})$), with convergence in $L^1([0,1);X)$ and almost everywhere by Theorems \ref{thm:mgale-conv-Lp} and \ref{thm:mgale-pw-conv}.
The Haar expansion \eqref{eq:haar-expansion} follows from computing
\begin{equation}\label{eq:haar-difference}
  d\mb{f}_{n} = \sum_{I \in \mc{D}_{n-1}} h_{I} \otimes \langle \mb{f}, h_{I} \rangle.
\end{equation}

Note that the convergence of \eqref{eq:haar-mgale-repn} depends \emph{a priori} on the order of summation of the infinite series.
When $X$ is UMD, however, the unconditionality of martingale differences says that the sequence $(d\mb{f}_{n})_{n \in \N}$ is unconditional in $L^p([0,1];X)$ for all $p \in (1,\infty)$, and thus that the convergence of \eqref{eq:haar-mgale-repn} in $L^p$ is independent of the order of summation.\index{Haar decomposition!unconditionality in UMD spaces}
This observation can be extended to a boundedness result for \emph{Haar multipliers}.

\begin{prop}\label{prop:haar-multipliers}\index{Haar multipliers!boundedness in UMD spaces}
  Let $X$ be a UMD Banach space and $p \in (1,\infty)$.
  For each dyadic interval $I \subset [0,1)$, let $a_{I} \in \K$ be a scalar, and suppose that $\sup_{I \in \mc{D}} |a_{I}| < \infty$.
  Then for all $\mb{f} \in L^p([0,1);X)$,
  \begin{equation*}
    \Big\| \sum_{I \in \mc{D}} a_{I} h_{I} \otimes \langle \mb{f}, h_{I} \rangle \Big\|_{L^p([0,1); X)} \lesssim_{p,X} \Big( \sup_{I \in \mc{D}} |a_{I}| \Big) \|\mb{f}\|_{L^p([0,1);X)}.
  \end{equation*}
  In particular, taking $a_{I} = \pm 1$, this implies that Haar decompositions are unconditional in $L^p([0,1);X)$.\footnote{We will see later that this characterises the UMD property.}
\end{prop}

\begin{proof}
  For each $n \in \N$ consider the function
  \begin{equation*}
    g_{n} := \sum_{I \in \mc{D}_{n-1}} a_{I} \1_{I} \qquad (g_{0} := 0).
  \end{equation*}
  Then $g_{n}$ is constant on each dyadic interval in $\mc{D}_{n-1}$, and thus $\mc{A}_{n-1}$ measurable.
  That is, the stochastic process $g_{\bullet}$ is $\mc{A}_{\bullet}$-predictable.
  Thus by Theorem \ref{thm:scalar-mgale-tf-bdd} and the representation \ref{eq:haar-difference} of the difference sequence $d\mb{f}_{n}$,
  \begin{equation*}
    \begin{aligned}
      \Big\| \sum_{n \in \N} g_{n} \sum_{I \in \mc{D}_{n-1}} h_{I} \otimes \langle \mb{f}, h_{I} \rangle \Big\|_{L^p([0,1);X)} 
      &\lesssim_{p,X} \Big( \sup_{n \in \N} \|g_{n}\|_{L^\infty([0,1))} \Big) \|\mb{f}\|_{L^p([0,1);X)} \\
      &= \Big( \sup_{I \in \mc{D}} |a_{I}| \Big) \|\mb{f}\|_{L^p([0,1);X)}.
  \end{aligned}
\end{equation*}
But since the Haar function $h_{I}$ is supported on $I$, we have
\begin{equation*}
  \begin{aligned}
    g_{n} \sum_{I \in \mc{D}_{n-1}} h_{I} \otimes \langle \mb{f}, h_{I} \rangle 
    &= \Big( \sum_{J \in \mc{D}_{n-1}} a_{J} \1_{J} \Big) \Big( \sum_{I \in \mc{D}_{n-1}} h_{I} \otimes \langle \mb{f}, h_{I} \rangle \Big) \\
    &= \sum_{I \in \mc{D}_{n-1}} a_{I} h_{I} \otimes \langle \mb{f}, h_{I} \rangle,
  \end{aligned}
\end{equation*}
which proves the result.
\end{proof}

In the scalar case $X = \K$, this leads to a quick proof of a classical result.

\begin{cor}\index{Haar decomposition!unconditionality in Lebesgue spaces}
  For all $p \in (1,\infty)$, the Haar functions $(h_{I})_{I \in \mc{D}}$ are unconditional in $L^p([0,1))$.
  That is, for any finite subset $\mc{I} \subset \mc{D}$ and any sequence of signs $(\xi_{I})_{I \in \mc{I}}$,
  \begin{equation*}
    \Big\| \sum_{I \in \mc{I}} \xi_{I} h_{I} \Big\|_{L^p([0,1))} \lesssim_{p} \Big\| \sum_{I \in \mc{I}} h_{I} \Big\|_{L^p([0,1))}
  \end{equation*}
  (the implicit constant does not depend on the choice of $\mc{I}$).
\end{cor}

\begin{proof}
  Let $a_{I} = \xi_{I}$ if $I \in \mc{I}$ and $a_{I} = 0$ for all $I \in \mc{D} \sm \mc{I}$.
  With $f = \sum_{I \in \mc{I}} h_{I}$, Proposition \ref{prop:haar-multipliers} says that
  \begin{equation*}
    \Big\|\sum_{I \in \mc{I}} a_{I} h_{I} \langle f, h_{I} \rangle \Big\|_{L^p([0,1))} \lesssim_{p} \|f\|_{L^p([0,1))}.
  \end{equation*}
  But since $\langle h_{I}, h_{J} \rangle = \delta_{I}(J)$, it follows that $\langle f, h_{I} \rangle = 1$ for all $I \in \mc{I}$, and thus
  \begin{equation*}
    \Big\|\sum_{I \in \mc{I}} a_{I} h_{I} \Big\|_{L^p([0,1))} \lesssim_{p} \Big\| \sum_{I \in \mc{I}} h_{I}\Big\|_{L^p([0,1))}
  \end{equation*}
  as claimed.
\end{proof}

Since every function $f \in L^p([0,1))$ has a Haar expansion, this says that the Haar functions are an \emph{unconditional basis} of $L^p([0,1))$ for all $p \in (1,\infty)$.
For more on bases and their relation with Banach space theory see \cite{AK06}.

\section{Failure of UMD for $L^1$ and other spaces}

So far we have not proven that any spaces do \emph{not} have the UMD property.
The fundamental example of such spaces are (infinite dimensional) $L^1$ spaces.
We will prove this through a stronger quantitative result, showing that the UMD constants of finite dimensional $L^1$ spaces grow at least logarithmically in their dimension.
We let $\ell^1_{n}$ denote the space of scalar sequences $(x_{i})_{i=1}^{n}$, or equivalently $\K^n$, with the $\ell^1$-norm.

\begin{thm}\label{thm:L1-not-UMD}
  There exists a constant $c > 0$ such that for all $n \in \N$, the $\UMD_{p}$ constant of the finite-dimensional $\ell^1$-space $\ell^{1}_{n}$ is bounded from below by
  \begin{equation*}
    \beta_{p}(\ell^{1}_{n}) > c\log(n).
  \end{equation*}
\end{thm}

\begin{proof}
  It suffices to show that there exists $c > 0$ such that
  \begin{equation*}
    \beta_{p}(\ell^{1}_{2^N}) > cN.
  \end{equation*}
  for all $N \in \N$, as for all $m \geq 1$ we can write
  \begin{equation*}
    \beta_{p}(\ell^1_{m}) \geq \beta_{p}(\ell^1_{2^N})
  \end{equation*}
  with $N = \lfloor \log_{2}(m) \rfloor$.

  Let $\Omega = \prod_{\N} \{-1,+1\}$ with the product $\sigma$-algebra and probability measure, and let $X = L^1(\Omega)$.
  Consider the $X$-valued martingale $\mb{f}_{\bullet}$ on $\Omega$ from Example \ref{eg:L1-noMCP}:
  \begin{equation*}
    \mb{f}_{n}(\omega) := \prod_{k = 1}^{n} (1 + \pi_{k}(\omega)\pi_{k})  \qquad (\mb{f}_{0} \equiv 1)
  \end{equation*}
  where $\pi_{k} \colon \Omega \to \{-1,+1\}$ is the $k$th coordinate function (we start indexing from $k = 1$ in this proof to ease notation).
  This is a martingale with respect to the filtration $\mc{A}_{\bullet}$ generated by $\pi_{\bullet}$.
  
  Now fix $N \geq 1$.
  Since the functions $\mb{f}_{1}, \mb{f}_{2}, \ldots, \mb{f}_{N}$ are all functions of the $\mc{A}_{N}$-measurable coordinate functions $\pi_{1}, \pi_{2}, \ldots, \pi_{N}$, and since $\mc{A}_{N}$ is generated by $2^{N}$ atoms of equal measure, the functions $\mb{f}_{1}, \ldots, \mb{f}_{N}$ take values in a subspace of $X$ which is isometric to $\ell^1_{2^{N}}$.
  Given a finite sequence of signs $(\xi_{n})_{n=0}^{N}$, this lets us write
  \begin{equation}\label{eq:L1-notUMD-est1}
    \Big\| \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)} \leq \beta_{p}(\ell^{1}_{2^N}) \| \mb{f}_{N} \|_{L^p(\Omega;X)}
  \end{equation}
  for all $p \in (1,\infty)$.
  We are left with estimating the norms on borh sides of this inequality.
  For the right hand side, note that for all $\omega \in \Omega$ we have
  \begin{equation*}
    \begin{aligned}
      \|\mb{f}_{N}(\omega)\|_{X}
      &= \int_{\Omega} \prod_{k=1}^{N} (1 + \pi_{k}(\omega) \pi_{k}(\eta)) \, \dd\P(\eta) \\
      &= \int_{\Omega} \prod_{k=1}^{N} (1 + \pi_{k}(\eta))\, \dd\P(\eta) 
      = \prod_{k=1}^{N} \int_{\Omega} 1 + \pi_{k}(\eta) \, \dd\P(\eta) = 1
    \end{aligned}
  \end{equation*}
  using that for each $\omega$, the sequence of random variables $(\pi_{k}(\omega) \pi_{k})_{k \in \N}$ has the same distribution as $(\pi_{k})_{k \in \N}$ (in fact, it is a Rademacher sequence), and then using independence of these random variables.
  Taking the $L^p$-norm over $\Omega$ then gives
  \begin{equation*}
    \Big\| \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)} \leq \beta_{p}(\ell^{1}_{2^N})
  \end{equation*}
  and we are left with the left hand side.
  
  For each $\omega \in \Omega$, a similar argument to the previous paragraph shows that
  \begin{equation*}
    \begin{aligned}
      \Big\|  \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n}(\omega) \Big\|_{X} 
      &= \int_{\Omega} \Big| \sum_{n=0}^{N} \xi_{n} \Big(\prod_{k=1}^{n-1} 1 + \pi_k(\omega) \pi_k (\eta) \Big) \Big((1 + \pi_n(\omega) \pi_n(\eta)) - 1\Big) \Big|\, \dd\P(\eta) \\
      &= \int_{\Omega} \Big| \sum_{n=0}^{N} \xi_{n} \Big(\prod_{k=1}^{n-1} 1 + \pi_k (\eta) \Big) \pi_n(\eta) \Big|\, \dd\P(\eta) \\
      &= \int_{\Omega} \Big| \sum_{n=0}^{N} \xi_{n} dg_{n}(\omega) \Big| \, \dd\P(\omega)
    \end{aligned}
  \end{equation*}
  where $g_{\bullet}$ is the scalar-valued martingale on $\Omega$ given by $g_{n} = \prod_{k=1}^{n} (1 + \pi_{k})$.
  We can estimate from below
  \begin{equation*}
      \sup_{\xi_{n} \in \pm 1} \Big\|  \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n}(\omega) \Big\|_{X}
      = \sup_{\xi_n \in \pm 1} \Big\|  \sum_{n=0}^{N} \xi_{n} dg_{n} \Big\|_{L^1(\Omega)} 
      \geq \E \Big\| \sum_{n=0}^{N} \varepsilon_{n} dg_{n} \Big\|_{L^1(\Omega)}
  \end{equation*}
  where $\varepsilon_{\bullet}$ is a Rademacher sequence (the average over all sign sequences is of course bounded by the supremum over all sign sequences).
  Thus by Khintchine's inequality (Theorem \ref{thm:khintchine}),
  \begin{equation*}
    \beta_{p}(\ell^{1}_{2^N}) \geq \sup_{\xi_{n} \in \pm 1} \Big\|  \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n}(\omega) \Big\|_{X}
    \gtrsim \Big\| \Big( \sum_{n=0}^{N} |dg_{n}|^2 \Big)^{1/2} \Big\|_{L^1(\Omega)}.
  \end{equation*}

  It remains to estimate the square function of $g_{\bullet}$ from below.
  Define disjoint sets
  \begin{equation*}
    \begin{aligned}
      \Omega_{k} &:= \{\omega_{1} = \cdots = \omega_{k} = 1, \omega_{k+1} = -1\} \qquad (0 < k < N). \\
    \end{aligned}
  \end{equation*}
  By disjointness we have
  \begin{equation*}
    \Big\| \Big( \sum_{n=0}^{N} |dg_{n}|^2 \Big)^{1/2} \Big\|_{L^1(\Omega)}
    \geq \sum_{m = 1}^{N - 1} \int_{\Omega_{m}} \Big( \sum_{n=0}^{N} |dg_{n}(\omega)|^2 \Big)^{1/2} \, \dd\P(\omega).
  \end{equation*}
  We can compute
  \begin{equation*}
    |dg_{n}(\omega)| = |\pi_{n}(\omega) g_{n-1}(\omega)| = 2^{n-1} \1_{\{\omega_{1} = \cdots = \omega_{n-1} = 1\}}
  \end{equation*}
  for $n > 1$, and $dg_{0}(\omega) \equiv 1$, so we can estimate
  \begin{equation*}
    \begin{aligned}
    \int_{\Omega_{m}} \Big( \sum_{n=0}^{N} |dg_{n}(\omega)|^2 \Big)^{1/2} \, \dd\P(\omega) 
    &= \int_{\Omega_{m}} \Big( 1 + \sum_{n=1}^{N} 2^{2n - 2} \1_{\{ \omega_{1} = \cdots = \omega_{n-1} = 1\}}(\omega) \Big)^{1/2} \, \dd\P(\omega) \\
    &= \int_{\Omega_{m}} \Big( 1 + \sum_{n=1}^{m+1} 2^{2n - 2} \Big)^{1/2} \, \dd\P(\omega) \\
    &= \P(\Omega_{m})\Big( 1 + \sum_{n=1}^{m+1} 2^{2n - 2} \Big)^{1/2} \\
    &\geq 2^{-m-1}\Big(\sum_{n=0}^{m} 4^n\Big)^{1/2} 
    \simeq 2^{-m} 2^{m} = 1.
  \end{aligned}
\end{equation*}
This crude estimate is enough to establish
\begin{equation*}
  \beta_{p}(\ell^1_{2^N}) \gtrsim \Big\| \Big( \sum_{n=0}^{N} |dg_{n}|^2 \Big)^{1/2} \Big\|_{L^1(\Omega)}
  \gtrsim \sum_{m=1}^{N-1} 1 \simeq N
\end{equation*}
for large $N$. which establishes the result.
  
  
\end{proof}

\begin{cor}\index{UMD property!failure for $L^1$}
  Let $(S,\mc{A},\mu)$ be a measure space such that $L^1(S)$ is infinite dimensional.
  Then $L^1(S)$ is not $\UMD$.
\end{cor}

\begin{proof}
  Suppose $n \in \N$.
  Since $L^1(S)$ has dimension greater than $n$, there exist pairwise disjoint sets $E_{1}, \ldots, E_{n} \in \mc{A}$ with $0 < \mu(E_{i}) < \infty$.
  Define a map $\iota_{n} \colon \ell^1_n \to L^1(S)$ by
  \begin{equation*}
    \iota_{n}((a_i)_{i=1}^{n}) := \sum_{i=1}^{n} \frac{a_{i}}{\mu(E_{i})} \1_{E_{i}}.
  \end{equation*}
  Then $\iota_{n}$ is linear, and
  \begin{equation*}
    \|\iota_{n}(a_{\bullet})\|_{L^1(S)} = \sum_{i=1}^{n} \frac{|a_{i}|}{\mu(E_{i})} \mu(E_{i}) = \|a_{\bullet}\|_{\ell^1_n},
  \end{equation*}
  so $\iota_{n}$ is an isometry.
  In particular, $\iota_{n}$ is an isometric isomorphism from $\ell^1_n$ to the closed subspace $\iota_{n}(\ell^1_n)$ of $L^1(S)$.
  Thus by Exercise \ref{ex:UMD-isomorphism},
  \begin{equation*}
    \beta_{2}(L^1(S)) \geq \beta_{2}(\ell^1_n) \gtrsim \log(n)
  \end{equation*}
  with implicit constant independent of $n$.
  Since $n$ was arbitrary, it follows that $\beta_{2}(L^1(S)) = \infty$, and thus $L^1(S)$ is not UMD.
\end{proof}

\begin{cor}
  The following Banach spaces are not UMD:\index{UMD property!failure for $L^\infty$, $c_0$, and $C(K)$}
  \begin{itemize}
  \item $L^\infty(S)$ whenever $(S,\mc{A},\mu)$ is a measure space such that $L^\infty(S)$ is infinite dimensional,
  \item The sequence space $c_{0}$ on $\N$,
  \item The space $M(K)$ of Borel measures of bounded variation on a compact Hausdorff space $K$ with infinitely many points.
  \item The space of continuous functions $C(K)$ on a compact Hausdorff topological space $K$ with infinitely many points.
  \end{itemize}
\end{cor}

\begin{proof}
  We repeatedly use Proposition \ref{prop:UMD-duality}: a Banach space $X$ is UMD if and only if its dual space $X^{*}$ is UMD.
  Since $L^\infty(S)$ is the dual of $L^1(S)$, and since $L^1(S)$ is infinite-dimensional when $L^\infty(S)$ is, we find that $L^\infty(S)$ is not UMD in this case.
  The dual of $c_{0}$ is $\ell^1$, which is not UMD.
  As for $M(K)$, there exists a finite measure $\mu \in M(K)$ such that $L^1(\mu)$ is an infinite dimensional closed subspace of $M(K)$.
  Since $L^1(\mu)$ is not UMD, neither is $M(K)$.
  Finally, the Riesz--Markov--Kakutani theorem says that the dual of $C(K)$ is $M(K)$, so $C(K)$ is also not UMD.
\end{proof}

We close this chapter by showing that UMD spaces have the Radon--Nikodym property.\footnote{A harder argument shows that UMD spaces are reflexive, and in fact `super-reflexive'. Recall that reflexive spaces have RNP.}

\begin{thm}\index{UMD property!implies the Radon--Nikodym property}\index{Radon--Nikodym property!of UMD spaces}
  Suppose that $X$ is a UMD Banach space.
  Then $X$ has the Radon--Nikodym property.
\end{thm}

\begin{proof}
  Aiming for a contradiction, suppose that $X$ does not have the Radon--Nikodym property.
  Then by Theorem \ref{thm:RNP-characterisations}, the unit ball of $X$ contains a $\delta$-separated tree for some $\delta > 0$.
  Thus there exists an $L^p$-bounded martingale $\mb{f}_{\bullet}$ valued in the unit ball of $X$ such that
  \begin{equation*}
    \|d\mb{f}_{n}\|_{L^p(X)} = \|\mb{f}_{n} - \mb{f}_{n-1}\|_{L^p(X)} \geq \delta
  \end{equation*}
  for all $n \geq 1$.
  Now fix $N \in \N$ and define a linear map $T \colon \ell^\infty_N \to L^p(X)$ by
  \begin{equation*}
    T(a_{\bullet}) := \sum_{n=1}^{N} a_{n} d\mb{f}_{n}.
  \end{equation*}
  Burkholder's inequalities (Theorem \ref{thm:burkholder}) and the contraction principle yields
  \begin{equation*}
    \begin{aligned}
    \|T(a_{\bullet})\|_{L^p(X)} = \Big\| \sum_{n=1}^{N} a_{n} d\mb{f}_{n} \Big\|_{L^p(X)}
    &\lesssim_{p,X} \|a_{n} d\mb{f}_{n}\|_{\varepsilon_{N}(L^p(X))} \\
    &\lesssim \|a_{\bullet}\|_{\ell^{\infty}_{N}} \|d\mb{f}_{n}\|_{\varepsilon_{N}(L^p(X))} \\
    &\leq \|a_{\bullet}\|_{\ell^{\infty}_{N}},
  \end{aligned}
\end{equation*}
so $T$ is bounded uniformly in $N$.
On the other hand we have for all $k$ 
\begin{equation*}
  |a_{k}| \leq \delta^{-1} \|a_{k} d\mb{f}_{k}\|_{L^p(X)} \lesssim_{p,X} \Big\| \sum_{k = 0}^{N} a_{k} d\mb{f}_{k}\Big\|_{L^p(X)}
\end{equation*}
using the UMD property for the second estimate, so we have that
\begin{equation*}
  \|T(a_{\bullet})\|_{L^p(X)} \simeq_{p,X,\delta} \|a_{\bullet}\|_{\ell^\infty_{N}}.
\end{equation*}
Thus $T$ is an isomorphism, and so by Exercise \ref{ex:UMD-isomorphism} and Propositions \ref{prop:UMD-duality} and \ref{prop:Bochner-UMDp},
\begin{equation*}
  \beta_{p}(X) = \beta_{p}(L^p(X)) \gtrsim_{p,X,\delta} \beta_{p}(\ell^\infty_N) = \beta_{p'}(\ell^1_N) \gtrsim \log(N).
\end{equation*}
Since this holds for all $N \in \N$ with implicit constants independent of $N$, it follows that $\beta_{p}(X) = \infty$, which contradicts the assumption that $X$ is UMD.
\end{proof}

\section*{Exercises}

\begin{exercise}\label{ex:UMD-isomorphism}\index{UMD property!of closed subspaces of UMD spaces}
  Let $X$ be a Banach space and $Y$ a closed subspace of $X$.
  Suppose that $Z$ is a Banach space and $\map{\phi}{Z}{Y}$ is an isomorphism.
  Show that 
  \begin{equation*}
    \beta_{p}(Z) \leq \|\phi\|_{\Lin(Z,Y)} \|\phi^{-1}\|_{\Lin(Y,Z)} \beta_{p}(X) \qquad \forall p \in (1,\infty).
  \end{equation*}
\end{exercise}

\begin{exercise}\label{ex:UMD-Lp-reverse}
  Let $X$ be a Banach space and $(S,\mc{A},\mu)$ be a measure space with $\mu(S) > 0$.
  For $p \in (1,\infty)$, show that $\beta_{p}(L^p(\mu;X)) \geq \beta_{p}(X)$.
\end{exercise}

\begin{exercise}\label{ex:stopped-filtration}\index{UMD property!randomised version}
  Let $\mc{A}_{\bullet}$ be a filtration on a probability space $(\Omega,\mc{A},\P)$, and let $T$ be a stopping time with respect to $\mc{A}_{\bullet}$.
  Show that $\mc{A}_{\min(\bullet,T)}$ is a filtration.
\end{exercise}

\begin{exercise}
  For $p \in (1,\infty)$, a Banach space $X$ is said to have the \emph{randomised $\UMD_{p}$ property} if for all $X$-valued $L^p$-bounded martingales on any probability space $(\Omega,\mc{A},\P)$ and any Rademacher sequence $\varepsilon_{\bullet}$,
  \begin{equation*}
    \sup_{N \in \N} \E \Big\| \sum_{n=0}^{N} \varepsilon_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)} \simeq \sup_{N \in \N} \Big\| \sum_{n=0}^{N} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)}.
  \end{equation*}
  (this is exactly like the definition of $\UMD_{p}$, but now the signs are randomised).

  \begin{itemize}
  \item Show that the $\UMD_{p}$ property implies the randomised $\UMD_{p}$ property.\footnote{The converse is an open problem!}
  \item Show that if $p,q \in (1,\infty)$, the randomised $\UMD_{p}$ property is equivalent to the randomised $\UMD_{q}$ property.
  \end{itemize}

\end{exercise}

In the following exercise we use the notion of the $R$-bound of a set of operators.\index{R-bounds@$R$-bounds}
Let $X$ and $Y$ be Banach spaces and suppose $\mc{T} \subset \Lin(X,Y)$ is a set of bounded linear operators from $X$ to $Y$.
We say that the set $\mc{T}$ is \emph{$R$-bounded} if there exists a constant $C < \infty$ such that for all finite sequences $(T_{n})_{n=0}^{N}$ in $\mc{T}$ and $(\mb{x}_{n})_{n=0}^{N}$,
\begin{equation*}
  \|T_{\bullet} \mb{x}_{\bullet}\|_{\varepsilon_{N}(Y)} \leq C \|\mb{x}_{\bullet}\|_{\varepsilon_{N}(X)}.
\end{equation*}
The best possible constant $C$ is denoted $R(\mc{T})$, and called the $R$-bound of $\mc{T}$.

\begin{exercise}\label{ex:R-bd-mgale-tf}\index{martingale transforms!boundedness in UMD spaces}
  Let $X$ and $Y$ be UMD Banach spaces, let $p \in (1,\infty)$, and let $\mb{f}_{\bullet}$ be an $X$-valued $L^p$-bounded martingale with respect to a filtration $\mc{A}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$.
  Let $T_{\bullet}$ be an $R$-bounded sequence of operators in $\Lin(X,Y)$.
  Show that the martingale transform with coefficients $T_{\bullet}$ is bounded, i.e.
  \begin{equation*}
    \| (T \cdot \mb{f})_{N} \|_{L^p(\Omega;Y)} \lesssim R(T_{\bullet}) \|\mb{f}_{N}\|_{L^p(\Omega;X)}.
  \end{equation*}
\end{exercise}

\begin{exercise}\index{UMD property!failure for $\mc{K}(H)$}
  Let $H$ be an infinite dimensional Hilbert space, and let $\mc{K}(H)$ be the space of all compact operators on $H$.
  Show that $\mc{K}(H)$ is not UMD.
\end{exercise}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
