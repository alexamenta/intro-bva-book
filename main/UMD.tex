In many applications of Banach-valued analysis, the most natural assumption to make on the target Banach space is the \emph{UMD property}.
This can be characterised in various ways, but the characterisation which gives it its name is the unconditionality of difference sequences of martingales valued in the Banach space.
In this chapter we will consider `basic' characterisations and implications of this property.
In the next chapter, which is probably the most important in terms of applications, we will discuss consequences for Fourier multiplier theorems and Littlewood--Paley theory.

\section{Definition and first examples}

Recall that the difference sequence of a stochastic process $\mb{f}_{\bullet}$ is the stochastic process $d\mb{f}_{\bullet}$ defined by $d\mb{f}_{n} := \mb{f}_{n} - \mb{f}_{n-1}$, with $\mb{f}_{-1} := \mb{0}$.

\begin{defn}
  For $p \in (1,\infty)$ a Banach space $X$ is said to have the \emph{$\UMD_{p}$ property} (Unconditionality of Martingale Differences in $L^p$) if there exists a constant $C < \infty$ such that for all probability spaces $(\Omega,\mc{A},\P)$, all $X$-valued $L^p$-bounded martingales $\mb{f}_{\bullet}$, and all sequences of signs $\xi_n = \pm 1$,
  \begin{equation}\label{eq:UMD-property}
    \sup_{N \in \N} \Big\| \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)} \leq C \sup_{N \in \N} \Big\| \sum_{n=0}^{N} d\mb{f}_{n}\Big\|_{L^p(\Omega;X)}.
  \end{equation}
  The best possible constant $C$ in this inequality is denoted $\beta_{p}(X)$.
  We say that $X$ has the \emph{UMD property} if it has the $\UMD_{p}$ property for all $p \in (1,\infty)$.
\end{defn}

\begin{rmk}
  We will see in Section \ref{sec:UMD-p-independence} that the $\UMD_{p}$ property for some $p \in (1,\infty)$ implies the $\UMD$ property, so (as with the $p$-martingale convergence properties) there is no need to isolate $\UMD_{p}$ as a separate property other than for pedagogical reasons.
\end{rmk}

In Banach space lingo, \eqref{eq:UMD-property} says that the sequence $d\mb{f}_\bullet$ in $L^p(\Omega;X)$ is \emph{unconditional}, or equivalently, that the convergence (or divergence) in $L^p(\Omega;X)$ of the series $\sum_{n \in \N} d\mb{f}_n$ is independent of the order of summation.
Put very coarsely, the UMD property says that martingale differences behave somewhat like orthogonal sequences in $L^p(\Omega;X)$.
In fact, when $X$ is a Hilbert space, the $\UMD_{2}$ property follows directly from orthogonality considerations.

\begin{prop}\label{prop:Hilbert-UMD2}
  Every Hilbert space $H$ has the $\UMD_{2}$ property, with best constant $\beta_{2}(H) = 1$.
\end{prop}

\begin{proof}
  Let $\mb{f}_{\bullet}$ be an $H$-valued martingale on a probability space $(\Omega,\mc{A},\P)$.
  Then for all sign sequences $\xi_{\bullet}$ and all $N \in \N$, since the differences $d\mb{f}_{n}$ are independent and hence pairwise orthogonal, we have
  \begin{equation*}
    \Big\| \sum_{n=0}^N \xi_{n} d\mb{f}_{n} \Big\|_{L^2(\Omega;H)}^{2}
    = \sum_{n = 0}^{N} \xi_{n}^{2} \|d\mb{f}_{n}\|_{L^2(\Omega;H)}^{2}
    = \sum_{n = 0}^{N} \|d\mb{f}_{n}\|_{L^2(\Omega;H)}^{2}
    = \Big\| \sum_{n=0}^N d\mb{f}_{n} \Big\|_{L^2(\Omega;H)}^{2}.
  \end{equation*}
  Thus we have equality in \eqref{eq:UMD-property}, with constant $C = 1$.
\end{proof}

The UMD property can be rephrased succinctly in terms of \emph{martingale sign transforms}.
Given a finite martingale $(\mb{f}_{n})_{n=0}^{N}$ valued in a Banach space $X$, and given a finite sequence of signs $(\xi_{n})_{n=0}^{N}$, recall that we defined the transformed martingale $(\xi \cdot \mb{f})_{\bullet}$ in terms of its difference sequence,
\begin{equation*}
  d(\xi \cdot \mb{f})_{n} := \xi_{n} d\mb{f}_{n} \qquad \text{(or equivalently, $(\xi \cdot \mb{f})_{k} = \sum_{n=0}^{k} \xi_{n} d\mb{f}_{n}$.)}
\end{equation*}
The $\UMD_{p}$ property can then be restated as follows: for all finite martingales $(\mb{f}_{n})_{n=0}^{N}$ and all finite sign sequences $(\xi_{n})_{n=1}^{N}$, we have a bound
\begin{equation*}
  \|(\xi \cdot \mb{f})_{N}\|_{L^p(\Omega;X)} \lesssim \|\mb{f}_{N}\|_{L^p(\Omega;X)}.
\end{equation*}
This can be restated in terms of filtrations.
Given a finite filtration $(\mc{A}_{n})_{n=0}^{N}$ on a probability space $(\Omega,\mc{A},\P)$ and a finite sequence of signs $(\xi_{n})_{n=0}^{N}$, let $T_{\mc{A}_{\bullet}, \xi_{\bullet}}$ denote the bounded operator acting on $L^1(\Omega;X)$ by
\begin{equation*}
  T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f} := \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n} = (\xi \cdot \mb{f})_{N},
\end{equation*}
where $\mb{f}_{\bullet}$ is the finite martingale associated with $\mb{f}$ and $\mc{A}_{\bullet}$.
At this point, we have already given all the necessary details to prove the following convenient characterisation of the UMD property.

\begin{prop}\label{prop:UMD-signtransforms}
  Let $X$ be a Banach space.
  Then the $\UMD_{p}$ constant of $X$ is given by
  \begin{equation*}
    \beta_{p}(X) = \sup_{\Omega,\mc{A}_{\bullet}, \xi_{\bullet}}  \|T_{\mc{A}_{\bullet}, \xi_{\bullet}}\|_{\Lin(L^p(\Omega;X))},
  \end{equation*}
  where the supremum is taken over all probability spaces $(\Omega,\mc{A},\P)$, all finite filtrations $\mc{A}_{\bullet}$, and all finite sign sequences $\xi_{\bullet}$.
\end{prop}

This characterisation of the $\UMD_{p}$ constant, with a short duality argument, implies the following.

\begin{prop}\label{prop:UMD-duality}
  Let $p \in (1,\infty)$, and let $X$ be a Banach space.
  Then $X$ has the $\UMD_{p}$ property if and only if the dual space $X^{*}$ has the $\UMD_{p'}$ property, and $\beta_{p}(X) = \beta_{p'}(X^{*})$.
\end{prop}

\begin{proof}
  Fix a finite filtration $\mc{A}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$, and a finite sign sequence $\xi_{\bullet}$.
  Suppose $\mb{f} \in L^{p}(\Omega;X)$ and $\mb{g} \in L^{p'}(\Omega;X^{*})$.
  Since the (scalar) conditional expectation $\E^{\mc{A}_{n}}$ on $L^p(\Omega)$ is adjoint to the corresponding conditional expectation on $L^{p'}(\Omega)$ (Proposition \ref{prop:CE-adjoint}), and since
  \begin{equation*}
    \langle \E^{\mc{A}_{n}} \mb{f}, \mb{g} \rangle = \langle \mb{f}, \E^{\mc{A}_{n}} \mb{g} \rangle 
  \end{equation*}
  by Exercise \ref{ex:tensor-adjoint}, we get
  \begin{equation*}
    \langle T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}, \mb{g} \rangle =  \langle \mb{f}, T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{g} \rangle.
  \end{equation*}
  Thus by the norming property of $L^{p'}(\Omega;X^{*})$ as a subset of $L^{p}(\Omega;X)^{*}$ (Proposition \ref{prop:bochner-preduality}), and of $L^{p}(\Omega;X)$ as a subset of $L^{p'}(\Omega;X^{*})^{*}$,\footnote{Technically we haven't proven this. The proof is a small modification of that of Proposition \ref{prop:bochner-preduality}, using that $X$ is a norming subspace of $X^{**} = (X^{*})^{*}$. See \cite[Proposition 1.3.1]{HNVW16} for the needed duality result in full generality.}  we get
  \begin{equation*}
    \begin{aligned}
      \|T_{\mc{A}_{\bullet}, \xi_{\bullet}}\|_{\Lin(L^{p}(\Omega;X))}
      &= \sup_{\mb{f}} \| T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f} \|_{L^{p}(\Omega;X)} \\
      &= \sup_{\mb{f}, \mb{g}} \langle T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}, \mb{g} \rangle \\
      &= \sup_{\mb{f}, \mb{g}} \langle  \mb{f}, T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{g} \rangle \\
      &= \sup_{\mb{g}} \| T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{g} \|_{L^{p'}(\Omega;X^{*})}
      &= \|T_{\mc{A}_{\bullet}, \xi_{\bullet}}\|_{\Lin(L^{p'}(\Omega;X^{*}))},
  \end{aligned}
\end{equation*}
with suprema taken over all normalised $\mb{f} \in L^{p}(\Omega;X)$ and $\mb{g} \in L^{p'}(\Omega;X^{*})$.
Taking the supremum over all $\Omega$, $\mc{A}_{\bullet}$, and $\xi_{\bullet}$ then shows that
\begin{equation*}
  \beta_{p}(X) = \beta_{p'}(X^{*}), 
\end{equation*}
completing the proof.
\end{proof}

Before moving on to more subtle properties, we show that the $\UMD_{p}$ property extends to Bochner spaces.
This will be more useful once we know the $p$-independence of the UMD property.

\begin{prop}\label{prop:Bochner-UMDp}
  Let $p \in (1,\infty)$, and suppose $X$ is a Banach space with the $\UMD_{p}$ property.
  Let $(S,\mc{A},\mu)$ be a measure space with $\mu(S) > 0$.
  Then the Bochner space $L^p(\mu;X)$ has $\UMD_{p}$, with $\beta_{p}(L^p(\mu;X)) = \beta_{p}(X)$.
\end{prop}

\begin{proof}
  To avoid confusion let $Y = L^p(\mu;X)$, and let $\mb{f}_{\bullet}$ be a $Y$-valued martingale on a probability space $(\Omega,\mc{A},\P)$.
  Then for all $N \in \N$, by wrapping the $\UMD_{p}$ property of $X$ in two applications of Fubini's theorem, we have
  \begin{equation*}
    \begin{aligned}
      \Big\| \sum_{n=0}^N \xi_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;Y)}^{p}
      &= \int_{\Omega} \int_{S} \Big\| \sum_{n=0}^N \xi_{n} d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\mu(s) \, \dd\P(\omega) \\
      &=  \int_{S} \Big( \int_{\Omega} \Big\| \sum_{n=0}^N \xi_{n} d\mb{f}_{n}(\omega) \Big\|_{X}^{p}  \, \dd\P(\omega) \Big) \, \dd\mu(s) \\
      &\leq \beta_{p}(X)^{p} \int_{S} \Big( \int_{\Omega} \Big\| \sum_{n=0}^N d\mb{f}_{n}(\omega) \Big\|_{X}^{p}  \, \dd\P(\omega) \Big) \, \dd\mu(s) \\
      &= \beta_{p}(X)^{p} \int_{\Omega}  \int_{S} \Big\| \sum_{n=0}^N d\mb{f}_{n}(\omega) \Big\|_{X}^{p}   \, \dd\mu(s) \, \dd\P(\omega) \\
      &= \beta_{p}(X)^{p} \Big\| \sum_{n=0}^N d\mb{f}_{n} \Big\|_{L^p(\Omega;Y)}^{p},
    \end{aligned}
  \end{equation*}
  which establishes $\beta_{p}(Y) \leq \beta_{p}(X)$.
  The reverse estimate is Exercise \ref{ex:UMD-Lp-reverse}.
\end{proof}

\section{$p$-independence of the UMD property}\label{sec:UMD-p-independence}

Our goal now is to prove that the $\UMD_{p}$ property is independent of $p$.
For this, we will use the characterisation from Proposition \ref{prop:UMD-signtransforms} of the $\UMD_{p}$ property in terms of finite martingale sign transforms $T_{\mc{A}_{\bullet}, \xi_{\bullet}}$.
The basic strategy is as follows:
\begin{itemize}
\item Assuming that the finite sign transforms $T_{\mc{A}_{\bullet},\xi_{\bullet}}$ are uniformly bounded on $L^p(\Omega;X)$, prove that they have weak-type $(1,1)$ (i.e. that they are bounded from $L^1(\Omega;X)$ to the Lorentz space $L^{1,\infty}(\Omega;X)$ uniformly over all finite filtrations and sign sequences.
\item Use Marcinkiewicz's interpolation theorem to deduce that the finite sign transforms are uniformly bounded on $L^q(\Omega;X)$ for all $1 < q < p$, and thus that $X$ has the $\UMD_{q}$ property for all $1 < q < p$.
\item Argue by duality to show that $X$ has the $\UMD_{r}$ property for all $p < r < \infty$, and thus that $X$ has the full $\UMD$ property.
\end{itemize}

The first two steps are similar to how we proved Doob's maximal inequality (Theorem \ref{thm:doob}), except there we implicitly interpolated between a weak-type $(1,1)$ estimate and an $L^\infty$ bound that such maximal operators automatically satisfy.
The weak-type $(1,1)$ estimate for sign transforms will be proven relies on \emph{Gundy's decomposition} for martingales, which is analogous to the Calder\'on--Zygmund decomposition of a function.\footnote{
Students of harmonic analysis may recognise this procedure as essentially being the same as that used in Calder\'on--Zygmund theory: start with an assumed $L^p$ estimate, use properties of the operators under consideration (here we use the martingale property rather than kernel estimates) to deduce weak-type $(1,1)$, and finally use interpolation and duality to extend the $L^p$-boundedness to $L^q$ for all $q \in (1,\infty)$.}

\begin{thm}[Gundy's decomposition]\label{thm:gundy}
  Let $X$ be a Banach space and $(\Omega,\mc{A},\P)$ a probability space.
  Suppose $\mb{f} \in L^1(\Omega;X)$ with $\|\mb{f}\|_{L^1(\Omega;X)} = 1$, and let $\mc{A}_{\bullet}$ be a filtration.
  Then given $\lambda > 0$, there is a decomposition
  \begin{equation*}
    \mb{f} = \mb{a} + \mb{b} + \mb{c}
  \end{equation*}
  with $\mb{a}, \mb{b}, \mb{c} \in L^1(\Omega;X)$ satisfying
  \begin{itemize}
  \item $\|\mb{a}\|_{L^{1}(\Omega;X)} \lesssim 1$, $\P\big(\{\sup_{n \in \N} \|d\mb{a}_{n}\|_{X} \neq 0\}\big) \lesssim \lambda^{-1}$,
  \item $\| \sum_{n \in \N} \|d\mb{b}_{n}\|_{X} \|_{L^1(\Omega)} \lesssim 1$,
  \item $\| \mb{c} \|_{L^\infty(\Omega;X)} \lesssim \lambda$ and $\|\mb{c}\|_{L^1(\Omega;X)} \lesssim 1$.
  \end{itemize}
  where $\mb{a}_{n} = \E^{\mc{A}_{n}} \mb{a}$ and $\mb{b}_{n} = \E^{\mc{A}_{n}} \mb{b}$ are the martingales associated with the filtration $\mc{A}_{\bullet}$.
\end{thm}

We will prove this in the next section.
For now we will assume it, and deduce the weak-type $(1,1)$ estimate for finite sign transforms in $\UMD_{p}$ spaces.

\begin{prop}\label{prop:martingale-w11}
  Suppose that $X$ is a Banach space with the $\UMD_{p}$ property.
  Then for all probability spaces $(\Omega,\mc{A},\P)$ and all finite filtrations $\mc{A}_{\bullet}$ and sign sequences $\xi_{\bullet}$,
  \begin{equation*}
    \|T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}\|_{L^{1,\infty}(\Omega;X)} := \sup_{t > 0} t\P(\|T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}\|_{X} > t) \lesssim_{p,X} \|\mb{f}\|_{L^1(\Omega;X)}
  \end{equation*}
  for all $\mb{f} \in L^1(\Omega;X)$.
\end{prop}

\begin{proof}[Proof, assuming Theorem \ref{thm:gundy}]
  Fix a finite filtration $(\mc{A}_{n})_{n=0}^{N}$ on a probability space $(\Omega,\mc{A},\P)$, and a sequence of signs $(\xi_{n})_{n=0}^{N}$.
  Let $\mb{f} \in L^1(\Omega;X)$; by rescaling we may assume $\|\mb{f}\|_{L^1(\Omega;X)} = 1$.\footnote{If $\mb{f} = 0$ there is nothing to show anyway.}
  Fix $t > 0$, and let
  \begin{equation*}
    \mb{f} = \mb{a} + \mb{b} + \mb{c}
  \end{equation*}
  be the Gundy decomposition of $\mb{f}$ at level $t$ (Theorem \ref{thm:gundy}).
  Writing
  \begin{equation*}
    \td{\mb{f}} := T_{\mb{A}_{\bullet}, \xi_{\bullet}}\mb{f},
    \qquad \td{\mb{a}} := T_{\mb{A}_{\bullet}, \xi_{\bullet}}\mb{a},
    \qquad \td{\mb{b}} := T_{\mb{A}_{\bullet}, \xi_{\bullet}}\mb{b},
    \qquad \td{\mb{c}} := T_{\mb{A}_{\bullet}, \xi_{\bullet}}\mb{c},
  \end{equation*}
  We have by linearity of the martingale transform
  \begin{equation*}
    \td{\mb{f}} = \td{\mb{a}} + \td{\mb{b}} + \td{\mb{c}}
  \end{equation*}
  and thus
  \begin{equation*}
    \P(\|\td{\mb{f}}\|_{X} > 3t)
    \leq \P(\|\td{\mb{a}}\|_{X} > t) + \P(\|\td{\mb{b}}\|_{X} > t) + \P(\|\td{\mb{c}}\|_{X} > t).
  \end{equation*}

  We estimate these three summands separately.
  First, we have
  \begin{equation*}
    \begin{aligned}
      \P(\|\td{ \mb{a} }\|_{X} > t)
      \leq \P(\sup_{n} \|d \mb{a}_{n}\|_{X} \neq 0) \lesssim t^{-1},
    \end{aligned}
  \end{equation*}
  as we cannot have $\| \td{ \mb{a} }  \|_{X} = \|T_{ \mc{A}_{\bullet}, \xi_{\bullet} } \mb{a}\|_{X}> t$ without one of the differences $d\mb{a}_{n}$ being nonzero.
  For the second term, since
  \begin{equation*}
    \|\td{\mb{b}}(\omega)\|_{X} = \Big\| \sum_{n=0}^{N} \xi_{n} d\mb{b}_{n}(\omega) \Big\|_{X} \leq \sum_{n = 0}^{N} \|d\mb{b}_{n}(\omega)\|_{X} \qquad \forall \omega \in \Omega,
  \end{equation*}
  we have
  \begin{equation*}
    \P(\|\td{\mb{b}}\|_{X} > t) \leq \P\Big( \sum_{n = 0}^{N} \|d\mb{b}_{n}\|_{X} > t \Big)
    \leq t^{-1} \Big\|  \sum_{n = 0}^{N} \|d\mb{b}_{n}\|_{X} \Big\|_{L^1(\Omega)} \lesssim t^{-1}
  \end{equation*}
  by Chebyshev's inequality.
  Finally, using Chebyshev again, the $\UMD_{p}$ property, and log-convexity of $L^p$-norms,
  \begin{equation*}
    \begin{aligned}
      \P(\|\td{\mb{c}}\|_{X} > t)
      &\leq t^{-p}\|T_{\mc{A}_{\bullet, \xi_{\bullet}}} \mb{c} \|_{L^p(\Omega;X)}^{p} \\
      &\lesssim_{p,X} t^{-p} \|\mb{c} \|_{L^p(\Omega;X)}^{p} \\
      &\leq t^{-p}  \Big( \|\mb{c} \|_{L^1(\Omega;X)}^{\frac{1}{p}} \|\mb{c}\|_{L^\infty(\Omega;X)}^{1-\frac{1}{p}} \Big)^{p} \\
      &\lesssim_{p} t^{-p}  t^{p-1} = t^{-1} .
    \end{aligned}
  \end{equation*}
  Therefore we have
  \begin{equation*}
    3t\P(\|T_{\mc{A}_{\bullet}, \xi_{\bullet}} \mb{f}\|_{X} > 3t)
    \lesssim_{p,X} 3t(3t^{-1}) \simeq 1 = \|\mb{f}\|_{L^{1}(\Omega;X)}.
  \end{equation*}
  Taking the supremum over $t > 0$ completes the proof.
\end{proof}

Now we can prove the $p$-independence of the UMD property, still implicitly assuming the validity of Gundy's decomposition.

\begin{thm}\label{thm:UMD-p-independent}
  Let $X$ be a Banach space which has the $\UMD_{p}$ property for some $p \in (1,\infty)$.
  Then $X$ has $\UMD_{q}$ for all $q \in (1,\infty)$ (i.e. $X$ is UMD).
\end{thm}

\begin{proof}
  By Proposition \ref{prop:martingale-w11} we have that all finite martingale sign transforms $T_{\mc{A}_{\bullet}, \xi_{\bullet}}$ are uniformly bounded from $L^1(\Omega;X)$ to $L^{1,\infty}(\Omega;X)$.
  By the Marcinkiewicz interpolation theorem for Bochner spaces (Theorem \ref{thm:marcinkiewicz} in the appendix), since these operators are also uniformly bounded on $L^p(\Omega;X)$, we find that they are uniformly bounded on $L^q(\Omega;X)$ for all $q \in (1,p)$, and thus $X$ has the $\UMD_{q}$ property for all such $q$.

  On the other hand, by Proposition \ref{prop:UMD-duality}, we know that $X^{*}$ has the $\UMD_{p'}$ property, and thus by the argument above $X^{*}$ has the $\UMD_{r}$ property for all $r \in (1,p')$.
  Hence $X$ itself has the $\UMD_{r'}$ property for all $r \in (1,p')$, which (messing with H\"older conjugates) says that $X$ has the $\UMD_{s}$ property for all $s \in (p,\infty)$.
  Thus $X$ is UMD.
\end{proof}

\begin{cor}
  The following Banach spaces are UMD:
  \begin{itemize}
  \item every Hilbert space,
  \item every finite-dimensional space,
  \item every Lebesgue space $L^p(S)$ over a measure space $(S,\mc{A},\mu)$, with $p \in (1,\infty)$,
  \item every Bochner space $L^p(S;X)$ with $S$ and $p$ as above, provided that $X$ is UMD.
  \end{itemize}
\end{cor}

\begin{proof}
  We proved in Proposition \ref{prop:Hilbert-UMD2} that every Hilbert space has the $\UMD_{2}$ property, so by $p$-independence, every Hilbert space is $\UMD$.
  Every finite-dimensional space is isomorphic to a Hilbert space, hence also UMD (see Exercise \ref{ex:UMD-isomorphism}).
  If $X$ is a UMD space, then in particular $X$ is $\UMD_{p}$, and thus by Proposition \ref{prop:Bochner-UMDp} the Bochner space $L^p(S;X)$ is also $\UMD_{p}$ (hence $\UMD$).
  Taking $X$ to be the scalar field, which is a Hilbert space, proves that $L^p(S)$ is $\UMD$.
\end{proof}

\begin{rmk}
  Theorem \ref{thm:UMD-p-independent} says that if the constant $\beta_{p}(X)$ is finite for some $p \in (1,\infty)$, then $\beta_{q}(X) < \infty$ for all $q \in (1,\infty)$, but it does not give sharp control on how $\beta_{q}(X)$ changes.
  It is possible (but difficult) to prove that
  \begin{equation*}
    \beta_{p}(H) = \max(p,p') - 1
  \end{equation*}
  for every Hilbert space $H$ and all $p \in (1,\infty)$ \cite[Corollary 4.5.15]{HNVW16}.
  As far as I know, this is the only case where the UMD constant of a space is known exactly (excluding the cases $\beta_{p}(X) = \infty$).
\end{rmk}


\section{The proof of Gundy's decomposition}

The proof of Gundy's decomposition, like the proof of Doob's maximal inequality, involves stopping time arguments, but this time they are somewhat heavier.
We will use a few stopping time concepts that we didn't introduce earlier.
Stopping times were defined back in Definition \ref{defn:stopping-time}.

\begin{defn}
  Let $(\Omega,\mc{A},\P)$ be a probability space and $\mc{A}_{\bullet}$ a filtration.
  Let $\map{T}{\Omega}{\N \cup \{\infty\}}$ be a stopping time with respect to the filtration $\mc{A}_{\bullet}$.

  \begin{itemize}
  \item We define the \emph{$\sigma$-algebra associated with the stopping time} $\mc{A}_{T}$ as follows: a subset $A \in \mc{A}$ belongs to $\mc{A}_{T}$ if and only if $A \cap \{T \leq n\} \in \mc{A}_{n}$ for all $n \in \N$.
    One can show that $\mc{A}_{T}$ is indeed a $\sigma$-algebra.
  \item Given a stochastic process $\mb{f}_{\bullet}$ valued in a Banach space $X$, we define the random variable $\mb{f}_{T} \colon \Omega \sm \{T = \infty\} \to X$ by
    \begin{equation*}
      \mb{f}_{T}(\omega) := \mb{f}_{T(\omega)}(\omega).
    \end{equation*}
  \end{itemize}
\end{defn}

Note that for each $n \in \N$, the constant random variable $n(\omega) \equiv n$ is a stopping time with respect to every filtration.
Note also that for two stopping times $S,T$ with respect to a filtration $\mc{A}_{\bullet}$, the minimum $\min(S,T)$ is also a stopping time with respect to $\mc{A}_{\bullet}$.
In particular, $\min(n,T)$ is a stopping time with respect to $\mc{A}_{\bullet}$, and we can define the sequence of $\sigma$-algebras $(\mc{A}_{\min(n,T)})_{n \in \N}$ and the random variables $(\mb{f}_{\min(n,T)})_{n \in \N}$.
The sequence $(\mc{A}_{\min(n,T)})_{n \in \N}$ is in fact a filtration (Exercise \ref{ex:stopped-filtration}), and the stochastic process $\mb{f}_{\min(n,T)}$ is called the \emph{stopped process}: the idea is that one takes the process $\mb{f}_{\bullet}$, but once the stopping time $T$ is reached (keeping in mind that this time depends on $\omega \in \Omega$), the process is `stopped' at the value $\mb{f}_{T}$.

\begin{prop}\label{prop:stopped-martingale}
  Let $X$ be a Banach space and $(\Omega,\mc{A},\P)$ a probability space, and let $\mc{A}_{\bullet}$ be a filtration.
  Let $\mb{f} \in L^1(\Omega;X)$, and let $\mb{f}_{\bullet} := \E^{\mc{A}_{\bullet}} \mb{f}$ be the associated martingale. 
  If $\map{T}{\Omega}{\N \cup \{\infty\}}$ is a finite stopping time, then
  \begin{equation}\label{eq:stopmgale}
    \E^{\mc{A}_{T}} \mb{f} \aeeq \mb{f}_{T},
  \end{equation}
  and so $\|\mb{f}_{T}\|_{L^1(\Omega;X)} \leq \|\mb{f}\|_{L^1(\Omega;X)}$.
  Furthermore, the stopped process $\mb{f}_{\min(\bullet,T)}$ is a martingale with respect to $\mc{A}_{\min(\bullet,T)}$.
\end{prop}

\begin{proof}
  The fact that $\mb{f}_{\min(\bullet,T)}$ is a martingale follows from \eqref{eq:stopmgale}, which implies in addition that $\mb{f}_{\min(\bullet,T)} = \E^{\mc{A}_{\min(\bullet,T)}} \mb{f}$ is the martingale associated with $\mb{f}$ and the filtration $\mc{A}_{\min(\bullet,T)}$.
  Thus we need only prove \eqref{eq:stopmgale}.
  
  Fix a set $A \in \mc{A}_{T}$, and let $A_{k} = A \cap \{T = k\}$ for $k \in \N \cup \{\infty\}$, so that $A_{k} \in \mc{A}_{k}$ and $(A_{k})_{k \in \N \cup \{\infty\}}$ is a partition of $A$.
  With the interpretation $\mb{f}_{\infty} := \mb{f}$, we have
  \begin{equation*}
      \int_{A} \mb{f} \, \dd \P 
      = \sum_{k \in \N \cup \{\infty\}} \int_{A_k} \mb{f} \, \dd \P 
      = \sum_{k \in \N \cup \{\infty\}} \int_{A_{k}} \mb{f}_{k} \, \dd \P 
      = \sum_{k \in \N \cup \{\infty\}} \int_{A_{k}} \mb{f}_{T} \, \dd \P 
      = \int_{A} \mb{f}_{T} \, \dd \P.
  \end{equation*}
  Since this holds for all $A \in \mc{A}_{T}$, the defining property of conditional expectations says that $\mb{f}_{T} = \E^{\mc{A}_{T}} \mb{f}$.
  The nonexpansiveness of conditional expectations on $L^1$ then implies $\|\mb{f}_{T}\|_{L^1(\Omega;X)} \leq \|\mb{f}\|_{L^1(\Omega;X)}$.
\end{proof}

Now, to Gundy's decomposition.

\begin{proof}[Proof of Theorem \ref{thm:gundy}]
  Define a stopping time
  \begin{equation*}
    r(\omega) := \inf\big\{ n \in \N : \|\mb{f}_{n}(\omega)\|_{X} > \lambda \big\},
  \end{equation*}
  where $\mb{f}$ is the martingale associated with $\mb{f}$ and $\mc{A}_{\bullet}$.
  For $n \in \N$ define the scalar-valued function $v_{n} = \|d\mb{f}_{n}\|_{X} \1_{\{r = n\}}$, and define two more stopping times
  \begin{equation*}
    \begin{aligned}
      s(\omega) &:= \inf\Big\{ n \in \N : \sum_{k=0}^{n} \E^{\mc{A}_{k}} v_{k+1} > \lambda \Big\} \\
      T(\omega) &:= \min(r(\omega), s(\omega)).
    \end{aligned}
  \end{equation*}
  We define the first term in the decomposition,
  \begin{equation*}
    \mb{a} := \mb{f} - \mb{f}_{T}.
  \end{equation*}
  Then by Proposition \ref{prop:stopped-martingale} we can estimate
  \begin{equation}\label{eq:a-1stest}
    \|\mb{a}\|_{L^1(\Omega;X)} \leq \|\mb{f}\|_{L^1(\Omega;X)} + \|\E^{\mc{A}_{T}}\mb{f}\|_{L^1(\Omega;X)}
    \lesssim \|\mb{f}\|_{L^1(\Omega;X)} = 1,
  \end{equation}
  and at least for this term of the decomposition it remains to show that
  \begin{equation}\label{eq:a-2ndest}
    \P\Big(\Big\{\sup_{n \in \N} \|d\mb{a}_{n}\|_{X} \neq 0\Big\}\Big) = \P\Big(\bigcup_{n \in \N} \big\{\|d\mb{a}_{n}\|_{X} \neq 0\big\}\Big) \lesssim \lambda^{-1}.
  \end{equation}
  If $\|d\mb{a}_{n}(\omega)\|_{X} \neq 0$ for some $n \in \N$, then we must have $d\mb{f}_{n}(\omega) \neq d\mb{f}_{\min(n,T)}(\omega)$ for some $n \in \N$, which implies that $T(\omega) < \infty$ (in fact, $T(\omega) < n$ for this particular $n$).
  So we have
  \begin{equation*}
    \P\Big(\bigcup_{n \in \N} \big\{\|d\mb{a}_{n}\|_{X} \neq 0\big\}\Big) \leq \P(\{T < \infty\}) \leq \P(\{s < \infty\}) + \P(\{r < \infty\}).
  \end{equation*}
  The summand involving $r$ is estimated by
  \begin{equation}\label{eq:a-rest}
    \P\big(\{r < \infty\}\big) = \P\big( \{\sup_{n \in \N} \|\mb{f}_{n}\|_{X} > \lambda \} \big)
    = \P\big( \mc{M}(\mb{f}_{\bullet}) > \lambda \big) \lesssim \lambda^{-1},
  \end{equation}
  using Doob's maximal inequality (Theorem \ref{thm:doob}).
  As for the summand involving $s$, we can use Chebyshev and the properties of conditional expectations to estimate
  \begin{equation*}
    \begin{aligned}
      \P\big(\{s < \infty\}\big)
      &= \P\big( \big\{ \sum_{k=0}^{\infty} \E^{\mc{A}_{k}} v_{k+1} > \lambda \big\} \big) \\
      &\leq \lambda^{-1} \E\Big( \sum_{k=0}^{\infty} \E^{\mc{A}_{k}} v_{k+1} \Big)
      = \lambda^{-1} \sum_{k=0}^{\infty} \E v_{k+1}
      = \lambda^{-1} \sum_{k=1}^{\infty} \E v_{k}.
    \end{aligned}
  \end{equation*}
  By definition of $v_{k}$ we have
  \begin{equation*}
    \E v_{k} = \E ( \|d\mb{f}_{k}\|_{X} \1_{\{r = k\}} ).
  \end{equation*}
  When $r(\omega) = k$ we have that
  \begin{equation*}
    \|\mb{f}_{k}(\omega)\|_{X} > \lambda \geq \|\mb{f}_{k-1}(\omega)\|_{X},
  \end{equation*}
  and thus also that
  \begin{equation*}
    \|d\mb{f}_{k}(\omega)\|_{X} \leq \|\mb{f}_{k}(\omega)\|_{X} + \|\mb{f}_{k-1}(\omega)\|_{X} \lesssim \|\mb{f}_{k}(\omega)\|_{X}.
  \end{equation*}
  This lets us estimate
  \begin{equation*}
    \E ( \|d\mb{f}_{k}\|_{X} \1_{\{r = k\}} )
    \lesssim \E ( \|\mb{f}_{k}\|_{X} \1_{\{r = k\}} )
    = \E ( \|\E^{\mc{A}_{k}} \mb{f}\|_{X} \1_{\{r = k\}} ).
  \end{equation*}
  The pointwise estimate for tensor extensions of positive operators in Theorem \ref{thm:positive-extensions} then allows us to proceed:
  \begin{equation*}
    \begin{aligned}
    \E ( \|\E^{\mc{A}_{k}} \mb{f}\|_{X} \1_{\{r = k\}} )
    \leq \E ( \E^{\mc{A}_{k}}(\|\mb{f}\|_{X}) \1_{\{r = k\}} )
    &= \E ( \E^{\mc{A}_{k}}(\|\mb{f}\|_{X} \1_{\{r = k\}}) ) \\
    &= \E (\|\mb{f}\|_{X} \1_{\{r = k\}})
  \end{aligned}
  \end{equation*}
  using that the set $\{r = k\}$ is $\mc{A}_{k}$-measurable (i.e. that $r$ is a stopping time with respect to $\mc{A}_{\bullet}$).
  Putting all this together yields
  \begin{equation*}
    \begin{aligned}
      \P\big(\{s < \infty\}\big)
      &\leq \lambda^{-1} \sum_{k=1}^{\infty} \E v_{k} \\
      &\lesssim \lambda^{-1} \sum_{k=1}^{\infty} \E (\|\mb{f}\|_{X} \1_{\{r = k\}})
      = \lambda^{-1} \E(\|\mb{f}\|_{X} \1_{\{1 \leq r < \infty\}}) \leq \lambda^{-1},
    \end{aligned}
  \end{equation*}
  which along with \eqref{eq:a-rest} proves the remaining estimate \eqref{eq:a-2ndest} for $\mb{a}$.
  
  Now we need to define functions $\mb{b}$ and $\mb{c}$ such that $\mb{f} - \mb{a} = \mb{b} + \mb{c}$.
  We will define $\mb{b}$ and $\mb{c}$ via their martingale difference sequences $d\mb{b}_{\bullet}$ and $d\mb{c}_{\bullet}$, and show that these indeed give rise to functions $\mb{b}, \mb{c} \in L^1(\Omega;X)$ with the desired properties.
  Since $\mb{f} - \mb{a} = \mb{f}_{T}$ by definition, we will need to have
  \begin{equation}\label{eq:db-req}
    d\mb{b}_{n} + d\mb{c}_{n} = d\mb{f}_{\min(n,T)} \qquad \forall n \in \N.
  \end{equation}
  By writing
  \begin{equation*}
    \begin{aligned}
      d\mb{f}_{\min(n,T)}
      &= \mb{f}_{\min(n,T)} - \mb{f}_{\min(n-1,T)} \\
      &= \1_{\{n \leq T\}} d\mb{f}_{n} 
      = \1_{\{n \leq r\}} \1_{\{n \leq s\}} d\mb{f}_{n} 
      =  ( \1_{\{n = r\}} + \1_{\{n < r\}}) \1_{\{n \leq s\}} d\mb{f}_{n},
    \end{aligned}
  \end{equation*}
  we see that \eqref{eq:db-req} is satisfied by the definitions
  \begin{equation*}
    \begin{aligned}
      d\mb{b}_{n} &:= \1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n}  - \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n} ), \\
      d\mb{c}_{n} &:= \1_{\{n < r\}} \1_{\{n \leq s\}} d\mb{f}_{n}  + \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n} )
    \end{aligned}
  \end{equation*}
  (for $n = 0$ we omit the conditional expectations).
  These are indeed martingale difference sequences: for all $n \in \N$ we have $\E^{\mc{A}_{n}} d\mb{b}_{n+1} = 0$ almost automatically, and
  \begin{equation*}
    \begin{aligned}
      \E^{\mc{A}_{n}} d\mb{c}_{n+1}
      &= \E^{\mc{A}_{n}} (d\mb{f}_{\min(n+1,T)} - d\mb{b}_{n+1}) \\
      &= \E^{\mc{A}_{n}} d\mb{f}_{\min(n+1,T)} \\
      &= \E^{\mc{A}_{n}} (\1_{\{n+1 \leq T\}} d\mb{f}_{n+1}) \\
      &= \1_{\{n+1 \leq T\}}\E^{\mc{A}_{n}} (d\mb{f}_{n+1}) = 0
    \end{aligned}
  \end{equation*}
  using that the set $\{n+1 \leq T\} = \Omega \sm \{T \leq n\}$ is $\mc{A}_{n}$-measurable ($T$ being a stopping time with respect to $\mc{A}_{\bullet}$) and that $\mb{f}_{\bullet}$ is a martingale.

  To ensure that we can use the martingale difference sequences $d\mb{b}_{\bullet}$ and $d\mb{c}_{\bullet}$ to define functions $\mb{b}, \mb{c} \in L^1(\Omega;X)$, we need to ensure convergence of the series $\sum_{n \in \N} d\mb{b}_{n}$ and $\sum_{n \in \N} d\mb{c}_{n}$ in $L^1(\Omega;X)$.
  Luckily we have
  \begin{equation*}
    \begin{aligned}
    \sum_{n \in \N} \|d\mb{b}_{n}\|_{X} \lesssim \sum_{n \in \N} \1_{\{n = r\}} \1_{\{n \leq s\}} \|d\mb{f}_{n}\|_{X}
    &\leq \sum_{n \in \N} v_{n} = \sum_{n \in \N} \E(\|\mb{f}\|_{X} \1_{\{r = n\}}) \lesssim 1
  \end{aligned}
  \end{equation*}
  by the argument used in estimating $\mb{a}$.
  This establishes that $\mb{b} = \sum_{n \in \N} d\mb{b}_{n} \in L^1(\Omega;X)$ exists, and also proves the claimed estimate
  \begin{equation*}
    \Big\| \sum_{n \in \N} \|d\mb{b}_{n}\|_{X} \Big\|_{L^1(\Omega;X)} \lesssim 1.
  \end{equation*}
  Now since $\mb{f}_{\min(n,T)} = \mb{b}_{n} + \mb{c}_{n}$, and since the martingales $\mb{f}_{\min(\bullet,T)}$ and $\mb{b}_{\bullet}$ both converge in $L^1(\Omega;X)$ and pointwise almost everywhere (Theorem \ref{thm:mgale-pw-conv}), it follows that $\mb{c}_{\bullet}$ also converges to some $\mb{c} \in L^1(\Omega;X)$, both in $L^1(\Omega;X)$ and pointwise almost everywhere.
  We have
  \begin{equation*}
    \begin{aligned}
    \|\mb{c}\|_{L^1(\Omega;X)} &\leq \|\mb{b}\|_{L^1(\Omega;X)} + \|\mb{f}_{T}\|_{L^1(\Omega;X)} \\
    &\leq \Big\| \sum_{n \in \N} \|d\mb{b}_{n}\|_{X} \Big\|_{L^1(\Omega;X)} + \|\E^{\mc{A}_{T}} \mb{f}\|_{L^1(\Omega;X)}
    \lesssim 1
  \end{aligned}
  \end{equation*}
  as required; only the bound $\|\mb{c}\|_{L^\infty(\Omega;X)} \lesssim \lambda$ remains.
  By the almost everywhere pointwise convergence we have for almost all $\omega \in \Omega$
  \begin{equation*}
    \begin{aligned}
      \mb{c}(\omega)
      &= \sum_{n \in \N} d\mb{c}_{n}(\omega) \\
      &= \sum_{n \in \N} \1_{\{n < r\}}(\omega) \1_{\{n \leq s\}}(\omega) d\mb{f}_{n}(\omega)  + \sum_{n \geq 1} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n} )(\omega).
    \end{aligned}
  \end{equation*}
  The first sum can be estimated by
  \begin{equation*}
    \begin{aligned}
      \sum_{n=0}^{N} \1_{\{n < r\}}(\omega) \1_{\{n \leq s\}}(\omega) d\mb{f}_{n}(\omega)
      &= \sum_{n=0}^{s(\omega)} \1_{\{n < r\}}(\omega) d\mb{f}_{n}(\omega) \\
      &= \begin{cases}
        \mb{f}_{\min(r-1,s)}(\omega) & \text{if $r(\omega) > 0$} \\ \mb{0} & \text{if $r(\omega) = 0$,}
      \end{cases}
    \end{aligned}
  \end{equation*}
  so by definition of the stopping time $r$ we have
  \begin{equation*}
    \Big\| \sum_{n=0}^{N} \1_{\{n < r\}}(\omega) \1_{\{n \leq s\}}(\omega) d\mb{f}_{n}(\omega) \Big\|_{X} \leq \lambda.
  \end{equation*}
  As for the second sum, since the set $\{n \leq s\} = \Omega \sm \{s \leq n-1\}$ is $\mc{A}_{n-1}$-measurable, we can write
  \begin{equation*}
    \begin{aligned}
      \sum_{n \geq 1} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \1_{\{n \leq s\}} d\mb{f}_{n} )(\omega)
      &= \sum_{n \geq 1} \1_{\{n \leq s\}}(\omega) \E^{\mc{A}_{n-1}} (\1_{\{n = r\}}  d\mb{f}_{n} )(\omega) \\
      &= \sum_{n = 1}^{s(\omega)} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}}  d\mb{f}_{n} )(\omega),
    \end{aligned}
  \end{equation*}
  and again using the pointwise estimate from Theorem \ref{thm:positive-extensions} we have
  \begin{equation*}
    \begin{aligned}
      \Big\| \sum_{n = 1}^{s(\omega)} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}}  d\mb{f}_{n} )(\omega) \Big\|_{X}
      &\leq \sum_{n=1}^{s(\omega)} \E^{\mc{A}_{n-1}} (\1_{\{n = r\}} \|d\mb{f}_{n}\|_{X} )(\omega) \\
      &= \sum_{n=0}^{s(\omega) - 1} \E^{\mc{A}_{n}} (v_{n-1} )(\omega) \leq \lambda
    \end{aligned}
  \end{equation*}
  by the definition of the stopping time $s$.
  All up, this yields
  \begin{equation*}
    \|\mb{c}\|_{L^\infty(\Omega;X)} \lesssim \lambda
  \end{equation*}
  and completes the proof.
\end{proof}

\section{Burkholder's inequalities and martingale transforms}

The UMD property is equivalent to \emph{Burkholder's inequalities}, which say that the $L^p$-norm of a martingale (i.e. the supremum of $L^p$ norms of its terms) is equivalent to a Rademacher average of its difference sequence in $L^p$.
We will state it in terms of the Rademacher spaces $\varepsilon(X)$ defined in Section \ref{sec:rademacher-spaces}.

\begin{thm}[Burkholder's inequalities]\label{thm:burkholder}
  A Banach space $X$ is $\UMD$ if and only if for all $p \in (1,\infty)$ and every $L^p$-bounded martingale $\mb{f}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$,
  \begin{equation}\label{eq:burkholder}
    \sup_{n \in \N} \|\mb{f}_{n}\|_{L^p(\Omega;X)} \simeq_{p,X} \|d\mb{f}_{\bullet}\|_{\varepsilon(L^p(\Omega;X))} = \Big( \E \Big\| \sum_{n \in \N} \varepsilon_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)}^{2} \Big)^{1/2}
  \end{equation}
  where $\varepsilon_{\bullet}$ is a Rademacher sequence on another probability space $(\Omega', \mc{A}', \P')$.
\end{thm}

\begin{proof}
  First suppose that $X$ is UMD.
  For one side of the estimate, use Kahane--Khintchine to write
  \begin{equation*}
    \begin{aligned}
      \|d\mb{f}_{\bullet}\|_{\varepsilon(L^p(\Omega;X))}
      &= \sup_{N \in \N} \|d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))} \\
      &\simeq_{p} \sup_{N \in \N} \Big( \int_{\Omega'} \int_{\Omega} \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega') d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega) \, \dd\P'(\omega') \Big)^{1/p}.
    \end{aligned}
  \end{equation*}
  For each $\omega' \in \Omega'$ and $N \in \N$, the UMD property of $X$ yields
  \begin{equation*}
    \begin{aligned}
      \int_{\Omega} \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega') d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega)
      &\leq \beta_{p}(X)^{p} \int_{\Omega} \Big\| \sum_{n = 0}^{N} d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega) \\
      &= \beta_{p}(X)^{p} \int_{\Omega} \|\mb{f}_{N}(\omega)\|_{X}^{p} \, \dd\P(\omega) \\
      &\leq \beta_{p}(X)^{p} \|\mb{f}_{N}\|_{L^p(\Omega;X)}^{p}.
  \end{aligned}
\end{equation*}
Integrating over $\omega' \in \Omega'$ and taking the $p$-th root and supremum over $N \in \N$ yields
\begin{equation*}
  \|d\mb{f}_{\bullet}\|_{\varepsilon(L^p(\Omega;X))} \lesssim_{p,X} \sup_{n \in \N} \|\mb{f}_{n}\|_{L^p(\Omega;X)}.
\end{equation*}
On the other hand, for $\omega' \in \Omega'$ and $N \in \N$ we can also write
\begin{equation*}
  \begin{aligned}
  \int_{\Omega} \Big\| \sum_{n = 0}^{N} d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega)
  &= \int_{\Omega} \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega')^{2} d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega) \\
  &\leq \beta_{p}(X)^{p} \Big\| \sum_{n = 0}^{N} \varepsilon_{n}(\omega') d\mb{f}_{n}(\omega) \Big\|_{X}^{p} \, \dd\P(\omega).
\end{aligned}
\end{equation*}
Integrating this over $\omega' \in \Omega'$, taking the $p$-th root, and using Kahane--Khintchine gives
\begin{equation*}
  \|\mb{f}_{N}\|_{L^p(\Omega;X)} \lesssim_{p,X} \|d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))},
\end{equation*}
so taking the supremum over $N \in \N$ establishes \eqref{eq:burkholder}.

Now let's assume \eqref{eq:burkholder} and establish that $X$ is $\UMD_{p}$, and hence $\UMD$.
Let $\xi_{\bullet}$ be a sequence of signs and $(\mb{f}_{\bullet})$ an $X$-valued $L^p$-bounded martingale, and fix $N \in \N$.
Then by assumption
\begin{equation*}
  \begin{aligned}
  \Big\| \sum_{n=0}^{N} \xi_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)}
  &\lesssim_{p,X} \| \xi_{\bullet} d\mb{f}_{\bullet} \|_{\varepsilon_{N}(L^p(\Omega;X))} \\
  &\leq \|d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))}
  \lesssim_{p,X} \|\mb{f}_{N}\|_{L^p(\Omega;X)} = \Big\| \sum_{n = 0}^{N} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)}
\end{aligned}
\end{equation*}
using the contraction principle in the second estimate.
Taking the supremum over all $N$says exactly that $X$ is $\UMD_{p}$.
\end{proof}

As a straightforward corollary, using the identification of Rademacher averages in Lebesgue spaces from Theorem \ref{thm:rademacher-lp}, we obtain Burkholder's classical square function estimate for scalar-valued martingales.

\begin{cor}
  Let $f_{\bullet}$ be an $L^p$-bounded scalar-valued martingale on a probability space $(\Omega,\mc{A},\P)$.
  Then
  \begin{equation*}
    \sup_{n \in \N} \|f_{n}\|_{L^p(\Omega)} \simeq_{p} \Big\| \Big( \sum_{n \in \N} |df_{n}| \Big)^{1/2} \Big\|_{L^p(\Omega)}. 
  \end{equation*}
\end{cor}

Burkholder's inequalities lead to an easy criterion for the boundedness of a martingale transform with scalar-valued coefficients.
Recall from Definition \ref{dfn:mgale-transform}: given a Banach space $X$ and an $X$-valued martingale $\mb{f}_{\bullet}$ with respect to a filtration $\mc{A}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$, and given a scalar-valued stochastic process $g_{\bullet}$ in $L^\infty(\Omega)$ which is predictable with respect to $\mc{A}_{\bullet}$,
the martingale transform $(g \cdot \mb{f})_{\bullet}$ is the martingale defined by
\begin{equation*}
  (g \cdot \mb{f})_n := \sum_{m=0}^n g_m d\mb{f}_m,
\end{equation*}
or equivalently by the difference sequence
\begin{equation*}
  d(g \cdot \mb{f})_{n} = g_{m} d\mb{f}_{m}.
\end{equation*}

\begin{thm}\label{thm:scalar-mgale-tf-bdd}
  Let $X$ be a Banach space.
  Let $p \in (1,\infty)$, and let $\mb{f}_{\bullet}$ an $X$-valued $L^p$-bounded martingale with respect to a filtration $\mc{A}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$.
  Suppose that $g_{\bullet}$ is a scalar-valued stochastic process which is $L^\infty$-bounded and $\mc{A}_{\bullet}$-predictable.
  Then for all $N \in \N$,
  \begin{equation*}
    \|(g \cdot \mb{f})_{N}\|_{L^p(\Omega;X)}
    \lesssim_{p,X} \Big(\sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \Big) \|\mb{f}_{N}\|_{L^p(\Omega;X)}.
  \end{equation*}
\end{thm}

\begin{proof}
  By Burkholder's inequalities (Theorem \ref{thm:burkholder}) and Exercise \ref{ex:rad-leb-comm} (a consequence of Kahane--Khintchine), %mk
  \begin{equation*}
    \begin{aligned}
      \|(g \cdot \mb{f})_{N}\|_{L^p(\Omega;X)}
      &\simeq_{p,X} \|d(g \cdot \mb{f})_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))} \\
      &= \|g_{\bullet} d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))} \\
      &\simeq_{p} \|g_{\bullet} d\mb{f}_{\bullet}\|_{L^p(\Omega;\varepsilon_{N}(X))} \\
      &\simeq_{p} \Big( \int_{\Omega} \|g_{\bullet}(\omega) d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p}.
    \end{aligned}
  \end{equation*}
  The contraction principle then yields
  \begin{equation*}
    \begin{aligned}
    \Big( \int_{\Omega} \|g_{\bullet}(\omega) d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p}
    &\lesssim \Big( \int_{\Omega} \big( \sup_{n \in \N} |g_{n}(\omega)| \big)^{p} \|d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p} \\
    &\lesssim \big( \sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \big)  \Big( \int_{\Omega} \|d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p},
  \end{aligned}
\end{equation*}
and one more application of Exercise \ref{ex:rad-leb-comm} and Burkholder's inequality gives
\begin{equation*}
  \begin{aligned}
    \big( \sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \big)  \Big( \int_{\Omega} \|d\mb{f}_{\bullet}(\omega)\|_{\varepsilon_{N}(X)}^{p} \, \dd\P(\omega) \Big)^{1/p}
    &= \big( \sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \big) \|d\mb{f}_{\bullet}\|_{\varepsilon_{N}(L^p(\Omega;X))} \\
    &\lesssim_{p,X} \big( \sup_{n \in \N} \|g_{n}\|_{L^\infty(\Omega)} \big) \|\mb{f}_{N}\|_{L^p(\Omega;X)},
  \end{aligned}
\end{equation*}
as we wanted.
\end{proof}

See Exercise \ref{ex:R-bd-mgale-tf} for one possible extension of this result to operator-valued martingale transforms.

\section{Haar decompositions}

We return to the Haar decomposition of a function, established way back in Example \ref{eg:haar}.
Given a Banach space $X$ and an integrable function $\mb{f} \in L^1([0,1);X)$ on the unit interval, the \emph{Haar expansion} of $\mb{f}$ is the representation
\begin{equation}\label{eq:haar-expansion}
  \mb{f} = \langle \mb{f} \rangle_{[0,1)} + \sum_{n \geq 1} \sum_{I \in \mc{D}_{n-1}} h_{I} \otimes \langle \mb{f}, h_{I} \rangle,
\end{equation}
where $\mc{D}_{n}$ is the set of dyadic subintervals of $[0,1)$ of length $2^{-n}$, $h_{I}$ is the $L^2$-normalised Haar function
\begin{equation*}
  h_{I} := \frac{1}{|I|^{1/2}} (\1_{I_{-}} - \1_{I_{+}})
\end{equation*}
where $I_{\pm}$ are the two dyadic subintervals of $I$ (with $\sup I_{-} = \inf I_{+}$).
To be precise, we have
\begin{equation}\label{eq:haar-mgale-repn}
  \mb{f} = \lim_{N \to \infty} \mb{f}_{N} =  \mb{f}_{0} + \lim_{N \to \infty} \sum_{n = 1}^{N} d\mb{f}_{n}
  = \mb{f}_{0} + \sum_{n = 1}^{\infty} d\mb{f}_{n}
\end{equation}
where $\mb{f}_{\bullet}$ is the martingale associated with $\mb{f}$ and the dyadic filtration $\mc{D}_{\bullet}$, with convergence in $L^1([0,1);X)$ and almost everywhere by Theorems \ref{thm:mgale-conv-Lp} and \ref{thm:mgale-pw-conv}.
The Haar expansion \eqref{eq:haar-expansion} follows from computing
\begin{equation*}
  d\mb{f}_{n} = \sum_{I \in \mc{D}_{n-1}} h_{I} \otimes \langle \mb{f}, h_{I} \rangle.
\end{equation*}
Note that the convergence of \eqref{eq:haar-mgale-repn} depends \emph{a priori} on the order of summation of the infinite series.

\section{Non-examples}

-$\beta_{p}(\ell^{1}_{n}) \gtrsim \log(n)$ (Pisier section 5.7), thus $L^1$ spaces are not UMD when infinite-dimensional
-$\ell^\infty$, $c_{0}$, $C([0,1])$, etc. by duality
-argument that UMD implies RNP
-remark that UMD implies reflexivity (i think we won't prove it)
-remark: Qiu's counterexample

\section*{Exercises}

\begin{exercise}\label{ex:UMD-isomorphism}
  Let $X$ be a Banach space and $Y$ a closed subspace of $X$.
  Suppose that $Z$ is a Banach space and $\map{\phi}{Z}{Y}$ is an isomorphism.
  Show that 
  \begin{equation*}
    \beta_{p}(Z) \leq \|\phi\|_{\Lin(Z,Y)} \|\phi^{-1}\|_{\Lin(Y,Z)} \beta_{p}(X) \qquad \forall p \in (1,\infty).
  \end{equation*}
\end{exercise}

\begin{exercise}\label{ex:UMD-Lp-reverse}
  Let $X$ be a Banach space and $(S,\mc{A},\mu)$ be a measure space with $\mu(S) > 0$.
  For $p \in (1,\infty)$, show that $\beta_{p}(L^p(\mu;X)) \geq \beta_{p}(X)$.
\end{exercise}

\begin{exercise}\label{ex:stopped-filtration}
  Let $\mc{A}_{\bullet}$ be a filtration on a probability space $(\Omega,\mc{A},\P)$, and let $T$ be a stopping time with respect to $\mc{A}_{\bullet}$.
  Show that $\mc{A}_{\min(\bullet,T)}$ is a filtration.
\end{exercise}

\begin{exercise}
  For $p \in (1,\infty)$, a Banach space $X$ is said to have the \emph{randomised $\UMD_{p}$ property} if for all $X$-valued $L^p$-bounded martingales on any probability space $(\Omega,\mc{A},\P)$ and any Rademacher sequence $\varepsilon_{\bullet}$,
  \begin{equation*}
    \sup_{N \in \N} \E \Big\| \sum_{n=0}^{N} \varepsilon_{n} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)} \simeq \sup_{N \in \N} \Big\| \sum_{n=0}^{N} d\mb{f}_{n} \Big\|_{L^p(\Omega;X)}.
  \end{equation*}
  (this is exactly like the definition of $\UMD_{p}$, but now the signs are randomised).

  \begin{itemize}
  \item Show that the $\UMD_{p}$ property implies the randomised $\UMD_{p}$ property.\footnote{The converse is an open problem!}
  \item Show that if $p,q \in (1,\infty)$, the randomised $\UMD_{p}$ property is equivalent to the randomised $\UMD_{q}$ property.
  \end{itemize}

\end{exercise}

In the following exercise we use the notion of the $R$-bound of a set of operators.
Let $X$ and $Y$ be Banach spaces and suppose $\mc{T} \subset \Lin(X,Y)$ is a set of bounded linear operators from $X$ to $Y$.
We say that the set $\mc{T}$ is \emph{$R$-bounded} if there exists a constant $C < \infty$ such that for all finite sequences $(T_{n})_{n=0}^{N}$ in $\mc{T}$ and $(\mb{x}_{n})_{n=0}^{N}$,
\begin{equation*}
  \|T_{\bullet} \mb{x}_{\bullet}\|_{\varepsilon_{N}(Y)} \leq C \|\mb{x}_{\bullet}\|_{\varepsilon_{N}(X)}.
\end{equation*}
The best possible constant $C$ is denoted $R(\mc{T})$, and called the $R$-bound of $\mc{T}$.

\begin{exercise}\label{ex:R-bd-mgale-tf}
  Let $X$ and $Y$ be UMD Banach spaces, let $p \in (1,\infty)$, and let $\mb{f}_{\bullet}$ be an $X$-valued $L^p$-bounded martingale with respect to a filtration $\mc{A}_{\bullet}$ on a probability space $(\Omega,\mc{A},\P)$.
  Let $T_{\bullet}$ be an $R$-bounded sequence of operators in $\Lin(X,Y)$.
  Show that the martingale transform with coefficients $T_{\bullet}$ is bounded, i.e.
  \begin{equation*}
    \| (T \cdot \mb{f})_{N} \|_{L^p(\Omega;Y)} \lesssim R(T_{\bullet}) \|\mb{f}_{N}\|_{L^p(\Omega;X)}.
  \end{equation*}
\end{exercise}





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main.tex"
%%% End:
